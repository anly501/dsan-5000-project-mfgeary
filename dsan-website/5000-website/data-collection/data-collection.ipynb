{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to answer questions regarding public sentiment on cannabis usage and its ties to psychosis and schizophrenia, we will get text from Reddit. Reddit functions as a public forum on a large variety of topics, making it a good source for text data featuring discussions on cannabis, schizophrenia, and psychosis.\n",
    "\n",
    "To get data from the Reddit API, I first made a user account and registered an app. This allowed me to generate a client ID and client secret for my app. My Reddit username and password are also necessary to gain access to the API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started getting data from the Reddit API, I generate an access token using a basic HTTP GET with the `requests` package in Python. Note that I have removed my personal information from this code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import requests.auth\n",
    "\n",
    "\n",
    "\n",
    "client_auth = requests.auth.HTTPBasicAuth(client_id, client_secret)\n",
    "post_data = {\"grant_type\": \"password\", \"username\": username, \"password\": password}\n",
    "headers = {\"User-Agent\": \"DSANProject/1.0 by u/Haunting_River_226\"}\n",
    "\n",
    "response = requests.post(\"https://www.reddit.com/api/v1/access_token\", auth=client_auth, data=post_data, headers=headers)\n",
    "response_data = response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this call to the API, the response returns an access token as well as more token information. I will use the access token and token type to construct my API requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "access_token = response_data['access_token']\n",
    "token_type = response_data['token_type']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I can use my access token to construct a header to use for all of my API calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\"Authorization\": str(token_type + access_token), \"User-Agent\": \"DSANProject/1.0 by u/Haunting_River_226\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to get the data, I have chosen three subreddits that will be relevant:\n",
    "    1. r/Psychosis\n",
    "    2. r/schizophrenia\n",
    "    3. r/weed\n",
    "Each of these subreddits relate to cannabis and/or psychosis, and I will be analyzing the text to determine if and how these topics intersect in public conversation. \n",
    "\n",
    "In order to get recent data, I will be pulling the top 10,000 posts from the previous year (October 12, 2022 - October 12, 2023). I use the `/top` end point to get the top posts in a given subreddit. The Reddit API pulls only the first 100 results from a subreddit, but I can get more than 100 results by using the `after` parameter and setting it equal to the `after` key in the `response` JSON. This starts by pulling the first 100 posts, then gets the next 100 posts, and so on until we have reached 10,000.\n",
    "\n",
    "The Reddit API also has stringent limits on the number of requests made per minute, so I'll use a sleep function that limits the API requests to 10 per minute.\n",
    "\n",
    "I will start with the r/Psychosis subreddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "post_id = \"\"\n",
    "data = {}\n",
    "for i in range(0, 100):\n",
    "    time.sleep(6)\n",
    "    response = requests.get(\"https://oauth.reddit.com/r/Psychosis/top.json\", params={'t': 'year', 'limit': 100, 'after': post_id}, headers=headers)\n",
    "    res = response.json()\n",
    "    data[i] = res\n",
    "    post_id = res[\"data\"][\"after\"][3:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will repeat this process to get data from r/schizophrenia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_id = \"\"\n",
    "data_schizophrenia = {}\n",
    "for i in range(0, 100):\n",
    "    time.sleep(6)\n",
    "    response = requests.get(\"https://oauth.reddit.com/r/schizophrenia/top.json\", params={'t': 'year', 'limit': 100, 'after': post_id}, headers=headers)\n",
    "    if(response.status_code != 200):\n",
    "        print(i)\n",
    "        print(response.status_code)\n",
    "    res = response.json()\n",
    "    data_schizophrenia[i] = res\n",
    "    post_id = res[\"data\"][\"after\"][3:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will repeat this process once more to get data from r/weed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_id = \"\"\n",
    "data_cannabis = {}\n",
    "for i in range(0, 100):\n",
    "    time.sleep(1)\n",
    "    response = requests.get(\"https://oauth.reddit.com/r/weed/top.json\", params={'t': 'year', 'limit': 100, 'after': post_id}, headers=headers)\n",
    "    res = response.json()\n",
    "    data_cannabis[i] = res\n",
    "    post_id = res[\"data\"][\"after\"][3:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsan5000",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
