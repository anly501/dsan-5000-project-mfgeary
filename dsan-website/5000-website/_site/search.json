[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DSAN-5000: Introduction",
    "section": "",
    "text": "Marion Geary Bauman is currently pursing a masters degree in Data Science and Analytics at Georgetown University. As a data scientist and statistician, she is drawn to the intersection of technical skills and their practical applications for improving the lives of others. She is driven to do technical work that is meaningful and can have a positive impact on those it influences. She loves to utilize her programming knowledge, analytical skills, and problem-solving abilities to devise creative and actionable solutions to real-world problems.\n\n\n\nIntellection\nEmpathy\nLearner\nAchiever\nInput\n\n\n\n\n\n\nMarion is particularly interested in studying Natural Language Processing and Large Language Models. She has experience using advanced NLP techniques in industry, and she is especially focused on the ethical usage of artificial intelligence.\n\n\n\n\n\n\n\nMarion has been a part of various open source development projects with a particular focus on R. She is currently developing a new set of themes for ggplot, and she supports the R community through answering questions on Stack Overflow.\n\n\n\n\nMarion developed an enterprise data science solution to classify medical research grant abstracts by intervention type to assist leadership at a large medical research organization. She has also used machine learning to build an NFL player performance predictor, which was used as the backend for a student-built app."
  },
  {
    "objectID": "index.html#meet-marion",
    "href": "index.html#meet-marion",
    "title": "DSAN-5000: Introduction",
    "section": "",
    "text": "Marion Geary Bauman is currently pursing a masters degree in Data Science and Analytics at Georgetown University. As a data scientist and statistician, she is drawn to the intersection of technical skills and their practical applications for improving the lives of others. She is driven to do technical work that is meaningful and can have a positive impact on those it influences. She loves to utilize her programming knowledge, analytical skills, and problem-solving abilities to devise creative and actionable solutions to real-world problems.\n\n\n\nIntellection\nEmpathy\nLearner\nAchiever\nInput"
  },
  {
    "objectID": "index.html#academic-interests",
    "href": "index.html#academic-interests",
    "title": "DSAN-5000: Introduction",
    "section": "",
    "text": "Marion is particularly interested in studying Natural Language Processing and Large Language Models. She has experience using advanced NLP techniques in industry, and she is especially focused on the ethical usage of artificial intelligence."
  },
  {
    "objectID": "index.html#previous-projects",
    "href": "index.html#previous-projects",
    "title": "DSAN-5000: Introduction",
    "section": "",
    "text": "Marion has been a part of various open source development projects with a particular focus on R. She is currently developing a new set of themes for ggplot, and she supports the R community through answering questions on Stack Overflow.\n\n\n\n\nMarion developed an enterprise data science solution to classify medical research grant abstracts by intervention type to assist leadership at a large medical research organization. She has also used machine learning to build an NFL player performance predictor, which was used as the backend for a student-built app."
  },
  {
    "objectID": "data-cleaning/data-cleaning.html",
    "href": "data-cleaning/data-cleaning.html",
    "title": "Data Cleaning",
    "section": "",
    "text": "Now that we have gathered a reasonable amount of textual and record data, we can begin the data cleaning process. Our ultimate goal is to do statistical modeling wtih our data, so we need to clean the data. Generally, we will follow the principles of tidy data (Wickham 2014) in cleaning our data."
  },
  {
    "objectID": "data-cleaning/data-cleaning.html#reddit-data",
    "href": "data-cleaning/data-cleaning.html#reddit-data",
    "title": "Data Cleaning",
    "section": "Reddit Data",
    "text": "Reddit Data\nRecall that the Reddit data was returned as a JSON. We retrieved 10,000 text posts for each of three different text files. Our goal is to turn each of these JSON files into an individual dataframe. From there, we can transform the data into a Bag of Words, Document Term Matrix, or any other helpful format.\nWe will use pandas and json to parse this data into a desired output. First let’s read in the data.\n\nimport pandas as pd\nimport json\n\nwith open(\"../data/raw_data/reddit_psychosis_data.json\") as f:\n    reddit_psychosis = json.load(f)\nwith open('../data/raw_data/reddit_cannabis_data.json') as f:\n    reddit_cannabis = json.load(f)\nwith open(\"../data/raw_data/reddit_schizophrenia_data.json\") as f:\n    reddit_schizophrenia = json.load(f)\n\nFrom the data pull, we know that each of these JSON files has 100 elements, each with 100 posts.\nLet’s look at the structure of one element to identify how we can extract the title and text information.\n\nreddit_psychosis['0'].keys()\n\ndict_keys(['kind', 'data'])\n\n\nFrom here, we see that the posts are within the parameter children.\n\nlen(reddit_psychosis['0']['data']['children'])\n\ndict_keys(['kind', 'data'])\n100\n\n\nIt looks like these are the 100 posts we’re looking for. Now we’ll extract the title and text from each of these children elements.\n\nreddit_psychosis['0']['data']['children'][1]['data'].keys()\n\ndict_keys(['approved_at_utc', 'subreddit', 'selftext', 'author_fullname', 'saved', 'mod_reason_title', 'gilded', 'clicked', 'title', 'link_flair_richtext', 'subreddit_name_prefixed', 'hidden', 'pwls', 'link_flair_css_class', 'downs', 'thumbnail_height', 'top_awarded_type', 'hide_score', 'name', 'quarantine', 'link_flair_text_color', 'upvote_ratio', 'author_flair_background_color', 'subreddit_type', 'ups', 'total_awards_received', 'media_embed', 'thumbnail_width', 'author_flair_template_id', 'is_original_content', 'user_reports', 'secure_media', 'is_reddit_media_domain', 'is_meta', 'category', 'secure_media_embed', 'link_flair_text', 'can_mod_post', 'score', 'approved_by', 'is_created_from_ads_ui', 'author_premium', 'thumbnail', 'edited', 'author_flair_css_class', 'author_flair_richtext', 'gildings', 'post_hint', 'content_categories', 'is_self', 'mod_note', 'created', 'link_flair_type', 'wls', 'removed_by_category', 'banned_by', 'author_flair_type', 'domain', 'allow_live_comments', 'selftext_html', 'likes', 'suggested_sort', 'banned_at_utc', 'url_overridden_by_dest', 'view_count', 'archived', 'no_follow', 'is_crosspostable', 'pinned', 'over_18', 'preview', 'all_awardings', 'awarders', 'media_only', 'can_gild', 'spoiler', 'locked', 'author_flair_text', 'treatment_tags', 'visited', 'removed_by', 'num_reports', 'distinguished', 'subreddit_id', 'author_is_blocked', 'mod_reason_by', 'removal_reason', 'link_flair_background_color', 'id', 'is_robot_indexable', 'report_reasons', 'author', 'discussion_type', 'num_comments', 'send_replies', 'whitelist_status', 'contest_mode', 'mod_reports', 'author_patreon_flair', 'author_flair_text_color', 'permalink', 'parent_whitelist_status', 'stickied', 'url', 'subreddit_subscribers', 'created_utc', 'num_crossposts', 'media', 'is_video'])\n\n\n\ntext = reddit_psychosis['0']['data']['children'][1]['data']['selftext']\ntitle = reddit_psychosis['0']['data']['children'][0]['data']['title']\n\nSince we have discovered the structure of this data, we can extract the text info for all of the posts. Let’s loop through all three files to get the data in a data frame.\nFirst, we can define function to loop through each of our JSON files.\n\ndef parse_reddit_json(reddit_json):\n    text_list = []\n    title_list = []\n    subreddit_list = []\n    for i in range(0, 100):\n        index = str(i)\n        for j in range(0, 100):\n            text_list.append(reddit_json[index]['data']['children'][j]['data']['selftext'])\n            title_list.append(reddit_json[index]['data']['children'][j]['data']['title'])\n            subreddit_list.append(reddit_json[index]['data']['children'][j]['data']['subreddit'])\n\n    return text_list, title_list, subreddit_list\n\nNow, the function parses each of the JSON files and outputs a list of the title of each post and the text contents of each post. All posts have titles, but not all posts have additional text.\n\npsy_text, psy_title, psy_sub = parse_reddit_json(reddit_psychosis)\nschiz_text, schiz_title, schiz_sub = parse_reddit_json(reddit_schizophrenia)\ncannabis_text, cannabis_title, cannabis_sub = parse_reddit_json(reddit_cannabis)\n\nNow that we have these lists, we can combine them into a pandas dataframe where each row is one post on Reddit.\n\ntext = psy_text + schiz_text + cannabis_text\ntitle = psy_title + schiz_title + cannabis_title\nsub = psy_sub + schiz_sub + cannabis_sub\n\nreddit_df = pd.DataFrame({'text': text, 'title': title, 'subreddit': sub})\nreddit_df.head()\n\n\n\n\n\n\n\n\ntext\ntitle\nsubreddit\n\n\n\n\n0\n3 years post-psychosis in recovery some days c...\nfirst time smiling on camera in... 3 years!\nPsychosis\n\n\n1\n\nI quit my meds lmfao\nPsychosis\n\n\n2\n\nI hate it here\nPsychosis\n\n\n3\n\nart by me. I thought it kinda visualized how I...\nPsychosis\n\n\n4\n\nBut I’m still god and this is neither a joke a...\nPsychosis\n\n\n\n\n\n\n\nNow we have a data frame of labeled text objects that will be easy to work with for modeling.\nTo take this data a step further, we can use a spacy pipeline to clean the text. This will do some standard cleaning to deal with things like emails, numbers, extra white space, and punctuation.\n\nreddit_df['all_text'] = reddit_df['title'] + \" \" + reddit_df['text']\ntext_list = list(reddit_df['all_text'])\n\n\n# Code adapted from DSAN 5800 Lab 2 by Dr. Larson\nimport spacy\n\npipeline = spacy.load('en_core_web_sm')\n\nimport re\nfrom spacy.language import Language\n\n# http://emailregex.com/\nemail_re = r\"\"\"(?:[a-z0-9!#$%&'*+/=?^_`{|}~-]+(?:\\.[a-z0-9!#$%&'*+/=?^_`{|}~-]+)*|\"(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21\\x23-\\x5b\\x5d-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])*\")@(?:(?:[a-z0-9](?:[a-z0-9-]*[a-z0-9])?\\.)+[a-z0-9](?:[a-z0-9-]*[a-z0-9])?|\\[(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?|[a-z0-9-]*[a-z0-9]:(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21-\\x5a\\x53-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])+)\\])\"\"\"\n\n# replace = [ (pattern-to-replace, replacement),  ...]\nreplace = [\n    (r\"&lt;a[^&gt;]*&gt;(.*?)&lt;/a&gt;\", r\"\\1\"),  # Matches most URLs\n    (email_re, \"email\"),            # Matches emails\n    (r\"(?&lt;=\\d),(?=\\d)\", \"\"),        # Remove commas in numbers\n    (r\"\\d+\", \"numbr\"),              # Map digits to special token &lt;numbr&gt;\n    (r\"[\\t\\n\\r\\*\\.\\@\\,\\-\\/]\", \" \"), # Punctuation and other junk\n    (r\"\\s+\", \" \")                   # Strips extra whitespace\n]\n\nfor repl in replace:\n    text_list = [re.sub(repl[0], repl[1], text) for text in text_list]\n\n@Language.component(\"DSAN5000\")\ndef DSAN5000_preprocess(doc):\n    tokens = [token for token in doc \n              if not any((token.is_stop, token.is_punct))]\n    tokens = [token.lemma_.lower().strip() for token in tokens]\n    tokens = [token for token in tokens if token]\n    return pipeline.make_doc(\" \".join(tokens))\n\npipeline.add_pipe(\"DSAN5000\")\n\n2023-10-12 21:34:10.776773: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n/Users/Marion/anaconda3/envs/dsan5800/lib/python3.11/site-packages/spacy/util.py:837: UserWarning: [W095] Model 'en_core_web_sm' (3.5.0) was trained with spaCy v3.5 and may not be 100% compatible with the current version (3.3.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n  warnings.warn(warn_msg)\n\n\n&lt;function __main__.DSAN5000_preprocess(doc)&gt;\n\n\nWe can use this pipeline to clean up the text data.\n\ntext_list_clean = [pipeline(doc) for doc in text_list]\n\n\ntext_list_clean[0:5]\n\n[time smile camera numbr year numbr year post psychosis recovery day complete nightmare nightmare u hold long time knock grow strong find breakthrough moment lt;numbr trust friend dm support ❤ ️,\n quit med lmfao,\n hate,\n art think kinda visualize feel,\n god joke crisis delusion realization understanding true nature]\n\n\nNow that our data is relatively clean, we can construct a bag of words using CountVectorizer.\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport numpy as np\n\nvectorizer = CountVectorizer()\nbow = vectorizer.fit_transform(text_list_clean)   \n\nWe can see the most popular words in the corpus:\n\nvectorizer.vocabulary_"
  },
  {
    "objectID": "data-cleaning/data-cleaning.html#wikipedia-data",
    "href": "data-cleaning/data-cleaning.html#wikipedia-data",
    "title": "Data Cleaning",
    "section": "Wikipedia Data",
    "text": "Wikipedia Data\nNext, let’s clean up our Wikipedia data. The Wikipedia API returned a complex nested R object. We already extracted the HTML from this R object and stored it in a csv, but we really want the main text of each webpage.\nWe will use rvest to “harvest” the data from each HTML and store all of the information in a tibble.\n\nlibrary(rvest)\n\nload(\"../data/wikipedia_scrape.Rdata\")\nwiki_data %&gt;% names()\n\n\n'title''text''link'\n\n\nThe HTML data is stored in the text column of the data frame. Let’s take the first element to parse out the text from each webpage.\n\nfirst &lt;- wiki_data$text[1]\nfirst %&gt;% \n    read_html() %&gt;%\n    html_element(\"body\") %&gt;% \n    html_element(\"div\") %&gt;% \n    html_elements(\"p\") %&gt;% \n    html_text() %&gt;% \n    head()\n\nThroughout exploring the structure of the HTML file in Wikipedia, we see that there is a simple way to get all paragraph text from each page. We just need to pull all the text from the &lt;p&gt; tags on each page.\nNow, we will loop through the tibble to convert the HTML string into a plain text string representing the paragraphs on the Wikipedia page.\n\ntext_column &lt;- list()\nfor(i in 1:nrow(wiki_data)) {\n    html &lt;- wiki_data$text[i]\n    text_list &lt;- html %&gt;%\n        read_html() %&gt;%\n        html_element(\"body\") %&gt;%\n        html_elements(\"p\") %&gt;%\n        html_text()\n    text_list &lt;- paste0(text_list, collapse = \" \")\n    text_column &lt;- append(text_column, text_list)\n}\n\nNow we have a list of all the text from each of these HTML files. Let’s add this as a new column to our tibble and get rid of the huge HTML strings to decrease our memory footprint.\n\nlibrary(tidyverse)\n\nwiki_data &lt;- wiki_data %&gt;%\n    select(-text) %&gt;%\n    tibble::add_column(raw_text = text_column)\n\nNow we have our text data from Wikipedia in a data frame with labels representing whether the page was a forward or backward link for the page titled “Long-term effects of cannabis.”"
  },
  {
    "objectID": "data-cleaning/data-cleaning.html#cannabinoids",
    "href": "data-cleaning/data-cleaning.html#cannabinoids",
    "title": "Data Cleaning",
    "section": "Cannabinoids",
    "text": "Cannabinoids\nWe can clean Gibson et al. (2020) data set on cannabinoids and their impact on individuals with psychosis. We will start again by visualizing missing data in our table.\n\nlibrary(readxl)\ncannabinoid &lt;- read_excel(\"../data/raw_data/DataforSumbission_FINAL.xlsx\", skip = 1)\n\ncannabinoid %&gt;% vis_dat()\n\nNew names:\n* `` -&gt; `...1`\n* `` -&gt; `...2`\n\n\n\n\n\nOverall, the completeness of this data is pretty good. We again remove the columns with a high rate of incompleteness. We can also drop rows with missing data because we see a pattern where the missing data tends to collect in rows.\n\ncannabinoid &lt;- cannabinoid %&gt;%\n    select(where(~ sum(is.na(.)) &lt; 20))\n    \ncannabinoid &lt;- cannabinoid %&gt;%\n    mutate(na_count = apply(., 1, function(x) sum(is.na(x)))) %&gt;%\n    filter(na_count &lt; 20) \n    \ncannabinoid %&gt;%\n    vis_dat()\n\n\n\n\nI notice that there seem to be a lot of character columns. Let’s check to make sure that these columns are standardized and do some cleaning up.\n\ncannabinoid &lt;- cannabinoid %&gt;% select(-`...1`)\n\n\nname_list &lt;- cannabinoid %&gt;%\n    select(where(is.character)) %&gt;%\n    names()\n\n\nfor(name in name_list) {\n    cannabinoid %&gt;%\n        select(all_of(name)) %&gt;% \n        distinct() %&gt;%\n        print(n = 10)\n}\n\n# A tibble: 3 x 1\n  Group_Toxicology    \n  &lt;chr&gt;               \n1 Cannabinoid-Positive\n2 Cannabinoid-Negative\n3 119                 \n# A tibble: 3 x 1\n  Toxicology_Synthetic_Cannabinoid\n  &lt;chr&gt;                           \n1 Synthetic Cannabinoid Negative  \n2 Synthetic Cannabinoid Positive  \n3 119                             \n# A tibble: 5 x 1\n  Batch \n  &lt;chr&gt; \n1 NA    \n2 DEC 17\n3 MAR 18\n4 JUL 18\n5 107   \n# A tibble: 6 x 1\n  RaceEthnicity\n  &lt;chr&gt;        \n1 Hispanic     \n2 Black        \n3 White        \n4 Asian        \n5 Unknown      \n6 119          \n# A tibble: 6 x 1\n  MaritalStatus               \n  &lt;chr&gt;                       \n1 \"Single\\u200e/Never married\"\n2  NA                         \n3 \"Widowed\\u200e/Divorced (?)\"\n4 \"Married\"                   \n5 \"0\"                         \n6 \"116\"                       \n# A tibble: 6 x 1\n  Education               \n  &lt;chr&gt;                   \n1 Non-high school graduate\n2 High school graduate    \n3 Some college            \n4 College graduate        \n5 NA                      \n6 114                     \n# A tibble: 5 x 1\n  WorkStatus \n  &lt;chr&gt;      \n1 Not working\n2 NA         \n3 Full time  \n4 Part time  \n5 111        \n# A tibble: 7 x 1\n  LivingSituation                                                      \n  &lt;chr&gt;                                                                \n1 \"Shelter\\u200e/homeless\"                                             \n2  NA                                                                  \n3 \"Independent Housing (own\\u200e/rent)\"                               \n4 \"Other\"                                                              \n5 \"Living with family\\u200e/friend\\u200e/domestic partner\\u200e/spouse\"\n6 \"Transitional Housing\\u200e (SRO, residential housing)\"              \n7 \"113\"                                                                \n# A tibble: 5 x 1\n  LivingSit_Combined                    \n  &lt;chr&gt;                                 \n1 \"Shelter\\u200e/homeless\"              \n2  NA                                   \n3 \"Independent Housing (own\\u200e/rent)\"\n4 \"Other\"                               \n5 \"113\"                                 \n# A tibble: 7 x 1\n  HouseholdSupport                    \n  &lt;chr&gt;                               \n1  NA                                 \n2 \"Self-support\"                      \n3 \"Government support (SSD\\u200e/SSI)\"\n4 \"Parental\\u200e/guardian support\"   \n5 \"Other\"                             \n6 \"Spouse\\u200e/partner support\"      \n7 \"108\"                               \n# A tibble: 5 x 1\n  Clinician_Admission_PANSS\n  &lt;chr&gt;                    \n1 AB                       \n2 HE                       \n3 DD                       \n4 CM                       \n5 119                      \n# A tibble: 5 x 1\n  Clinician_Discharge_PANSS\n  &lt;chr&gt;                    \n1 NA                       \n2 HE                       \n3 DD                       \n4 CM                       \n5 117                      \n# A tibble: 6 x 1\n  `Mini MDD Dx`   \n  &lt;chr&gt;           \n1 Past            \n2 None            \n3 Current and Past\n4 NA              \n5 Current         \n6 104             \n# A tibble: 6 x 1\n  `Mini Mania Dx` \n  &lt;chr&gt;           \n1 None            \n2 Current and Past\n3 NA              \n4 Past            \n5 Current         \n6 100             \n# A tibble: 3 x 1\n  `Pyschotic diagnosis at discharge`\n  &lt;chr&gt;                             \n1 Yes                               \n2 No                                \n3 119                               \n# A tibble: 3 x 1\n  `Mood diagnosis at discharge`\n  &lt;chr&gt;                        \n1 No                           \n2 Yes                          \n3 119                          \n# A tibble: 4 x 1\n  `History of mood disorder`\n  &lt;chr&gt;                     \n1 0                         \n2 Mania                     \n3 Depression                \n4 119                       \n# A tibble: 9 x 1\n  `Psychotic disoder dx at discharge` \n  &lt;chr&gt;                               \n1 Psychotic Disorder NOS              \n2 Schizoaffective Disorder            \n3 Bipolar Disorder                    \n4 Schizophrenia                       \n5 MDD with Psychotic Features         \n6 Substance Induced Psychotic Disorder\n7 None                                \n8 Other Psychotic Disorder            \n9 119                                 \n# A tibble: 3 x 1\n  `If patient required a PRN (extra) medication for agitation in CPEP`\n  &lt;chr&gt;                                                               \n1 No                                                                  \n2 Yes                                                                 \n3 119                                                                 \n# A tibble: 3 x 1\n  `If patient required a PRN (extra) medication for agitation in Unit`\n  &lt;chr&gt;                                                               \n1 No                                                                  \n2 Yes                                                                 \n3 119                                                                 \n# A tibble: 3 x 1\n  `If patient required a PRN (extra) medication for agitation in Unit or CPEP`\n  &lt;chr&gt;                                                                       \n1 No                                                                          \n2 Yes                                                                         \n3 119                                                                         \n# A tibble: 4 x 1\n  SMOKER\n  &lt;chr&gt; \n1 Yes   \n2 No    \n3 NA    \n4 117   \n# A tibble: 3 x 1\n  Sex   \n  &lt;chr&gt; \n1 Male  \n2 Female\n3 119   \n# A tibble: 4 x 1\n  `Smoker, no missing values (median)`\n  &lt;chr&gt;                               \n1 NA                                  \n2 Yes                                 \n3 No                                  \n4 107                                 \n# A tibble: 4 x 1\n  `If patient recieved antipsychotics before study blood draw`\n  &lt;chr&gt;                                                       \n1 NA                                                          \n2 Yes                                                         \n3 No                                                          \n4 113                                                         \n# A tibble: 4 x 1\n  `If patient recieved benzos before study blood draw`\n  &lt;chr&gt;                                               \n1 NA                                                  \n2 Yes                                                 \n3 No                                                  \n4 113                                                 \n# A tibble: 3 x 1\n  `Medical condition known to effect cytokine levels`\n  &lt;chr&gt;                                              \n1 No condition                                       \n2 Condition present                                  \n3 119                                                \n# A tibble: 4 x 1\n  Cocaine \n  &lt;chr&gt;   \n1 Negative\n2 Missing \n3 Positive\n4 119     \n# A tibble: 4 x 1\n  PCP     \n  &lt;chr&gt;   \n1 Positive\n2 Negative\n3 Missing \n4 119     \n# A tibble: 4 x 1\n  Opiates \n  &lt;chr&gt;   \n1 Negative\n2 Missing \n3 Positive\n4 119     \n# A tibble: 4 x 1\n  Benzos  \n  &lt;chr&gt;   \n1 Negative\n2 Missing \n3 Positive\n4 119     \n# A tibble: 4 x 1\n  Barbituates\n  &lt;chr&gt;      \n1 Negative   \n2 Missing    \n3 Positive   \n4 119        \n# A tibble: 4 x 1\n  Methadone\n  &lt;chr&gt;    \n1 Negative \n2 Missing  \n3 Positive \n4 119      \n# A tibble: 4 x 1\n  Amphetamines\n  &lt;chr&gt;       \n1 Negative    \n2 Missing     \n3 Positive    \n4 119         \n# A tibble: 4 x 1\n  Alcohol \n  &lt;chr&gt;   \n1 Negative\n2 Missing \n3 Positive\n4 119     \n# A tibble: 3 x 1\n  All_Other_Tox\n  &lt;chr&gt;        \n1 Positive     \n2 None         \n3 119          \n# A tibble: 56 x 1\n   `Medical Conditions`                                                         \n   &lt;chr&gt;                                                                        \n 1 None                                                                         \n 2 Hypothyroidism                                                               \n 3 Pt had leukocytosis axillary abscess drained while on unit and then a short ~\n 4 DM Type 2, HTN, CVA, HCV                                                     \n 5 Recently post-partum                                                         \n 6 AIDS                                                                         \n 7 Asthma, herpes, HTN                                                          \n 8 HTN                                                                          \n 9 None recorded but taking asthma medications                                  \n10 HIV                                                                          \n# i 46 more rows\n# A tibble: 3 x 1\n  `Patient Discharged on First Generation Antipsychotic`\n  &lt;chr&gt;                                                 \n1 No                                                    \n2 Yes                                                   \n3 119                                                   \n# A tibble: 3 x 1\n  `Patient Discharged on Second Generation Antipsychotic`\n  &lt;chr&gt;                                                  \n1 Yes                                                    \n2 No                                                     \n3 119                                                    \n# A tibble: 3 x 1\n  `Patient Discharged on Any Antipsychotic`\n  &lt;chr&gt;                                    \n1 Yes                                      \n2 No                                       \n3 119                                      \n# A tibble: 3 x 1\n  `Patient Discharged on Clozapine`\n  &lt;chr&gt;                            \n1 No                               \n2 Yes                              \n3 119                              \n# A tibble: 3 x 1\n  `Patient Discharged on Injectable Antipsychotic`\n  &lt;chr&gt;                                           \n1 Yes                                             \n2 No                                              \n3 119                                             \n# A tibble: 3 x 1\n  `Patient Discharged on Mood Stabilizer`\n  &lt;chr&gt;                                  \n1 No                                     \n2 Yes                                    \n3 119                                    \n# A tibble: 3 x 1\n  `Patient Discharged on Antidepressant`\n  &lt;chr&gt;                                 \n1 No                                    \n2 Yes                                   \n3 119                                   \n# A tibble: 3 x 1\n  `Patient Discharged on Anxioytic`\n  &lt;chr&gt;                            \n1 No                               \n2 Yes                              \n3 119                              \n# A tibble: 3 x 1\n  `Patient Discharged on Medication to Prevent Relapse`\n  &lt;chr&gt;                                                \n1 No                                                   \n2 Yes                                                  \n3 119                                                  \n# A tibble: 3 x 1\n  `Patient Discharged on Benzotropine or Equivalent`\n  &lt;chr&gt;                                             \n1 No                                                \n2 Yes                                               \n3 119                                               \n# A tibble: 3 x 1\n  `Patient Discharged on Propanolol for Akathisia`\n  &lt;chr&gt;                                           \n1 No                                              \n2 Yes                                             \n3 119                                             \n# A tibble: 3 x 1\n  `Patient Discharged on Benzotropine or Propanolol`\n  &lt;chr&gt;                                             \n1 No                                                \n2 Yes                                               \n3 119                                               \n\n\nGenerally, these character columns are very clean. However, I want to get rid of the encoding issue causing \\u200e to appear.\n\ncannabinoid %&gt;%\n    filter(if_any(where(is.character), ~str_detect(., \"\\u200e\"))) %&gt;%\n    slice_head(n = 1)\n\n\nA tibble: 1 x 104\n\n\nID\nGroup_Toxicology\nToxicology_Synthetic_Cannabinoid\nBatch\nRaceEthnicity\nMaritalStatus\nEducation\nNumberChildren\nWorkStatus\nLivingSituation\n...\nLogIL10_scaled\nLogIL12_scaled\nLogIL1b_scaled\nLogIL2_scaled\nLogIL21_scaled\nLogIL6_scaled\nLogIL8_scaled\nLogTNFa_scaled\nLogsIL2Ra_scaled\nLogCRP_scaled\n\n\n&lt;dbl&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n...\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1000\nCannabinoid-Positive\nSynthetic Cannabinoid Negative\nNA\nHispanic\nSingle&lt;U+200E&gt;/Never married\nNon-high school graduate\n0\nNot working\nShelter&lt;U+200E&gt;/homeless\n...\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\n\ncannabinoid &lt;- cannabinoid %&gt;%\n    mutate(across(where(is.character), ~str_replace_all(., \"\\u200e\", \"\")))\n\ncannabinoid %&gt;%\n    slice_head(n = 1)\n\n\nA tibble: 1 x 104\n\n\nID\nGroup_Toxicology\nToxicology_Synthetic_Cannabinoid\nBatch\nRaceEthnicity\nMaritalStatus\nEducation\nNumberChildren\nWorkStatus\nLivingSituation\n...\nLogIL10_scaled\nLogIL12_scaled\nLogIL1b_scaled\nLogIL2_scaled\nLogIL21_scaled\nLogIL6_scaled\nLogIL8_scaled\nLogTNFa_scaled\nLogsIL2Ra_scaled\nLogCRP_scaled\n\n\n&lt;dbl&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n...\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1000\nCannabinoid-Positive\nSynthetic Cannabinoid Negative\nNA\nHispanic\nSingle/Never married\nNon-high school graduate\n0\nNot working\nShelter/homeless\n...\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\nI also notice that there is one row where the values are numeric rather than character. It might be a total row or an error. Let’s get rid of that row.\n\n# Confirm that this is only one row\ncannabinoid %&gt;%\n    filter(Group_Toxicology == 119) %&gt;%\n    nrow() %&gt;%\n    print()\n\n# Remove the row\ncannabinoid &lt;- cannabinoid %&gt;%\n    filter(Group_Toxicology != 119)\n\n[1] 1\n\n\n\ncannabinoid %&gt;%\n    write_csv(\"../data/cannabinoid_clean.csv\")\n\nNow that our data is clean, we can move on to some exploratory data analysis."
  },
  {
    "objectID": "work-in-progress.html",
    "href": "work-in-progress.html",
    "title": "Work in Progress",
    "section": "",
    "text": "Work in Progress\nThis website is still being built. Check back later for more!"
  },
  {
    "objectID": "eda/eda.html",
    "href": "eda/eda.html",
    "title": "Data Exploration",
    "section": "",
    "text": "Build out your website tab for exploratory data analysis"
  },
  {
    "objectID": "eda/eda.html#quick-look-at-the-data",
    "href": "eda/eda.html#quick-look-at-the-data",
    "title": "Data Exploration",
    "section": "Quick look at the data",
    "text": "Quick look at the data\n\n# Import seaborn\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Apply the default theme\nsns.set_theme(style=\"whitegrid\", palette=\"pastel\")\n\n# Load an example dataset\ntips = sns.load_dataset(\"tips\")\nprint(tips)\n\n     total_bill   tip     sex smoker   day    time  size\n0         16.99  1.01  Female     No   Sun  Dinner     2\n1         10.34  1.66    Male     No   Sun  Dinner     3\n2         21.01  3.50    Male     No   Sun  Dinner     3\n3         23.68  3.31    Male     No   Sun  Dinner     2\n4         24.59  3.61  Female     No   Sun  Dinner     4\n..          ...   ...     ...    ...   ...     ...   ...\n239       29.03  5.92    Male     No   Sat  Dinner     3\n240       27.18  2.00  Female    Yes   Sat  Dinner     2\n241       22.67  2.00    Male    Yes   Sat  Dinner     2\n242       17.82  1.75    Male     No   Sat  Dinner     2\n243       18.78  3.00  Female     No  Thur  Dinner     2\n\n[244 rows x 7 columns]"
  },
  {
    "objectID": "eda/eda.html#basic-visualization",
    "href": "eda/eda.html#basic-visualization",
    "title": "Data Exploration",
    "section": "Basic visualization",
    "text": "Basic visualization\n\n\n# Create a visualization\nsns.relplot(\n    data=tips,\n    x=\"total_bill\", y=\"tip\", col=\"time\",\n    hue=\"smoker\", style=\"smoker\", size=\"size\",\n)\n\nplt.show()"
  },
  {
    "objectID": "data-collection/data-collection.html",
    "href": "data-collection/data-collection.html",
    "title": "Data Collection",
    "section": "",
    "text": "In order to answer questions regarding public sentiment on cannabis usage and its ties to psychosis and schizophrenia, we will get text from Reddit. Reddit functions as a public forum on a large variety of topics, making it a good source for text data featuring discussions on cannabis, schizophrenia, and psychosis.\nTo get data from the Reddit API, I first made a user account and registered an app. This allowed me to generate a client ID and client secret for my app. My Reddit username and password are also necessary to gain access to the API.\nTo get started getting data from the Reddit API, I generate an access token using a basic HTTP GET with the requests package in Python. Note that I have removed my personal information from this code.\n\nimport requests\nimport requests.auth\n\nclient_id = 'CLIENT_ID'\nclient_secret = 'CLIENT_SECRET'\nusername = 'USERNAME'\npassword = 'PASSWORD'\n\nclient_auth = requests.auth.HTTPBasicAuth(client_id, client_secret)\npost_data = {\"grant_type\": \"password\", \"username\": username, \"password\": password}\nheaders = {\"User-Agent\": \"DSANProject/1.0 by u/Haunting_River_226\"}\n\nresponse = requests.post(\"https://www.reddit.com/api/v1/access_token\", auth=client_auth, data=post_data, headers=headers)\nresponse_data = response.json()\n\nWith this call to the API, the response returns an access token as well as more token information. I will use the access token and token type to construct my API requests.\n\naccess_token = response_data['access_token']\ntoken_type = response_data['token_type']\n\nNow, I can use my access token to construct a header to use for all of my API calls.\n\nheaders = {\"Authorization\": str(token_type + access_token), \"User-Agent\": \"DSANProject/1.0 by u/Haunting_River_226\"}\n\nNow to get the data, I have chosen three subreddits that will be relevant: 1. r/Psychosis 2. r/schizophrenia 3. r/weed Each of these subreddits relate to cannabis and/or psychosis, and I will be analyzing the text to determine if and how these topics intersect in public conversation.\nIn order to get recent data, I will be pulling the top 10,000 posts from the previous year (October 12, 2022 - October 12, 2023). I use the /top end point to get the top posts in a given subreddit. The Reddit API pulls only the first 100 results from a subreddit, but I can get more than 100 results by using the after parameter and setting it equal to the after key in the response JSON. This starts by pulling the first 100 posts, then gets the next 100 posts, and so on until we have reached 10,000.\nThe Reddit API also has stringent limits on the number of requests made per minute, so I’ll use a sleep function that limits the API requests to 10 per minute.\nI will start with the r/Psychosis subreddit.\n\nimport time\n\npost_id = \"\"\ndata = {}\nfor i in range(0, 100):\n    time.sleep(6)\n    response = requests.get(\"https://oauth.reddit.com/r/Psychosis/top.json\", params={'t': 'year', 'limit': 100, 'after': post_id}, headers=headers)\n    res = response.json()\n    data[i] = res\n    post_id = res[\"data\"][\"after\"][3:]\n\nNext, we will repeat this process to get data from r/schizophrenia.\n\npost_id = \"\"\ndata_schizophrenia = {}\nfor i in range(0, 100):\n    time.sleep(6)\n    response = requests.get(\"https://oauth.reddit.com/r/schizophrenia/top.json\", params={'t': 'year', 'limit': 100, 'after': post_id}, headers=headers)\n    if(response.status_code != 200):\n        print(i)\n        print(response.status_code)\n    res = response.json()\n    data_schizophrenia[i] = res\n    post_id = res[\"data\"][\"after\"][3:]\n\nFinally, we will repeat this process once more to get data from r/weed.\n\npost_id = \"\"\ndata_cannabis = {}\nfor i in range(0, 100):\n    time.sleep(6)\n    response = requests.get(\"https://oauth.reddit.com/r/weed/top.json\", params={'t': 'year', 'limit': 100, 'after': post_id}, headers=headers)\n    res = response.json()\n    data_cannabis[i] = res\n    post_id = res[\"data\"][\"after\"][3:]\n\nLet’s save the data to limit calls to the API. We’ll save each dictionary as a JSON file to preserve it’s data structure. We will work on cleaning this data in the Data Cleaning page of this website.\n\nimport json\n\nwith open('reddit_psychosis_data.json', 'w') as json_file:\n        json.dump(data, json_file, indent=4)\n\nwith open('reddit_schizophrenia_data.json', 'w') as json_file:\n        json.dump(data_schizophrenia, json_file, indent=4)\n\nwith open('reddit_cannabis_data.json', 'w') as json_file:\n        json.dump(data_cannabis, json_file, indent=4)\n\n\n\n\nTo get a more academic perspective on the link between psychosis and cannabis, we will also pull data from Wikipedia. We will use R and the WikipediR package to get data from Wikipedia.\n\nlibrary(WikipediR)\n\n\nlong_term_cannabis_backlinks &lt;- page_backlinks(\n    \"en\",\n    \"wikipedia\",\n    page = \"Long-term effects of cannabis\",\n    limit = 500\n)\nlong_term_cannabis_links &lt;- page_links(\n    \"en\",\n    \"wikipedia\",\n    page = \"Long-term effects of cannabis\",\n    limit = 500,\n    namespaces = 0\n)\nlong_term_cannabis &lt;- page_content(\n    \"en\",\n    \"wikipedia\",\n    page_name = \"Long-term effects of cannabis\"\n)\n\n\nlong_term_cannabis_links$query$pages$`25905247`$links[[500]]\n\n\n    $ns\n        0\n    $title\n        'Effects of cannabis'\n\n\n\n\nlong_term_cannabis_backlinks$query$backlinks[[500]]\n\n\n    $pageid\n        53053428\n    $ns\n        0\n    $title\n        'San Marcos Seven'\n\n\n\nNow that we have the forward and back links for the “Long-term effects of cannabis” page, let’s get the content of the forward and back links to create our corpus.\n\nlibrary(tidyverse)\n\nwiki_data &lt;- tibble(\n    title = long_term_cannabis$parse$title,\n    text = long_term_cannabis$parse$text$`*`,\n    link = \"main\"\n)\n\n\nfor(i in 1:500) {\n    page_title = long_term_cannabis_links$query$pages$`25905247`$links[[i]]$title\n    tryCatch({\n        page_details &lt;- page_content(\n            \"en\",\n            \"wikipedia\",\n            page_name = page_title\n        )\n    },\n    error = function(e) {\n        print(paste0(\"error with \", page_title))\n    })\n\n    wiki_data &lt;- wiki_data %&gt;%\n        add_row(\n            title = page_details$parse$title,\n            text = page_details$parse$text$`*`,\n            link = \"link\"\n        )\n}\nfor(i in 1:500) {\n    page_id = long_term_cannabis_backlinks$query$backlinks[[i]]$page_id\n    tryCatch({\n        page_details &lt;- page_content(\n            \"en\",\n            \"wikipedia\",\n            page = page_id\n        )\n    },\n    error = function(e) {\n        print(paste0(\"error with \", page_id))\n    })\n    wiki_data &lt;- wiki_data %&gt;%\n        add_row(\n            title = page_details$parse$title,\n            text = page_details$parse$text$`*`,\n            link = \"back\"\n        )\n}\n\n\nwiki_data %&gt;% nrow()\n\n1001\n\n\n\nwiki_data %&gt;% \n    write_csv(file = \"wikipedia_scrape.csv\")\n\n\nsave(wiki_data, file = \"wikipedia_scrape.Rdata\")"
  },
  {
    "objectID": "data-collection/data-collection.html#reddit",
    "href": "data-collection/data-collection.html#reddit",
    "title": "Data Collection",
    "section": "",
    "text": "In order to answer questions regarding public sentiment on cannabis usage and its ties to psychosis and schizophrenia, we will get text from Reddit. Reddit functions as a public forum on a large variety of topics, making it a good source for text data featuring discussions on cannabis, schizophrenia, and psychosis.\nTo get data from the Reddit API, I first made a user account and registered an app. This allowed me to generate a client ID and client secret for my app. My Reddit username and password are also necessary to gain access to the API.\nTo get started getting data from the Reddit API, I generate an access token using a basic HTTP GET with the requests package in Python. Note that I have removed my personal information from this code.\n\nimport requests\nimport requests.auth\n\nclient_id = 'CLIENT_ID'\nclient_secret = 'CLIENT_SECRET'\nusername = 'USERNAME'\npassword = 'PASSWORD'\n\nclient_auth = requests.auth.HTTPBasicAuth(client_id, client_secret)\npost_data = {\"grant_type\": \"password\", \"username\": username, \"password\": password}\nheaders = {\"User-Agent\": \"DSANProject/1.0 by u/Haunting_River_226\"}\n\nresponse = requests.post(\"https://www.reddit.com/api/v1/access_token\", auth=client_auth, data=post_data, headers=headers)\nresponse_data = response.json()\n\nWith this call to the API, the response returns an access token as well as more token information. I will use the access token and token type to construct my API requests.\n\naccess_token = response_data['access_token']\ntoken_type = response_data['token_type']\n\nNow, I can use my access token to construct a header to use for all of my API calls.\n\nheaders = {\"Authorization\": str(token_type + access_token), \"User-Agent\": \"DSANProject/1.0 by u/Haunting_River_226\"}\n\nNow to get the data, I have chosen three subreddits that will be relevant: 1. r/Psychosis 2. r/schizophrenia 3. r/weed Each of these subreddits relate to cannabis and/or psychosis, and I will be analyzing the text to determine if and how these topics intersect in public conversation.\nIn order to get recent data, I will be pulling the top 10,000 posts from the previous year (October 12, 2022 - October 12, 2023). I use the /top end point to get the top posts in a given subreddit. The Reddit API pulls only the first 100 results from a subreddit, but I can get more than 100 results by using the after parameter and setting it equal to the after key in the response JSON. This starts by pulling the first 100 posts, then gets the next 100 posts, and so on until we have reached 10,000.\nThe Reddit API also has stringent limits on the number of requests made per minute, so I’ll use a sleep function that limits the API requests to 10 per minute.\nI will start with the r/Psychosis subreddit.\n\nimport time\n\npost_id = \"\"\ndata = {}\nfor i in range(0, 100):\n    time.sleep(6)\n    response = requests.get(\"https://oauth.reddit.com/r/Psychosis/top.json\", params={'t': 'year', 'limit': 100, 'after': post_id}, headers=headers)\n    res = response.json()\n    data[i] = res\n    post_id = res[\"data\"][\"after\"][3:]\n\nNext, we will repeat this process to get data from r/schizophrenia.\n\npost_id = \"\"\ndata_schizophrenia = {}\nfor i in range(0, 100):\n    time.sleep(6)\n    response = requests.get(\"https://oauth.reddit.com/r/schizophrenia/top.json\", params={'t': 'year', 'limit': 100, 'after': post_id}, headers=headers)\n    if(response.status_code != 200):\n        print(i)\n        print(response.status_code)\n    res = response.json()\n    data_schizophrenia[i] = res\n    post_id = res[\"data\"][\"after\"][3:]\n\nFinally, we will repeat this process once more to get data from r/weed.\n\npost_id = \"\"\ndata_cannabis = {}\nfor i in range(0, 100):\n    time.sleep(6)\n    response = requests.get(\"https://oauth.reddit.com/r/weed/top.json\", params={'t': 'year', 'limit': 100, 'after': post_id}, headers=headers)\n    res = response.json()\n    data_cannabis[i] = res\n    post_id = res[\"data\"][\"after\"][3:]\n\nLet’s save the data to limit calls to the API. We’ll save each dictionary as a JSON file to preserve it’s data structure. We will work on cleaning this data in the Data Cleaning page of this website.\n\nimport json\n\nwith open('reddit_psychosis_data.json', 'w') as json_file:\n        json.dump(data, json_file, indent=4)\n\nwith open('reddit_schizophrenia_data.json', 'w') as json_file:\n        json.dump(data_schizophrenia, json_file, indent=4)\n\nwith open('reddit_cannabis_data.json', 'w') as json_file:\n        json.dump(data_cannabis, json_file, indent=4)"
  },
  {
    "objectID": "data-collection/data-collection.html#wikipedia",
    "href": "data-collection/data-collection.html#wikipedia",
    "title": "Data Collection",
    "section": "",
    "text": "To get a more academic perspective on the link between psychosis and cannabis, we will also pull data from Wikipedia. We will use R and the WikipediR package to get data from Wikipedia.\n\nlibrary(WikipediR)\n\n\nlong_term_cannabis_backlinks &lt;- page_backlinks(\n    \"en\",\n    \"wikipedia\",\n    page = \"Long-term effects of cannabis\",\n    limit = 500\n)\nlong_term_cannabis_links &lt;- page_links(\n    \"en\",\n    \"wikipedia\",\n    page = \"Long-term effects of cannabis\",\n    limit = 500,\n    namespaces = 0\n)\nlong_term_cannabis &lt;- page_content(\n    \"en\",\n    \"wikipedia\",\n    page_name = \"Long-term effects of cannabis\"\n)\n\n\nlong_term_cannabis_links$query$pages$`25905247`$links[[500]]\n\n\n    $ns\n        0\n    $title\n        'Effects of cannabis'\n\n\n\n\nlong_term_cannabis_backlinks$query$backlinks[[500]]\n\n\n    $pageid\n        53053428\n    $ns\n        0\n    $title\n        'San Marcos Seven'\n\n\n\nNow that we have the forward and back links for the “Long-term effects of cannabis” page, let’s get the content of the forward and back links to create our corpus.\n\nlibrary(tidyverse)\n\nwiki_data &lt;- tibble(\n    title = long_term_cannabis$parse$title,\n    text = long_term_cannabis$parse$text$`*`,\n    link = \"main\"\n)\n\n\nfor(i in 1:500) {\n    page_title = long_term_cannabis_links$query$pages$`25905247`$links[[i]]$title\n    tryCatch({\n        page_details &lt;- page_content(\n            \"en\",\n            \"wikipedia\",\n            page_name = page_title\n        )\n    },\n    error = function(e) {\n        print(paste0(\"error with \", page_title))\n    })\n\n    wiki_data &lt;- wiki_data %&gt;%\n        add_row(\n            title = page_details$parse$title,\n            text = page_details$parse$text$`*`,\n            link = \"link\"\n        )\n}\nfor(i in 1:500) {\n    page_id = long_term_cannabis_backlinks$query$backlinks[[i]]$page_id\n    tryCatch({\n        page_details &lt;- page_content(\n            \"en\",\n            \"wikipedia\",\n            page = page_id\n        )\n    },\n    error = function(e) {\n        print(paste0(\"error with \", page_id))\n    })\n    wiki_data &lt;- wiki_data %&gt;%\n        add_row(\n            title = page_details$parse$title,\n            text = page_details$parse$text$`*`,\n            link = \"back\"\n        )\n}\n\n\nwiki_data %&gt;% nrow()\n\n1001\n\n\n\nwiki_data %&gt;% \n    write_csv(file = \"wikipedia_scrape.csv\")\n\n\nsave(wiki_data, file = \"wikipedia_scrape.Rdata\")"
  },
  {
    "objectID": "data-collection/data-collection.html#the-behavioral-sequelae-of-cannabis-use-in-health-people",
    "href": "data-collection/data-collection.html#the-behavioral-sequelae-of-cannabis-use-in-health-people",
    "title": "Data Collection",
    "section": "The Behavioral Sequelae of Cannabis Use in Health People",
    "text": "The Behavioral Sequelae of Cannabis Use in Health People\nThe first dataset comes from Sorkhou, Bedder, and George (2021) in The Behavioral Sequelae of Cannabis Use in Health People: A Systematic Review. This data was gathered as a collection of longitundial studies on the “cannabis-related adverse behavioral outcomes.”\nThe data comes in the form of a word document, so we can use the docxtractr package in R to extract the table. We will clean this table in the next step.\n\nlibrary(docxtractr)\n\ntable_as_docx &lt;- read_docx(\"../data/raw_data/Table_1_The Behavioral Sequelae of Cannabis Use in Healthy People_ A Systematic Review.DOCX\")\ntbl_out &lt;- docx_extract_tbl(table_as_docx)\ntbl_out %&gt;% write_csv(\"../data/behavioral_sequelae.csv\")"
  },
  {
    "objectID": "data-collection/data-collection.html#cannabis-research-article",
    "href": "data-collection/data-collection.html#cannabis-research-article",
    "title": "Data Collection",
    "section": "Cannabis Research Article",
    "text": "Cannabis Research Article\nThe next dataset comes from “cannabis research article” by baklaci (2023). This data was created to study the differences in cannabis usage between users with PEs and users without PEs.\nThis data comes in the form of and SPSS file, .sav, so we can read it using the haven package in R.\n\nlibrary(haven)\n\ncannabis_research_data &lt;- read_sav(\"../data/raw_data/dataset.sav\")\ncannabis_research_data %&gt;% write_csv(\"../data/cannabis_research_data.csv\")"
  },
  {
    "objectID": "data-collection/data-collection.html#cannabinoid-use-in-psychotic-patients-impacts-inflammatory-levels-and-their-association-with-psychosis-severity",
    "href": "data-collection/data-collection.html#cannabinoid-use-in-psychotic-patients-impacts-inflammatory-levels-and-their-association-with-psychosis-severity",
    "title": "Data Collection",
    "section": "Cannabinoid use in psychotic patients impacts inflammatory levels and their association with psychosis severity",
    "text": "Cannabinoid use in psychotic patients impacts inflammatory levels and their association with psychosis severity\nThe next dataset comes from Gibson et al. (2020) and their research on the impact of cannabinoid usage on psychotic patients. This data is easily parsed as an excel file using readxl.\n\nlibrary(readxl)\n\ncannabinoid &lt;- read_excel(\"../data/raw_data/DataforSumbission_FINAL.xlsx\")"
  },
  {
    "objectID": "data-collection/data-collection.html#cannabis-use-schizotypy-and-kamin-blocking-performance",
    "href": "data-collection/data-collection.html#cannabis-use-schizotypy-and-kamin-blocking-performance",
    "title": "Data Collection",
    "section": "Cannabis Use, Schizotypy and Kamin Blocking Performance",
    "text": "Cannabis Use, Schizotypy and Kamin Blocking Performance\nNext, we will utilize a data set that was used by Dawes et al. (2021) to study cannabis usage and schizotypy and their relationship with Kamin blocking. We will again use docxtractr to read in this data.\n\ndocx_table_2 &lt;- read_docx(\"../data/raw_data/Table_2_Cannabis Use, Schizotypy and Kamin Blocking Performance.DOCX\")\nkamin_blocking &lt;- docx_table_2 %&gt;%\n    docx_extract_tbl() %&gt;%\n    write_csv(\"../data/kamin_blocking.csv\")"
  },
  {
    "objectID": "data-collection/data-collection.html#cannabis-use-in-male-and-female-first-episode-of-non-affective-psychosis-patients-long-term-clinical-neuropsychological-and-functional-differences",
    "href": "data-collection/data-collection.html#cannabis-use-in-male-and-female-first-episode-of-non-affective-psychosis-patients-long-term-clinical-neuropsychological-and-functional-differences",
    "title": "Data Collection",
    "section": "Cannabis use in male and female first episode of non-affective psychosis patients: Long-term clinical, neuropsychological and functional differences",
    "text": "Cannabis use in male and female first episode of non-affective psychosis patients: Long-term clinical, neuropsychological and functional differences\nIn our final dataset, Setién-Suero et al. (2017) aim to study the difference in men and women in the link between cannabis usage and psychosis. Setién-Suero et. al. provide two datasets that will be utilized in this analysis.\n\nread_sav(\"../data/raw_data/S1File.sav\") %&gt;%\n    write_csv(\"../data/s1file.csv\")\n\nread_sav(\"../data/raw_data/S2File.sav\") %&gt;%\n    write_csv(\"../data/s2file.csv\")\n\nNow that we have collected an ample amount of data, we can move on to data cleaning."
  },
  {
    "objectID": "introduction/introduction.html",
    "href": "introduction/introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "As the stigma surrounding the usage of recreational drugs has lessened in recent years, many have called for the legalization of some illicit substances. In particular, law makers and lobbyists have considered the legalization of cannabis, also known as marijuana, due to its prevalence, medicinal properties, and low addictiveness. In fact, while cannabis is still illegal at the federal level, 23 states and D.C. have fully legalized marijuana for medical and recreational purposes, and 38 states have legalized marijuana usage for medical purposes(DISA n.d.).\n(DISA n.d.)\nHowever, there is also a growing body of research that indicates that cannabis usage can be linked to episodes of psychosis and trigger the development of schizophrenia. Research has show that there is “an association between cannabis use and psychosis”, but the question about causation remains open (Ganesh and D’Souza 2022). Increasingly, people are starting to use cannabis at a younger age and with more frequency, resulting in more questions about the health impact of such usage (Ganesh and D’Souza 2022). Furthermore, there has been a rising level of THC, the active ingredient in cannabis, in recent years. The average potency of THC in cannabis has increased from 3% in the 1960’s to 16% in England(Murray et al. 2016). Research has “robustly deomnstrated that THC can transiently induce clinically relevant acute schizophrenia-like symptoms”, meaning that THC has been show to cause episodes of psychosis (Ganesh and D’Souza 2022). On the other hand, genetic research has indicated that schizophrenia may be the causation of cannabis usage, leading to continue debate on the causality of this relationship (Ganesh and D’Souza 2022). A meta analysis of relevant research supports the possibility of “cannabis causing psychosis”, but the approach has limitations (deepak 9). It is also notable that cannabis use alone is “neither necessary nor sufficient for psychosis”, meaning other factors such as genetics are relevant for the development of psychosis (Ganesh and D’Souza 2022). Ganesh and D’Souza note that the increasing legalization and usage of cannabis must be studied closely, as it will elucidate whether there is a causational relationship between marijuana and psychosis.\nRelated to the usage of cannabis is the growing use of cannabinoids, substances that are closely related to cannabis. Examples of cannnabinoids include synthetic THC and CBD which have become prevalent through a “regulatory loophold” in 2018 (Dotson et al. 2022). Often, these substances are sold in very high concentrations that can lead to unknown risks. Research has shown tha “cannabis use was associated with an odds ratio of 1.4 for the development of schizophrenia”, and there is evidence for a risk of addiction to THC similar to other illicit substances. Even more concerning, research in Europe also indicates that rising THC levels and rising “first episode pyschosis admissions” to hospitals have coincided (Dotson et al. 2022).\nIn the U.S., approaches such as “vaping” and “dabbing” allow users to ingest THC in concentrations of up to 80% (Murray et al. 2016). These synthetic cannabinoids are much more dangerous that cannabis, increasing the risk of hospitalization by 30% (Murray et al. 2016). The first scientific study on the connection between cannabis and psychosis was led by Andreasson in 1987, and longitudinal studies have indicated an association between cannabis and psychosis, though not all results have been statistically significant (Murray et al. 2016). There is evidence that genetic predisposition plays a role in the development of psychosis and can be trigger by cannabis usage, but it is unclear if cannabis and psychosis are associated without genetic predisposition (Murray et al. 2016).\n\n\n\nThe increasing legalization of cannabis despite research indicating an association between cannabis and psychosis has motivated me to perform data-driven research on the correlation between cannabis usage and the development of psychosis. Some motivating questions include:\n\nIs cannabis usage a predictor of the development of a psychotic episode?\nIs cannabis usage a predictor of the development of a psychotic disorder, such as schizophrenia?\nHow does THC content contribute to the potential development of psychosis?\nHas the legalization of cannabis contributed to an increase in the prevelance of psychotic disorders?\nWhat role do genetics play in the development of a psychotic disorder? How does cannabis usage impact genetics?\nHow does age impact the development of psychosis due to cannabis usage?\nHow does frequency of cannabis usage impact the potential development of psychosis? The number of psychotic episodes?\nWhat is the public sentiment regarding cannabis?\nDoes public sentiment indicate an awareness of the dangers of cannabis, especially as related to psychosis?\nWhat is the public sentiment regarding cannabinoids such as CBD and THC products?\n\nOverall, the goal of this research is two-fold: (1) to perform a thorough, analytic analysis to assess the causational role of cannabis in the development of psychosis; and (2) gain insights into why legalization efforts have continued despite the severity of known risks associated with cannabis usage. My hypotheses are that cannabis does play a causal role in psychosis and development of schizophrenia and that public knowledge on the risks of cannabis usage have been poorly communicated and understated."
  },
  {
    "objectID": "introduction/introduction.html#survey-of-existing-research",
    "href": "introduction/introduction.html#survey-of-existing-research",
    "title": "Introduction",
    "section": "",
    "text": "As the stigma surrounding the usage of recreational drugs has lessened in recent years, many have called for the legalization of some illicit substances. In particular, law makers and lobbyists have considered the legalization of cannabis, also known as marijuana, due to its prevalence, medicinal properties, and low addictiveness. In fact, while cannabis is still illegal at the federal level, 23 states and D.C. have fully legalized marijuana for medical and recreational purposes, and 38 states have legalized marijuana usage for medical purposes(DISA n.d.).\n(DISA n.d.)\nHowever, there is also a growing body of research that indicates that cannabis usage can be linked to episodes of psychosis and trigger the development of schizophrenia. Research has show that there is “an association between cannabis use and psychosis”, but the question about causation remains open (Ganesh and D’Souza 2022). Increasingly, people are starting to use cannabis at a younger age and with more frequency, resulting in more questions about the health impact of such usage (Ganesh and D’Souza 2022). Furthermore, there has been a rising level of THC, the active ingredient in cannabis, in recent years. The average potency of THC in cannabis has increased from 3% in the 1960’s to 16% in England(Murray et al. 2016). Research has “robustly deomnstrated that THC can transiently induce clinically relevant acute schizophrenia-like symptoms”, meaning that THC has been show to cause episodes of psychosis (Ganesh and D’Souza 2022). On the other hand, genetic research has indicated that schizophrenia may be the causation of cannabis usage, leading to continue debate on the causality of this relationship (Ganesh and D’Souza 2022). A meta analysis of relevant research supports the possibility of “cannabis causing psychosis”, but the approach has limitations (deepak 9). It is also notable that cannabis use alone is “neither necessary nor sufficient for psychosis”, meaning other factors such as genetics are relevant for the development of psychosis (Ganesh and D’Souza 2022). Ganesh and D’Souza note that the increasing legalization and usage of cannabis must be studied closely, as it will elucidate whether there is a causational relationship between marijuana and psychosis.\nRelated to the usage of cannabis is the growing use of cannabinoids, substances that are closely related to cannabis. Examples of cannnabinoids include synthetic THC and CBD which have become prevalent through a “regulatory loophold” in 2018 (Dotson et al. 2022). Often, these substances are sold in very high concentrations that can lead to unknown risks. Research has shown tha “cannabis use was associated with an odds ratio of 1.4 for the development of schizophrenia”, and there is evidence for a risk of addiction to THC similar to other illicit substances. Even more concerning, research in Europe also indicates that rising THC levels and rising “first episode pyschosis admissions” to hospitals have coincided (Dotson et al. 2022).\nIn the U.S., approaches such as “vaping” and “dabbing” allow users to ingest THC in concentrations of up to 80% (Murray et al. 2016). These synthetic cannabinoids are much more dangerous that cannabis, increasing the risk of hospitalization by 30% (Murray et al. 2016). The first scientific study on the connection between cannabis and psychosis was led by Andreasson in 1987, and longitudinal studies have indicated an association between cannabis and psychosis, though not all results have been statistically significant (Murray et al. 2016). There is evidence that genetic predisposition plays a role in the development of psychosis and can be trigger by cannabis usage, but it is unclear if cannabis and psychosis are associated without genetic predisposition (Murray et al. 2016)."
  },
  {
    "objectID": "introduction/introduction.html#data-science-research-project",
    "href": "introduction/introduction.html#data-science-research-project",
    "title": "Introduction",
    "section": "",
    "text": "The increasing legalization of cannabis despite research indicating an association between cannabis and psychosis has motivated me to perform data-driven research on the correlation between cannabis usage and the development of psychosis. Some motivating questions include:\n\nIs cannabis usage a predictor of the development of a psychotic episode?\nIs cannabis usage a predictor of the development of a psychotic disorder, such as schizophrenia?\nHow does THC content contribute to the potential development of psychosis?\nHas the legalization of cannabis contributed to an increase in the prevelance of psychotic disorders?\nWhat role do genetics play in the development of a psychotic disorder? How does cannabis usage impact genetics?\nHow does age impact the development of psychosis due to cannabis usage?\nHow does frequency of cannabis usage impact the potential development of psychosis? The number of psychotic episodes?\nWhat is the public sentiment regarding cannabis?\nDoes public sentiment indicate an awareness of the dangers of cannabis, especially as related to psychosis?\nWhat is the public sentiment regarding cannabinoids such as CBD and THC products?\n\nOverall, the goal of this research is two-fold: (1) to perform a thorough, analytic analysis to assess the causational role of cannabis in the development of psychosis; and (2) gain insights into why legalization efforts have continued despite the severity of known risks associated with cannabis usage. My hypotheses are that cannabis does play a causal role in psychosis and development of schizophrenia and that public knowledge on the risks of cannabis usage have been poorly communicated and understated."
  },
  {
    "objectID": "clustering/clustering.html",
    "href": "clustering/clustering.html",
    "title": "Clustering",
    "section": "",
    "text": "Build out your website tab for “clustering”"
  }
]