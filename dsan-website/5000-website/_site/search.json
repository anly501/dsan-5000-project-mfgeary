[
  {
    "objectID": "naive-bayes/naive-bayes.html",
    "href": "naive-bayes/naive-bayes.html",
    "title": "Naïve Bayes",
    "section": "",
    "text": "In this next section of the project, I will be using the Naïve Bayes algorithm to classify both textual and numerical data. The Naïve Bayes method is an algorithm based on Bayes’ Theorem, which states that:\n\\[P(Y|X) = \\frac{P(X|Y)P(Y)}{P(X)}.\\]\nFrom here we also know that\n\\[P(Y|X) \\propto P(X|Y)P(Y).\\]\nPut differently, this means that the probability of \\(Y\\) given \\(X\\) is proportional to the probability of \\(X\\) given \\(Y\\) times the probability of \\(Y\\).\nGenerally, machine learning algorithms aim to find \\(P(Y|X)\\), that is, to predict \\(Y\\) given all features \\(X\\). This is called discriminative learning. Naïve Bayes is different because it aims to do the opposite: find \\(P(X|Y)\\) and \\(P(Y)\\) in order to predict \\(Y\\). We call this generative learning.\nNaïve Bayes has the moniker “naïve” because it relies on a naïve assumption that is generally not true in practice. It assumes that all features in the data are independent of one another. While this assumption is rarely true, we can still get good results by using the Naïve Bayes model.\nNaïve Bayes works for numeric and textual data. For numeric data, we use a Gaussian Naïve Bayes model. Gaussian Naïve Bayes works by using a normal distribution to model the data. The model then calculates the probability of the input value given each class, and the class with the highest probability is returned as the prediction.\nWe use a Bernoulli Naïve Bayes (binary target) or Multinomial Naïve Bayes (multiclass target) model for factor data. Multinomial Naïve Bayes makes predictions by calculating the probability of the input value given each class. The class with the highest probability is then returned as the prediction.\nIn this section, I will use a Multinomial Naïve Bayes model to classify text data to determine which of the subreddits it belongs to out of r/Psychosis, r/schizophrenia, and r/weed. For the record data, I will use a Bernoulli Naïve Bayes to predict whether an individual uses cannabis based on their mental health data."
  },
  {
    "objectID": "naive-bayes/naive-bayes.html#introduction-to-naïve-bayes",
    "href": "naive-bayes/naive-bayes.html#introduction-to-naïve-bayes",
    "title": "Naïve Bayes",
    "section": "",
    "text": "In this next section of the project, I will be using the Naïve Bayes algorithm to classify both textual and numerical data. The Naïve Bayes method is an algorithm based on Bayes’ Theorem, which states that:\n\\[P(Y|X) = \\frac{P(X|Y)P(Y)}{P(X)}.\\]\nFrom here we also know that\n\\[P(Y|X) \\propto P(X|Y)P(Y).\\]\nPut differently, this means that the probability of \\(Y\\) given \\(X\\) is proportional to the probability of \\(X\\) given \\(Y\\) times the probability of \\(Y\\).\nGenerally, machine learning algorithms aim to find \\(P(Y|X)\\), that is, to predict \\(Y\\) given all features \\(X\\). This is called discriminative learning. Naïve Bayes is different because it aims to do the opposite: find \\(P(X|Y)\\) and \\(P(Y)\\) in order to predict \\(Y\\). We call this generative learning.\nNaïve Bayes has the moniker “naïve” because it relies on a naïve assumption that is generally not true in practice. It assumes that all features in the data are independent of one another. While this assumption is rarely true, we can still get good results by using the Naïve Bayes model.\nNaïve Bayes works for numeric and textual data. For numeric data, we use a Gaussian Naïve Bayes model. Gaussian Naïve Bayes works by using a normal distribution to model the data. The model then calculates the probability of the input value given each class, and the class with the highest probability is returned as the prediction.\nWe use a Bernoulli Naïve Bayes (binary target) or Multinomial Naïve Bayes (multiclass target) model for factor data. Multinomial Naïve Bayes makes predictions by calculating the probability of the input value given each class. The class with the highest probability is then returned as the prediction.\nIn this section, I will use a Multinomial Naïve Bayes model to classify text data to determine which of the subreddits it belongs to out of r/Psychosis, r/schizophrenia, and r/weed. For the record data, I will use a Bernoulli Naïve Bayes to predict whether an individual uses cannabis based on their mental health data."
  },
  {
    "objectID": "naive-bayes/naive-bayes.html#data-preparation",
    "href": "naive-bayes/naive-bayes.html#data-preparation",
    "title": "Naïve Bayes",
    "section": "Data Preparation",
    "text": "Data Preparation\nNow we prepare the data so that we can make predictions using the Naïve Bayes model.\n\n\nCode\nimport pandas as pd\n\nreddit_text = pd.read_csv('../data/clean_data/reddit_cleaned_text.csv')\nfull_data = pd.read_csv('../data/clean_data/full_data.csv')\n\n\nAll of our data is already labeled as is required. The text data has already been cleaned, with each entry representing one post of Reddit.\nOur record data is mostly clean as well. Let’s normalize everything so that no feature has an outsized impact on our model.\n\n\nCode\n# scale all columns to be between 0 and 1\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nfull_data = full_data.apply(lambda x: pd.Series(scaler.fit_transform(x.values.reshape(-1,1)).reshape(-1), index=x.index))\n\n\n/Users/mariongeary/anaconda3/envs/dsan5000/lib/python3.11/site-packages/sklearn/preprocessing/_data.py:473: RuntimeWarning: All-NaN slice encountered\n  data_min = np.nanmin(X, axis=0)\n/Users/mariongeary/anaconda3/envs/dsan5000/lib/python3.11/site-packages/sklearn/preprocessing/_data.py:474: RuntimeWarning: All-NaN slice encountered\n  data_max = np.nanmax(X, axis=0)\n\n\n\n\nCode\nfull_data = full_data.drop(['cds0'], axis=1).dropna()\nfull_data.head()\n\n\n\n\n\n\n\n\n\ncode\nProgram\nsex\nage_psychosis\nfamily_history\nhospita\ndui\ndup\nlevelsocioeco\nurbanarea\n...\nunemployed\nyears_edu\nCannabisBinary\nSAPS0\nPsychoticdim0\nDisorganizeddim0\nNegativedimen0\ndasgl0\ndiagnosis\ninsight\n\n\n\n\n0\n0.000000\n0.0\n1.0\n0.163471\n1.0\n1.0\n0.006600\n0.003918\n0.0\n0.0\n...\n1.0\n0.545455\n1.0\n0.208333\n0.4\n0.133333\n0.25\n0.4\n1.0\n0.0\n\n\n1\n0.001305\n0.0\n0.0\n0.396282\n0.0\n1.0\n0.001389\n0.001834\n1.0\n0.0\n...\n1.0\n0.818182\n1.0\n0.333333\n0.8\n0.066667\n0.20\n0.0\n1.0\n0.0\n\n\n3\n0.003916\n0.0\n0.0\n0.214728\n1.0\n1.0\n0.062174\n0.016421\n1.0\n0.0\n...\n1.0\n0.545455\n1.0\n0.541667\n0.8\n0.400000\n0.35\n0.0\n0.0\n1.0\n\n\n4\n0.005222\n0.0\n0.0\n0.146080\n1.0\n0.0\n0.096909\n0.049762\n0.0\n0.0\n...\n0.0\n0.181818\n1.0\n0.416667\n0.4\n0.466667\n0.90\n0.4\n0.0\n1.0\n\n\n5\n0.006527\n0.0\n0.0\n0.048563\n1.0\n0.0\n0.003126\n0.003918\n1.0\n1.0\n...\n1.0\n0.181818\n1.0\n0.375000\n0.4\n0.400000\n0.25\n0.0\n1.0\n1.0\n\n\n\n\n5 rows × 22 columns\n\n\n\nNow that our data is clean, we can split our data into training, validation, and test sets. We will use the training set to train our model, the validation set to tune our model, and the test set to evaluate our model. We split the data into 3 sets to avoid overfitting. If we train the model on all of our data, it will be too specific to our data and will not generalize well to new data. Without test data, we will not be able to assess the performance of our model. We also use validation data to tune our model by assessing hyperparameters based on data the model has not seen before.\n\n\nCode\n## Split data into training, validation, and test sets\n# 80% training, 10% validation, 10% test\ntrain_full = full_data.sample(frac=0.8, random_state=200)\ntest_full = full_data.drop(train_full.index)\nvalidate_full = test_full.sample(frac=0.5, random_state=200)\ntest_full = test_full.drop(validate_full.index)\n\n\n\n\nCode\n## Split data into training, validation, and test sets\n# 80% training, 10% validation, 10% test\ntrain_reddit = reddit_text.sample(frac=0.8, random_state=200)\ntest_reddit = reddit_text.drop(train_reddit.index)\nvalidate_reddit = test_reddit.sample(frac=0.5, random_state=200)\ntest_reddit = test_reddit.drop(validate_reddit.index)"
  },
  {
    "objectID": "naive-bayes/naive-bayes.html#feature-selection-for-record-data",
    "href": "naive-bayes/naive-bayes.html#feature-selection-for-record-data",
    "title": "Naïve Bayes",
    "section": "Feature Selection for Record Data",
    "text": "Feature Selection for Record Data\n\n\nCode\nimport numpy as np\n\ny = train_full['CannabisBinary']\ny = np.array(y)\n\n\n\n\nCode\nx = train_full.drop(['CannabisBinary', 'code', 'Program'], axis=1).to_numpy()\nx_test = test_full.drop(['CannabisBinary', 'code', 'Program'], axis=1).to_numpy()\nx_validate = validate_full.drop(['CannabisBinary', 'code', 'Program'], axis=1).to_numpy()\n\n\n\n\nCode\ny_test = test_full['CannabisBinary']\ny_validate = validate_full['CannabisBinary']\ny_test = np.array(y_test)\ny_validate = np.array(y_validate)\n\n\n\n\nCode\nx = full_data.drop(['CannabisBinary', 'code', 'Program'], axis=1).to_numpy()\ny = full_data['CannabisBinary'].to_numpy()\n\n\n\n\nCode\nimport random\nN=x.shape[0]\nl = [*range(N)]     # indices\ncut = int(0.8 * N) #80% of the list\nrandom.shuffle(l)   # randomize\ntrain_index = l[:cut] # first 80% of shuffled list\ntest_index = l[cut:] # last 20% of shuffled list\n\nprint(train_index[0:10])\nprint(test_index[0:10])\n\n\n[125, 395, 203, 166, 197, 90, 95, 88, 69, 108]\n[469, 32, 311, 433, 33, 514, 153, 112, 259, 497]\n\n\n\n\nCode\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score\nimport time\n\ndef train_MNB_model(X, Y, i_print=False):\n\n    if(i_print):\n        print(X.shape,Y.shape)\n\n    #SPLIT\n    x_train=X[train_index]\n    y_train=Y[train_index].flatten()\n\n    x_test=X[test_index]\n    y_test=Y[test_index].flatten()\n\n    # INITIALIZE MODEL \n    model = MultinomialNB()\n\n    # TRAIN MODEL \n    start = time.process_time()\n    model.fit(x_train,y_train)\n    time_train=time.process_time() - start\n\n    # LABEL PREDICTIONS FOR TRAINING AND TEST SET \n    start = time.process_time()\n    yp_train = model.predict(x_train)\n    yp_test = model.predict(x_test)\n    time_eval=time.process_time() - start\n\n    acc_train= accuracy_score(y_train, yp_train)*100\n    acc_test= accuracy_score(y_test, yp_test)*100\n\n    if(i_print):\n        print(acc_train,acc_test,time_train,time_eval)\n\n    return (acc_train,acc_test,time_train,time_eval)\n\n\n\n\nCode\n##UTILITY FUNCTION TO INITIALIZE RELEVANT ARRAYS\ndef initialize_arrays():\n    global num_features,train_accuracies\n    global test_accuracies,train_time,eval_time\n    num_features=[]\n    train_accuracies=[]\n    test_accuracies=[]\n    train_time=[]\n    eval_time=[]\n# INITIALIZE ARRAYS\ninitialize_arrays()\n\n# DEFINE SEARCH FUNCTION\ndef partial_grid_search(num_runs, min_index, max_index):\n    for i in range(1, num_runs+1):\n        # SUBSET FEATURES \n        upper_index=min_index+i*int((max_index-min_index)/num_runs)\n        xtmp=x[:,0:upper_index]\n\n        #TRAIN \n        (acc_train,acc_test,time_train,time_eval)=train_MNB_model(xtmp,y,i_print=False)\n\n        if(i%1==0):\n            print(i,upper_index,xtmp.shape[1],acc_train,acc_test)\n            \n        #RECORD \n        num_features.append(xtmp.shape[1])\n        train_accuracies.append(acc_train)\n        test_accuracies.append(acc_test)\n        train_time.append(time_train)\n        eval_time.append(time_eval)\n\n# DENSE SEARCH (SMALL NUMBER OF FEATURES (FAST))\npartial_grid_search(num_runs=19, min_index=0, max_index=19)\n\n# # SPARSE SEARCH (LARGE NUMBER OF FEATURES (SLOWER))\n# partial_grid_search(num_runs=20, min_index=1000, max_index=10000)\n\n\n1 1 1 56.324582338902154 59.04761904761905\n2 2 2 55.60859188544153 59.04761904761905\n3 3 3 67.78042959427208 64.76190476190476\n4 4 4 70.64439140811456 67.61904761904762\n5 5 5 70.88305489260142 68.57142857142857\n6 6 6 70.64439140811456 69.52380952380952\n7 7 7 63.48448687350835 68.57142857142857\n8 8 8 65.63245823389022 63.8095238095238\n9 9 9 65.39379474940334 62.857142857142854\n10 10 10 67.5417661097852 69.52380952380952\n11 11 11 67.30310262529832 65.71428571428571\n12 12 12 67.78042959427208 67.61904761904762\n13 13 13 69.21241050119332 69.52380952380952\n14 14 14 71.1217183770883 70.47619047619048\n15 15 15 71.36038186157518 71.42857142857143\n16 16 16 71.36038186157518 73.33333333333333\n17 17 17 71.83770883054893 73.33333333333333\n18 18 18 71.36038186157518 73.33333333333333\n19 19 19 70.88305489260142 70.47619047619048\n\n\n\n\nCode\n#UTILITY FUNCTION TO SAVE RESULTS\ndef save_results(path_root):\n    out=np.transpose(np.array([num_features,train_accuracies,test_accuracies,train_time,eval_time])) \n    out=pd.DataFrame(out)\n    out.to_csv(path_root+\".csv\")\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\n\n#UTILITY FUNCTION TO PLOT RESULTS\ndef plot_results(path_root):\n\n    #PLOT-1\n    plt.plot(num_features,train_accuracies,'-or')\n    plt.plot(num_features,test_accuracies,'-ob')\n    plt.xlabel('Number of features')\n    plt.ylabel('ACCURACY: Training (blue) and Test (red)')\n    plt.savefig(path_root+'-1.png')\n    plt.show()\n\n    # #PLOT-2\n    plt.plot(num_features,train_time,'-or')\n    plt.plot(num_features,eval_time,'-ob')\n    plt.xlabel('Number of features')\n    plt.ylabel('Runtime: training time (red) and evaluation time(blue)')\n    plt.savefig(path_root+'-2.png')\n    plt.show()\n\n    # #PLOT-3\n    plt.plot(np.array(test_accuracies),train_time,'-or')\n    plt.plot(np.array(test_accuracies),eval_time,'-ob')\n    plt.xlabel('test_accuracies')\n    plt.ylabel('Runtime: training time (red) and evaluation time (blue)')\n    plt.savefig(path_root+'-3.png')\n    plt.show()\n\n    # #PLOT-3\n    plt.plot(num_features,np.array(train_accuracies)-np.array(test_accuracies),'-or')\n    plt.xlabel('Number of features')\n    plt.ylabel('train_accuracies-test_accuracies')\n    plt.savefig(path_root+'-4.png')\n    plt.show()\n\n\n\n\nCode\noutput_dir = \"../results/naive-bayes/\"\nsave_results(output_dir+\"/partial_grid_search\")\nplot_results(output_dir+\"/partial_grid_search\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# INITIALIZE ARRAYS\ninitialize_arrays()\n\n# FIRST FEATURE \nfeatures=[random.randint(0, x.shape[1]-1)]; #print(features)\nmax_iter=19\n\n# STORE ATTEMPTS\nalready_tried=[]\n\nfor iter in range(0,max_iter):\n    if iter%1==0: print(iter,len(features),len(already_tried))\n    if(iter==0):\n        xtmp=x[:,features]\n        (acc_train,acc_test,time_train,time_eval)=train_MNB_model(xtmp,y,i_print=False)\n        acc_train_im1=acc_train\n    else:\n        didnt_find=True\n        while(didnt_find or len(features)+len(already_tried)&gt;0.8*x.shape[1]):\n            # NEW TRIAL FEATURE  \n            new_feature=random.randint(0, x.shape[1]-1)\n            if new_feature not in features and new_feature not in already_tried:\n                # ADD NEW TRIAL FEATURE FROM LIST\n                features.append(new_feature); \n                # FEATURE SUBSET\n                xtmp=x[:,features]\n\n                # TRAIN \n                (acc_train,acc_test,time_train,time_eval)=train_MNB_model(xtmp,y,i_print=False)\n\n                # ACCEPTANCE CRITERION \n                if(acc_train-acc_train_im1&gt;0):\n                    #print(features)\n                    acc_train_im1=acc_train                    \n                    #RECORD \n                    num_features.append(xtmp.shape[1])\n                    train_accuracies.append(acc_train)\n                    test_accuracies.append(acc_test)\n                    train_time.append(time_train)\n                    eval_time.append(time_eval)\n\n                    didnt_find=False\n                else:\n                    # RECORD ATTEMPT\n                    already_tried.append(new_feature)\n\n                    # REMOVE NEW TRIAL FEATURE FROM LIST\n                    features.pop()\n\n\n0 1 0\n1 1 0\n2 2 8\n3 3 9\n4 4 9\n5 5 9\n6 6 9\n\n\nKeyboardInterrupt: \n\n\n\n\nCode\n# CHECK RESULTS \nsave_results(output_dir+\"/random_search\")\nplot_results(output_dir+\"/random_search\")\n\n\n\n\nCode\nx_var=np.var(x,axis=0)\nprint(np.min(x_var))\nprint(np.max(x_var))\n\n\n0.013777621322713766\n0.24996722218984915\n\n\n\n\nCode\nfrom sklearn.feature_selection import VarianceThreshold\n\n# DEFINE GRID OF THRESHOLDS \nnum_thresholds=30\nthresholds=np.linspace(np.min(x_var),np.max(x_var),num_thresholds)\n\n#DOESN\"T WORK WELL WITH EDGE VALUES \nthresholds=thresholds[1:-2]; #print(thresholds)\n\n# INITIALIZE ARRAYS\ninitialize_arrays()\n\n# SEARCH FOR OPTIMAL THRESHOLD\nfor THRESHOLD in thresholds:\n    feature_selector = VarianceThreshold(threshold=THRESHOLD)\n    xtmp=feature_selector.fit_transform(x)\n    print(\"THRESHOLD =\",THRESHOLD, xtmp.shape[1])\n\n    (acc_train,acc_test,time_train,time_eval)=train_MNB_model(xtmp,y,i_print=False)\n             \n    #RECORD \n    num_features.append(xtmp.shape[1])\n    train_accuracies.append(acc_train)\n    test_accuracies.append(acc_test)\n    train_time.append(time_train)\n    eval_time.append(time_eval)\n\n\nTHRESHOLD = 0.021922090318132227 17\nTHRESHOLD = 0.03006655931355069 17\nTHRESHOLD = 0.03821102830896915 16\nTHRESHOLD = 0.04635549730438761 15\nTHRESHOLD = 0.054499966299806066 14\nTHRESHOLD = 0.06264443529522454 13\nTHRESHOLD = 0.070788904290643 13\nTHRESHOLD = 0.07893337328606145 13\nTHRESHOLD = 0.08707784228147991 10\nTHRESHOLD = 0.09522231127689837 10\nTHRESHOLD = 0.10336678027231684 10\nTHRESHOLD = 0.1115112492677353 10\nTHRESHOLD = 0.11965571826315376 10\nTHRESHOLD = 0.12780018725857223 10\nTHRESHOLD = 0.1359446562539907 10\nTHRESHOLD = 0.14408912524940914 10\nTHRESHOLD = 0.1522335942448276 10\nTHRESHOLD = 0.16037806324024606 10\nTHRESHOLD = 0.16852253223566452 10\nTHRESHOLD = 0.17666700123108298 10\nTHRESHOLD = 0.18481147022650146 9\nTHRESHOLD = 0.19295593922191992 9\nTHRESHOLD = 0.20110040821733838 8\nTHRESHOLD = 0.20924487721275684 7\nTHRESHOLD = 0.2173893462081753 6\nTHRESHOLD = 0.22553381520359375 5\nTHRESHOLD = 0.2336782841990122 5\n\n\n\n\nCode\n# CHECK RESULTS \nsave_results(output_dir+\"/variance_threshold\")\nplot_results(output_dir+\"/variance_threshold\")"
  },
  {
    "objectID": "naive-bayes/naive-bayes.html#feature-selection-for-text-data",
    "href": "naive-bayes/naive-bayes.html#feature-selection-for-text-data",
    "title": "Naïve Bayes",
    "section": "Feature Selection for Text Data",
    "text": "Feature Selection for Text Data\n\n\nCode\n# convert string labels in reddit_text.label to integers\n# 0 = weed, 1 = psychosis, 2 = schizophrenia\nreddit_text.dropna(inplace=True)\nreddit_text['label'] = reddit_text['label'].replace(['weed', 'Psychosis', 'schizophrenia'], [0, 1, 2])\ny = reddit_text['label'].to_numpy()\n\n\n\n\nCode\n# PARAMETERS TO CONTROL SIZE OF FEATURE SPACE WITH COUNT-VECTORIZER\n# minDF = 0.01 means \"ignore terms that appear in less than 1% of the documents\". \n# minDF = 5 means \"ignore terms that appear in less than 5 documents\".\n# max_features=int, default=None\n#   If not None, build a vocabulary that only consider the top max_features ordered by term frequency across the corpus.\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ndef vectorize(corpus,MAX_FEATURES):\n    vectorizer=CountVectorizer(max_features=MAX_FEATURES,stop_words=\"english\")   \n    # RUN COUNT VECTORIZER ON OUR COURPUS \n    Xs  =  vectorizer.fit_transform(corpus)   \n    X=np.array(Xs.todense())\n    #CONVERT TO ONE-HOT VECTORS (can also be done with binary=true in CountVectorizer)\n    maxs=np.max(X,axis=0)\n    return (np.ceil(X/maxs),vectorizer.vocabulary_)\n\n(x,vocab0)=vectorize(reddit_text['text'],MAX_FEATURES=10000)\n\n\n\n\nCode\n#swap keys and values (value --&gt; key)\nvocab1 = dict([(value, key) for key, value in vocab0.items()])\n#RE-ORDER COLUMN SO IT IS SORTED FROM HIGH FREQ TERMS TO LOW \n# https://stackoverflow.com/questions/60758625/sort-pandas-dataframe-by-sum-of-columns\ndf2=pd.DataFrame(x)\ns = df2.sum(axis=0)\ndf2=df2[s.sort_values(ascending=False).index[:]]\nprint(df2.head())\n\n\n   937   1099  791   495   311   1433  1600  1011  1299  1442  ...  644   \\\n0   1.0   1.0   0.0   0.0   1.0   0.0   1.0   0.0   0.0   1.0  ...   0.0   \n1   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n2   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n3   0.0   0.0   0.0   1.0   0.0   1.0   0.0   0.0   0.0   0.0  ...   0.0   \n4   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n\n   629   643   641   639   638   636   635   632   1609  \n0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n1   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n2   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n3   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n4   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n\n[5 rows x 1610 columns]\n\n\n\n\nCode\n# REMAP DICTIONARY TO CORRESPOND TO NEW COLUMN NUMBERS\nprint()\ni1=0\nvocab2={}\nfor i2 in list(df2.columns):\n    # print(i2)\n    vocab2[i1]=vocab1[int(i2)]\n    i1+=1\n\n#DOUBLE CHECK \nprint(vocab2[0],vocab1[643])\nprint(vocab2[1],vocab1[2])\n\n\n\nnumbr hopeful\npsychosis absolutely\n\n\n\n\nCode\n# RENAME COLUMNS 0,1,2,3 .. \ndf2.columns = range(df2.columns.size)\nprint(df2.head())\nprint(df2.sum(axis=0))\nx=df2.to_numpy()\n\n\n   0     1     2     3     4     5     6     7     8     9     ...  1600  \\\n0   1.0   1.0   0.0   0.0   1.0   0.0   1.0   0.0   0.0   1.0  ...   0.0   \n1   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n2   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n3   0.0   0.0   0.0   1.0   0.0   1.0   0.0   0.0   0.0   0.0  ...   0.0   \n4   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n\n   1601  1602  1603  1604  1605  1606  1607  1608  1609  \n0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n1   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n2   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n3   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n4   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n\n[5 rows x 1610 columns]\n0       4500.0\n1       3800.0\n2       3800.0\n3       3100.0\n4       2700.0\n         ...  \n1605     100.0\n1606     100.0\n1607     100.0\n1608     100.0\n1609     100.0\nLength: 1610, dtype: float64\n\n\n\n\nCode\nimport random\nN=x.shape[0]\nl = [*range(N)]     # indices\ncut = int(0.8 * N) #80% of the list\nrandom.shuffle(l)   # randomize\ntrain_index = l[:cut] # first 80% of shuffled list\ntest_index = l[cut:] # last 20% of shuffled list\n\nprint(train_index[0:10])\nprint(test_index[0:10])\n\n\n[26323, 23215, 16763, 2468, 15662, 12383, 19344, 4685, 29639, 2583]\n[3293, 24500, 28662, 13071, 1827, 2764, 10425, 14129, 14017, 12574]\n\n\n\n\nCode\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score\nimport time\n\ndef train_MNB_model(X,Y,i_print=False):\n\n    if(i_print):\n        print(X.shape,Y.shape)\n\n    #SPLIT\n    x_train=X[train_index]\n    y_train=Y[train_index].flatten()\n\n    x_test=X[test_index]\n    y_test=Y[test_index].flatten()\n\n    # INITIALIZE MODEL \n    model = MultinomialNB()\n\n    # TRAIN MODEL \n    start = time.process_time()\n    model.fit(x_train,y_train)\n    time_train=time.process_time() - start\n\n    # LABEL PREDICTIONS FOR TRAINING AND TEST SET \n    start = time.process_time()\n    yp_train = model.predict(x_train)\n    yp_test = model.predict(x_test)\n    time_eval=time.process_time() - start\n\n    acc_train= accuracy_score(y_train, yp_train)*100\n    acc_test= accuracy_score(y_test, yp_test)*100\n\n    if(i_print):\n        print(acc_train,acc_test,time_train,time_eval)\n\n    return (acc_train,acc_test,time_train,time_eval)\n\n\n#TEST\nprint(type(x),type(y))\nprint(x.shape,y.shape)\n(acc_train,acc_test,time_train,time_eval)=train_MNB_model(x,y,i_print=True)\n\n\n&lt;class 'numpy.ndarray'&gt; &lt;class 'numpy.ndarray'&gt;\n(29700, 1610) (29700,)\n(29700, 1610) (29700,)\n91.91077441077441 91.95286195286195 17.717603000000054 0.4340500000002976\n\n\n\n\nCode\n# INITIALIZE ARRAYS\ninitialize_arrays()\n# DENSE SEARCH (SMALL NUMBER OF FEATURES (FAST))\npartial_grid_search(num_runs=100, min_index=0, max_index=1000)\n\n\n1 10 10 53.26599326599326 54.612794612794616\n2 20 20 61.30050505050505 62.878787878787875\n3 30 30 64.29292929292929 66.06060606060606\n4 40 40 65.94276094276094 67.87878787878789\n5 50 50 67.0496632996633 68.5016835016835\n6 60 60 66.38047138047139 67.8114478114478\n7 70 70 70.05892255892256 71.6161616161616\n8 80 80 71.82659932659932 72.96296296296296\n9 90 90 72.81986531986531 74.04040404040404\n10 100 100 73.16077441077441 74.36026936026936\n11 110 110 72.82828282828284 74.006734006734\n12 120 120 72.10437710437711 73.53535353535354\n13 130 130 72.18434343434343 73.21548821548821\n14 140 140 73.14393939393939 74.42760942760943\n15 150 150 73.12710437710437 74.4949494949495\n16 160 160 73.16498316498317 74.34343434343434\n17 170 170 74.52020202020202 75.65656565656566\n18 180 180 74.52020202020202 75.65656565656566\n19 190 190 74.53282828282828 75.60606060606061\n20 200 200 75.19781144781145 76.31313131313131\n21 210 210 75.19781144781145 76.31313131313131\n22 220 220 75.19781144781145 76.31313131313131\n23 230 230 75.53451178451178 76.64983164983164\n24 240 240 76.19107744107744 77.39057239057239\n25 250 250 76.88973063973063 77.96296296296296\n26 260 260 76.52356902356902 77.74410774410775\n27 270 270 76.52356902356902 77.74410774410775\n28 280 280 76.84764309764309 78.13131313131314\n29 290 290 77.1969696969697 78.41750841750842\n30 300 300 77.87878787878788 79.05723905723906\n31 310 310 78.58164983164984 79.61279461279462\n32 320 320 78.58164983164984 79.61279461279462\n33 330 330 78.58164983164984 79.61279461279462\n34 340 340 78.58164983164984 79.61279461279462\n35 350 350 78.58164983164984 79.61279461279462\n36 360 360 78.96464646464646 79.76430976430976\n37 370 370 79.6969696969697 80.2020202020202\n38 380 380 80.0378787878788 80.52188552188552\n39 390 390 80.41666666666667 80.6902356902357\n40 400 400 80.41666666666667 80.6902356902357\n41 410 410 80.76599326599326 80.97643097643098\n42 420 420 80.42087542087543 80.67340067340068\n43 430 430 80.77441077441078 80.94276094276094\n44 440 440 81.13636363636364 81.17845117845118\n45 450 450 81.13636363636364 81.17845117845118\n46 460 460 81.49410774410775 81.43097643097643\n47 470 470 81.49410774410775 81.43097643097643\n48 480 480 81.7929292929293 81.91919191919192\n49 490 490 81.7929292929293 81.91919191919192\n50 500 500 81.7929292929293 81.91919191919192\n51 510 510 82.11279461279462 82.32323232323232\n52 520 520 82.11279461279462 82.32323232323232\n53 530 530 82.11279461279462 82.32323232323232\n54 540 540 82.11279461279462 82.32323232323232\n55 550 550 82.11279461279462 82.32323232323232\n56 560 560 82.11279461279462 82.32323232323232\n57 570 570 82.11279461279462 82.32323232323232\n58 580 580 82.11279461279462 82.32323232323232\n59 590 590 82.11279461279462 82.32323232323232\n60 600 600 82.4621212121212 82.60942760942761\n61 610 610 82.4621212121212 82.60942760942761\n62 620 620 82.4621212121212 82.60942760942761\n63 630 630 82.16329966329965 82.12121212121211\n64 640 640 82.16329966329965 82.12121212121211\n65 650 650 82.7946127946128 82.96296296296296\n66 660 660 83.14814814814815 83.23232323232324\n67 670 670 83.82575757575758 83.88888888888889\n68 680 680 83.82575757575758 83.88888888888889\n69 690 690 83.82575757575758 83.88888888888889\n70 700 700 83.82575757575758 83.88888888888889\n71 710 710 84.17929292929293 84.15824915824915\n72 720 720 84.51178451178451 84.51178451178451\n73 730 730 84.82323232323232 84.94949494949495\n74 740 740 84.82323232323232 84.94949494949495\n75 750 750 84.82323232323232 84.94949494949495\n76 760 760 85.1557239057239 85.3030303030303\n77 770 770 85.1557239057239 85.3030303030303\n78 780 780 85.1557239057239 85.3030303030303\n79 790 790 85.1557239057239 85.3030303030303\n80 800 800 85.48821548821549 85.65656565656565\n81 810 810 85.48821548821549 85.65656565656565\n82 820 820 85.81228956228956 86.04377104377105\n83 830 830 85.81228956228956 86.04377104377105\n84 840 840 86.47306397306397 86.76767676767678\n85 850 850 86.47306397306397 86.76767676767678\n86 860 860 86.47306397306397 86.76767676767678\n87 870 870 86.47306397306397 86.76767676767678\n88 880 880 86.47306397306397 86.76767676767678\n89 890 890 86.47306397306397 86.76767676767678\n90 900 900 86.80555555555556 87.12121212121212\n91 910 910 86.46043771043772 86.81818181818181\n92 920 920 86.8013468013468 87.13804713804714\n93 930 930 87.15488215488215 87.4074074074074\n94 940 940 87.15488215488215 87.4074074074074\n95 950 950 87.15488215488215 87.4074074074074\n96 960 960 87.44949494949495 87.91245791245791\n97 970 970 87.44949494949495 87.91245791245791\n98 980 980 87.79461279461279 88.21548821548821\n99 990 990 87.79461279461279 88.21548821548821\n100 1000 1000 87.79461279461279 88.21548821548821\n\n\n\n\nCode\nsave_results(output_dir+\"/partial_grid_search_text\")\nplot_results(output_dir+\"/partial_grid_search_text\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# INITIALIZE ARRAYS\ninitialize_arrays()\n\n# FIRST FEATURE \nfeatures=[random.randint(0, x.shape[1])]; #print(features)\nmax_iter=500\n\n# STORE ATTEMPTS\nalready_tried=[]\n\nfor iter in range(0,max_iter):\n    if iter%50==0: print(iter,len(features),len(already_tried))\n    if(iter==0):\n        xtmp=x[:,features]\n        (acc_train,acc_test,time_train,time_eval)=train_MNB_model(xtmp,y,i_print=False)\n        acc_train_im1=acc_train\n    else:\n        didnt_find=True\n        while(didnt_find or len(features)+len(already_tried)&gt;0.8*x.shape[1]):\n            # NEW TRIAL FEATURE  \n            new_feature=random.randint(0, x.shape[1])\n            if new_feature not in features and new_feature not in already_tried:\n                # ADD NEW TRIAL FEATURE FROM LIST\n                features.append(new_feature); \n                # FEATURE SUBSET\n                xtmp=x[:,features]\n\n                # TRAIN \n                (acc_train,acc_test,time_train,time_eval)=train_MNB_model(xtmp,y,i_print=False)\n\n                # ACCEPTANCE CRITERION \n                if(acc_train-acc_train_im1&gt;0):\n                    #print(features)\n                    acc_train_im1=acc_train                    \n                    #RECORD \n                    num_features.append(xtmp.shape[1])\n                    train_accuracies.append(acc_train)\n                    test_accuracies.append(acc_test)\n                    train_time.append(time_train)\n                    eval_time.append(time_eval)\n\n                    didnt_find=False\n                else:\n                    # RECORD ATTEMPT\n                    already_tried.append(new_feature)\n\n                    # REMOVE NEW TRIAL FEATURE FROM LIST\n                    features.pop()\n\n\n\n\nCode\n# CHECK RESULTS \nsave_results(output_dir+\"/random_search_text\")\nplot_results(output_dir+\"/random_search_text\")\n\n\n\n\nCode\nx_var=np.var(x,axis=0)\nprint(np.min(x_var))\nprint(np.max(x_var))\n\n\n0.0033556666553305145\n0.1285583103765079\n\n\n\n\nCode\nfrom sklearn.feature_selection import VarianceThreshold\n\n# DEFINE GRID OF THRESHOLDS \nnum_thresholds=30\nthresholds=np.linspace(np.min(x_var),np.max(x_var),num_thresholds)\n\n#DOESN\"T WORK WELL WITH EDGE VALUES \nthresholds=thresholds[1:-2]; #print(thresholds)\n\n# INITIALIZE ARRAYS\ninitialize_arrays()\n\n# SEARCH FOR OPTIMAL THRESHOLD\nfor THRESHOLD in thresholds:\n    feature_selector = VarianceThreshold(threshold=THRESHOLD)\n    xtmp=feature_selector.fit_transform(x)\n    print(\"THRESHOLD =\",THRESHOLD, xtmp.shape[1])\n\n    (acc_train,acc_test,time_train,time_eval)=train_MNB_model(xtmp,y,i_print=False)\n             \n    #RECORD \n    num_features.append(xtmp.shape[1])\n    train_accuracies.append(acc_train)\n    test_accuracies.append(acc_test)\n    train_time.append(time_train)\n    eval_time.append(time_eval)\n\n\nTHRESHOLD = 0.00767299919744008 295\nTHRESHOLD = 0.011990331739549644 186\nTHRESHOLD = 0.01630766428165921 136\nTHRESHOLD = 0.020624996823768775 76\nTHRESHOLD = 0.02494232936587834 64\nTHRESHOLD = 0.029259661907987903 58\nTHRESHOLD = 0.033576994450097465 40\nTHRESHOLD = 0.03789432699220703 35\nTHRESHOLD = 0.042211659534316597 28\nTHRESHOLD = 0.04652899207642616 24\nTHRESHOLD = 0.05084632461853573 21\nTHRESHOLD = 0.05516365716064529 16\nTHRESHOLD = 0.05948098970275485 14\nTHRESHOLD = 0.06379832224486442 12\nTHRESHOLD = 0.068115654786974 10\nTHRESHOLD = 0.07243298732908356 6\nTHRESHOLD = 0.07675031987119312 6\nTHRESHOLD = 0.08106765241330269 5\nTHRESHOLD = 0.08538498495541225 4\nTHRESHOLD = 0.08970231749752182 4\nTHRESHOLD = 0.09401965003963138 3\nTHRESHOLD = 0.09833698258174095 3\nTHRESHOLD = 0.10265431512385051 3\nTHRESHOLD = 0.10697164766596007 3\nTHRESHOLD = 0.11128898020806964 3\nTHRESHOLD = 0.1156063127501792 1\nTHRESHOLD = 0.11992364529228877 1\n\n\n\n\nCode\n# CHECK RESULTS \nsave_results(output_dir+\"/variance_threshold_text\")\nplot_results(output_dir+\"/variance_threshold_text\")"
  },
  {
    "objectID": "naive-bayes/naive-bayes.html#naïve-bayes-with-labeled-record-data",
    "href": "naive-bayes/naive-bayes.html#naïve-bayes-with-labeled-record-data",
    "title": "Naïve Bayes",
    "section": "Naïve Bayes with Labeled Record Data",
    "text": "Naïve Bayes with Labeled Record Data\nNow that we have look at some feature selection with Naïve Bayes, let’s try to predict whether an individual uses cannabis using our psychosis data. We will use a Bernoulli Naïve Bayes model because our target is binary.\nFirst, we can take the optimal feature set from our record data, full_data, based on our feature selection above. Using the elbow method from the partial grid search, we see that the optimal number of features is 5 with a train accuracy of 70.88% and a test accuracy of 68.57%. We will use this feature set for our Bernoulli Naïve Bayes model.\n\n\nCode\nx = full_data.drop(['CannabisBinary', 'code', 'Program'], axis=1).to_numpy()\ny = full_data['CannabisBinary'].to_numpy()\n\n\n\n\nCode\nupper_index=0+5*int((19-0)/19)\nx=x[:,0:upper_index]\n\n\nRecall that we have already split the data into a training, validation, and test set. Let’s use the training set to train our model and the validation set to tune our model. Then, we can assess the model’s performance using the test set.\n\n\nCode\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\nimport random\nN=x.shape[0]\nl = [*range(N)]     # indices\ncut = int(0.8 * N) #80% of the list\nrandom.shuffle(l)   # randomize\ntrain_index = l[:cut] # first 80% of shuffled list\ntest_index = l[cut:]\n\n#SPLIT\nx_train=x[train_index]\ny_train=y[train_index].flatten()\n\nx_test=x[test_index]\ny_test=y[test_index].flatten()\n\nbnb = BernoulliNB()\n\n# Train the model\nbnb.fit(x_train, y_train)\n\n# Predict on test set\ny_pred = bnb.predict(x_test)\ny_pred_prob = bnb.predict_proba(x_test)\n\n# Print accuracy, precision, recall, and F1 score\nprint(\"Accuracy: {:.2f}%\".format(accuracy_score(y_test, y_pred) * 100))\nprint(\"Precision: {:.2f}%\".format(precision_score(y_test, y_pred, average='weighted') * 100))\nprint(\"Recall: {:.2f}%\".format(recall_score(y_test, y_pred, average='weighted') * 100))\nprint(\"F1: {:.2f}%\".format(f1_score(y_test, y_pred, average='weighted') * 100))\n\n\nAccuracy: 65.71%\nPrecision: 69.12%\nRecall: 65.71%\nF1: 65.86%\n\n\nAfter training our model to approximate \\(P(X|Y)\\) and using a prior for \\(P(Y)\\), we make predictions on our test set. The predictions are made by calculating the probability of the input value given each class based on our training data. The class with the highest probability is then returned as the prediction.\nTo evaluate our model, we use accuracy, precision, recall, and F1 score. Accuracy tells us how many of our predictions are correct. Precision indicates how many of our positive prediction (i.e. that an individual uses cannabis) are correct. Recall indicates how many of the true positives we correctly identified with our model. F1 score averages precision and recall. Our precision and recall are very similar to our accuracy, meaning our model isn’t simply predicting everything as one class. This also indicates that our data is well stratified into the two classes with an even balance of positive and negative cases. Our accuracy is relatively high at 65.71%, with a very similar F1 score of 65.86%.\nThe findings of this project will be maintained in a GitHub repository as well as this website. I will also present my findings in a presentation to the class in December 2023.\nOur findings mean that we can predict whether an individual uses cannabis with about 80% accuracy based on their mental health and psychosis history. This is a fairly high accuracy. Using these results, we gain evidence that there is a relationship between cannabis use and psychosis. We will continue to explore this relationship in the next section of the project.\n\n\nCode\n# confusion matrix\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\n\n# plot confusion matrix\nplt.figure(figsize=(5, 5))\nsns.heatmap(cm, linewidths=.5, square=True, cmap='Blues_r', annot=True, fmt='d')\n\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')\n\nplt.title('Confusion Matrix of Naive\\nBayes Classifier for Cannabis Usage', size=15)\n\n\n[[33 10]\n [26 36]]\n\n\nText(0.5, 1.0, 'Confusion Matrix of Naive\\nBayes Classifier for Cannabis Usage')\n\n\n\n\n\nBy looking at this confusion matrix, we see that our model minimizes the number of false negatives and false positives. This means that our recall and precision are fairly high. However, we see that about 20% of our data is getting misclassified, so we will continue to explore other models to see if we can improve our accuracy in future sections of the project.\n\n\nCode\n# ROC curve\nfrom sklearn.metrics import roc_curve, auc, roc_auc_score\n\n# Compute ROC curve and ROC area for each class\nfpr, tpr, _ = roc_curve(y_test, y_pred_prob[:, 1])\nroc_auc = auc(fpr, tpr)\n\n# Plot ROC curve\nplt.figure(figsize=(5, 5))\nlw = 2\nplt.plot(fpr, tpr, color='darkorange',\n         lw=lw, label='ROC curve (area = {:.2f}%)'.format(roc_auc * 100))\n\nplt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\n\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve of Naive\\nBayes Classifier for Cannabis Usage', size=15)\n\n\nText(0.5, 1.0, 'ROC Curve of Naive\\nBayes Classifier for Cannabis Usage')\n\n\n\n\n\n\n\nCode\nroc_auc\n\n\n0.6436609152288073\n\n\nBy looking at the ROC curve, we see that our model is better than one that makes predictions at random with an AUC score of 0.65. This means that our model generally predicts true positives while limiting false positives, although another model may perform better.\n\n\nCode\nplt.figure(figsize=(10, 5))\nscatter = plt.scatter(y_pred_prob[:, 0], y_pred_prob[:, 1], c=y_test)\n\nplt.xlabel('Probability of No Cannabis Usage')\nplt.ylabel('Probability of Cannabis Usage')\n\nplt.title('Naive Bayes Classifier for Cannabis Usage', size=15)\n\nplt.legend(*scatter.legend_elements())\n\n\n&lt;matplotlib.legend.Legend at 0x30aa93550&gt;\n\n\n\n\n\nWe see here that our data is fairly separable, meaning the model can predict decently well. We notice that the model tends to predict no cannabis usage where the truth indicates cannabis usage, indicating a need for better recall.\nOverall, our model performs well by classifying individuals who use cannabis with an accuracy of 65%. The model is likely not overfit because the accuracy on the test and training sets are very similar. An overfit model would have a high accuracy on the training set and a low accuracy on the test set.\nGenerally, our model gives more indication that there is a link between psychosis and cannabis use. This model shows that psychosis and mental health data is a fairly good predictor of cannabis usage. We will continue to explore this relationship in the next section of the project."
  },
  {
    "objectID": "naive-bayes/naive-bayes.html#naïve-bayes-with-labeled-text-data",
    "href": "naive-bayes/naive-bayes.html#naïve-bayes-with-labeled-text-data",
    "title": "Naïve Bayes",
    "section": "Naïve Bayes with Labeled Text Data",
    "text": "Naïve Bayes with Labeled Text Data\nNow that we have look at some feature selection with Naïve Bayes, let’s try to predict which subreddit a post belongs to using our text data. We will use a Multinomial Naïve Bayes model because our target is multiclass.\nWe will use the optimal feature set derived in the earlier analysis using the partial grid search. Using the elbow method from the partial grid search, we see that the optimal number of features is 250 with a train accuracy of 76.89% and a test accuracy of 77.96%. We will use this feature set for our Multinomial Naïve Bayes model.\n\n\nCode\nupper_index=0+25*int((1000-0)/100)\nx=x[:,0:upper_index]\n\n\n\n\nCode\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\nimport random\nN=x.shape[0]\nl = [*range(N)]     # indices\ncut = int(0.8 * N) #80% of the list\nrandom.shuffle(l)   # randomize\ntrain_index = l[:cut] # first 80% of shuffled list\ntest_index = l[cut:]\n\n\n\n\nCode\n#SPLIT\nx_train=x[train_index]\ny_train=y[train_index].flatten()\n\nx_test=x[test_index]\ny_test=y[test_index].flatten()\n\nmnb = MultinomialNB()\n\n# Train the model\nmnb.fit(x_train, y_train)\n\n# Predict on test set\ny_pred = mnb.predict(x_test)\ny_pred_prob = mnb.predict_proba(x_test)\n\n# Print accuracy, precision, recall, and F1 score\nprint(\"Accuracy: {:.2f}%\".format(accuracy_score(y_test, y_pred) * 100))\nprint(\"Precision: {:.2f}%\".format(precision_score(y_test, y_pred, average='weighted') * 100))\nprint(\"Recall: {:.2f}%\".format(recall_score(y_test, y_pred, average='weighted') * 100))\nprint(\"F1: {:.2f}%\".format(f1_score(y_test, y_pred, average='weighted') * 100))\n\n\nAccuracy: 76.85%\nPrecision: 79.32%\nRecall: 76.85%\nF1: 77.18%\n\n\nWhen classifiying the subreddits, we get an accuracy of 76.85%. The precision is 79.32% and the recall is 76.85%. The F1 score is 77.18%. These scores indicate that our model is giving fairly accurate results while predicting well across all three classes. This shows that our model is predicting all 3 classes fairly well, likely due to the even-distribution of our data across classes.\n\n\nCode\n# confusion matrix\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\n\n# plot confusion matrix\nplt.figure(figsize=(5, 5))\nsns.heatmap(cm, linewidths=.5, square=True, cmap='Blues_r', annot=True, fmt='d')\n\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')\n\nplt.title('Confusion Matrix of Naive\\nBayes Classifier for Subreddit Text', size=15)\n\n\n[[1413   62  528]\n [  80 1499  388]\n [  60  257 1653]]\n\n\nText(0.5, 1.0, 'Confusion Matrix of Naive\\nBayes Classifier for Subreddit Text')\n\n\n\n\n\nThis confusion matrix indicates that our model is doing a good job in predicting our classes. I was expecting that the model might have trouble distinguishing between the “psychosis” and “schizophrenia” classes, but we see here that the model is doing a good job of distinguishing between all three classes. There is also not a clear trend of one class being predicted less accurately than the others.\n\n\nCode\n# Plot the predicted probabilities, color by the true label\nplt.figure(figsize=(10, 5))\nscatter = plt.scatter(y_pred_prob[:, 0], y_pred_prob[:, 1], c=y_test)\n\nplt.xlabel('Predicted probability of r/weed')\nplt.ylabel('Predicted probability of r/Psychosis')\n\nplt.title('Predicted Probabilities of Naive\\nBayes Classifier for Subreddit Text', size=15)\n\nplt.legend(*scatter.legend_elements())\n\n\n&lt;matplotlib.legend.Legend at 0x30b328310&gt;\n\n\n\n\n\n\n\nCode\nplt.figure(figsize=(10, 5))\n\nscatter = plt.scatter(y_pred_prob[:, 0], y_pred_prob[:, 2], c=y_test)\n\nplt.xlabel('Predicted probability of r/weed')\nplt.ylabel('Predicted probability of r/schizophrenia')\n\nplt.title('Predicted Probabilities of Naive\\nBayes Classifier for Subreddit Text', size=15)\n\nplt.legend(*scatter.legend_elements())\n\n\n&lt;matplotlib.legend.Legend at 0x30b0d6c90&gt;\n\n\n\n\n\n\n\nCode\nplt.figure(figsize=(10, 5))\nscatter = plt.scatter(y_pred_prob[:, 1], y_pred_prob[:, 2], c=y_test)\n\nplt.xlabel('Predicted probability of r/Psychosis')\nplt.ylabel('Predicted probability of r/schizophrenia')\n\nplt.title('Predicted Probabilities of Naive\\nBayes Classifier for Subreddit Text', size=15)\n\nplt.legend(*scatter.legend_elements())\n\n\n&lt;matplotlib.legend.Legend at 0x30b313490&gt;\n\n\n\n\n\n\n\nCode\n# 3D plot of predicted probabilities\nfrom mpl_toolkits.mplot3d import Axes3D\n\nfig = plt.figure(figsize=(10, 10))\nax = fig.add_subplot(111, projection='3d')\n\nax.scatter(y_pred_prob[:, 0], y_pred_prob[:, 1], y_pred_prob[:, 2], c=y_test)\n\nax.set_xlabel('Predicted probability of r/weed')\nax.set_ylabel('Predicted probability of r/Psychosis')\nax.set_zlabel('Predicted probability of r/schizophrenia')\n\nax.set_title('Predicted Probabilities of Naive\\nBayes Classifier for Subreddit Text', size=15)\n\n# Add a legend\nax.legend(labels=['r/weed', 'r/Psychosis', 'r/schizophrenia'])\n\n\n&lt;matplotlib.legend.Legend at 0x3152eeb90&gt;\n\n\n\n\n\nThese plots demonstrate the separability of the three classes. As I expected, the “psychosis” and “schizophrenia” classes are more similar to each other than to the “weed” class. However, the “psychosis” and “schizophrenia” classes are still fairly separable, so our model can predict between them with a fairly high accuracy.\nOverall, our model is well trained and not overfit, as shown by the fact that the training and test accuracies are very similar. We are able to predict which subreddit a given text comes from with about 75% accuracy, which is very good considering the similarity in topics between all three subreddits.\nWe will continue to explore this text in future sections to determine sentiment regarding cannabis use and psychosis."
  },
  {
    "objectID": "clustering/clustering.html",
    "href": "clustering/clustering.html",
    "title": "Clustering",
    "section": "",
    "text": "Build out your website tab for “clustering”"
  },
  {
    "objectID": "introduction/introduction.html",
    "href": "introduction/introduction.html",
    "title": "Cannabis: Harmless or Hazardous?",
    "section": "",
    "text": "As the stigma surrounding the usage of recreational drugs has lessened in recent years, many have called for the legalization of some illicit substances. In particular, law makers and lobbyists have considered the legalization of cannabis, also known as marijuana, due to its prevalence, medicinal properties, and low addictiveness. In fact, while cannabis is still illegal at the federal level, 23 states and D.C. have fully legalized marijuana for medical and recreational purposes, and 38 states have legalized marijuana usage for medical purposes(DISA n.d.).\n(DISA n.d.)\nHowever, there is also a growing body of research that indicates that cannabis usage can be linked to episodes of psychosis and trigger the development of schizophrenia. Research has show that there is “an association between cannabis use and psychosis”, but the question about causation remains open (Ganesh and D’Souza 2022). Increasingly, people are starting to use cannabis at a younger age and with more frequency, resulting in more questions about the health impact of such usage (Ganesh and D’Souza 2022). Furthermore, there has been a rising level of THC, the active ingredient in cannabis, in recent years. The average potency of THC in cannabis has increased from 3% in the 1960’s to 16% in England(Murray et al. 2016). Research has “robustly deomnstrated that THC can transiently induce clinically relevant acute schizophrenia-like symptoms”, meaning that THC has been show to cause episodes of psychosis (Ganesh and D’Souza 2022). On the other hand, genetic research has indicated that schizophrenia may be the causation of cannabis usage, leading to continue debate on the causality of this relationship (Ganesh and D’Souza 2022). A meta analysis of relevant research supports the possibility of “cannabis causing psychosis”, but the approach has limitations (deepak 9). It is also notable that cannabis use alone is “neither necessary nor sufficient for psychosis”, meaning other factors such as genetics are relevant for the development of psychosis (Ganesh and D’Souza 2022). Ganesh and D’Souza note that the increasing legalization and usage of cannabis must be studied closely, as it will elucidate whether there is a causational relationship between marijuana and psychosis.\nRelated to the usage of cannabis is the growing use of cannabinoids, substances that are closely related to cannabis. Examples of cannnabinoids include synthetic THC and CBD which have become prevalent through a “regulatory loophold” in 2018 (Dotson et al. 2022). Often, these substances are sold in very high concentrations that can lead to unknown risks. Research has shown tha “cannabis use was associated with an odds ratio of 1.4 for the development of schizophrenia”, and there is evidence for a risk of addiction to THC similar to other illicit substances. Even more concerning, research in Europe also indicates that rising THC levels and rising “first episode pyschosis admissions” to hospitals have coincided (Dotson et al. 2022).\nIn the U.S., approaches such as “vaping” and “dabbing” allow users to ingest THC in concentrations of up to 80% (Murray et al. 2016). These synthetic cannabinoids are much more dangerous that cannabis, increasing the risk of hospitalization by 30% (Murray et al. 2016). The first scientific study on the connection between cannabis and psychosis was led by Andreasson in 1987, and longitudinal studies have indicated an association between cannabis and psychosis, though not all results have been statistically significant (Murray et al. 2016). There is evidence that genetic predisposition plays a role in the development of psychosis and can be trigger by cannabis usage, but it is unclear if cannabis and psychosis are associated without genetic predisposition (Murray et al. 2016).\n\n\n\nThe increasing legalization of cannabis despite research indicating an association between cannabis and psychosis has motivated me to perform data-driven research on the correlation between cannabis usage and the development of psychosis. Some motivating questions include:\n\nIs cannabis usage a predictor of the development of a psychotic episode?\nIs cannabis usage a predictor of the development of a psychotic disorder, such as schizophrenia?\nHow does THC content contribute to the potential development of psychosis?\nHas the legalization of cannabis contributed to an increase in the prevelance of psychotic disorders?\nWhat role do genetics play in the development of a psychotic disorder? How does cannabis usage impact genetics?\nHow does age impact the development of psychosis due to cannabis usage?\nHow does frequency of cannabis usage impact the potential development of psychosis? The number of psychotic episodes?\nWhat is the public sentiment regarding cannabis?\nDoes public sentiment indicate an awareness of the dangers of cannabis, especially as related to psychosis?\nWhat is the public sentiment regarding cannabinoids such as CBD and THC products?\n\nOverall, the goal of this research is two-fold: (1) to perform a thorough, analytic analysis to assess the causational role of cannabis in the development of psychosis; and (2) gain insights into why legalization efforts have continued despite the severity of known risks associated with cannabis usage. My hypotheses are that cannabis does play a causal role in psychosis and development of schizophrenia and that public knowledge on the risks of cannabis usage have been poorly communicated and understated."
  },
  {
    "objectID": "introduction/introduction.html#survey-of-existing-research",
    "href": "introduction/introduction.html#survey-of-existing-research",
    "title": "Cannabis: Harmless or Hazardous?",
    "section": "",
    "text": "As the stigma surrounding the usage of recreational drugs has lessened in recent years, many have called for the legalization of some illicit substances. In particular, law makers and lobbyists have considered the legalization of cannabis, also known as marijuana, due to its prevalence, medicinal properties, and low addictiveness. In fact, while cannabis is still illegal at the federal level, 23 states and D.C. have fully legalized marijuana for medical and recreational purposes, and 38 states have legalized marijuana usage for medical purposes(DISA n.d.).\n(DISA n.d.)\nHowever, there is also a growing body of research that indicates that cannabis usage can be linked to episodes of psychosis and trigger the development of schizophrenia. Research has show that there is “an association between cannabis use and psychosis”, but the question about causation remains open (Ganesh and D’Souza 2022). Increasingly, people are starting to use cannabis at a younger age and with more frequency, resulting in more questions about the health impact of such usage (Ganesh and D’Souza 2022). Furthermore, there has been a rising level of THC, the active ingredient in cannabis, in recent years. The average potency of THC in cannabis has increased from 3% in the 1960’s to 16% in England(Murray et al. 2016). Research has “robustly deomnstrated that THC can transiently induce clinically relevant acute schizophrenia-like symptoms”, meaning that THC has been show to cause episodes of psychosis (Ganesh and D’Souza 2022). On the other hand, genetic research has indicated that schizophrenia may be the causation of cannabis usage, leading to continue debate on the causality of this relationship (Ganesh and D’Souza 2022). A meta analysis of relevant research supports the possibility of “cannabis causing psychosis”, but the approach has limitations (deepak 9). It is also notable that cannabis use alone is “neither necessary nor sufficient for psychosis”, meaning other factors such as genetics are relevant for the development of psychosis (Ganesh and D’Souza 2022). Ganesh and D’Souza note that the increasing legalization and usage of cannabis must be studied closely, as it will elucidate whether there is a causational relationship between marijuana and psychosis.\nRelated to the usage of cannabis is the growing use of cannabinoids, substances that are closely related to cannabis. Examples of cannnabinoids include synthetic THC and CBD which have become prevalent through a “regulatory loophold” in 2018 (Dotson et al. 2022). Often, these substances are sold in very high concentrations that can lead to unknown risks. Research has shown tha “cannabis use was associated with an odds ratio of 1.4 for the development of schizophrenia”, and there is evidence for a risk of addiction to THC similar to other illicit substances. Even more concerning, research in Europe also indicates that rising THC levels and rising “first episode pyschosis admissions” to hospitals have coincided (Dotson et al. 2022).\nIn the U.S., approaches such as “vaping” and “dabbing” allow users to ingest THC in concentrations of up to 80% (Murray et al. 2016). These synthetic cannabinoids are much more dangerous that cannabis, increasing the risk of hospitalization by 30% (Murray et al. 2016). The first scientific study on the connection between cannabis and psychosis was led by Andreasson in 1987, and longitudinal studies have indicated an association between cannabis and psychosis, though not all results have been statistically significant (Murray et al. 2016). There is evidence that genetic predisposition plays a role in the development of psychosis and can be trigger by cannabis usage, but it is unclear if cannabis and psychosis are associated without genetic predisposition (Murray et al. 2016)."
  },
  {
    "objectID": "introduction/introduction.html#data-science-research-project",
    "href": "introduction/introduction.html#data-science-research-project",
    "title": "Cannabis: Harmless or Hazardous?",
    "section": "",
    "text": "The increasing legalization of cannabis despite research indicating an association between cannabis and psychosis has motivated me to perform data-driven research on the correlation between cannabis usage and the development of psychosis. Some motivating questions include:\n\nIs cannabis usage a predictor of the development of a psychotic episode?\nIs cannabis usage a predictor of the development of a psychotic disorder, such as schizophrenia?\nHow does THC content contribute to the potential development of psychosis?\nHas the legalization of cannabis contributed to an increase in the prevelance of psychotic disorders?\nWhat role do genetics play in the development of a psychotic disorder? How does cannabis usage impact genetics?\nHow does age impact the development of psychosis due to cannabis usage?\nHow does frequency of cannabis usage impact the potential development of psychosis? The number of psychotic episodes?\nWhat is the public sentiment regarding cannabis?\nDoes public sentiment indicate an awareness of the dangers of cannabis, especially as related to psychosis?\nWhat is the public sentiment regarding cannabinoids such as CBD and THC products?\n\nOverall, the goal of this research is two-fold: (1) to perform a thorough, analytic analysis to assess the causational role of cannabis in the development of psychosis; and (2) gain insights into why legalization efforts have continued despite the severity of known risks associated with cannabis usage. My hypotheses are that cannabis does play a causal role in psychosis and development of schizophrenia and that public knowledge on the risks of cannabis usage have been poorly communicated and understated."
  },
  {
    "objectID": "data-collection/data-collection.html",
    "href": "data-collection/data-collection.html",
    "title": "Data Collection",
    "section": "",
    "text": "In order to answer questions regarding public sentiment on cannabis usage and its ties to psychosis and schizophrenia, we will get text from Reddit. Reddit functions as a public forum on a large variety of topics, making it a good source for text data featuring discussions on cannabis, schizophrenia, and psychosis.\nTo get data from the Reddit API, I first made a user account and registered an app. This allowed me to generate a client ID and client secret for my app. My Reddit username and password are also necessary to gain access to the API.\nTo get started getting data from the Reddit API, I generate an access token using a basic HTTP GET with the requests package in Python. Note that I have removed my personal information from this code.\n\n\nCode\nimport requests\nimport requests.auth\n\nclient_id = 'CLIENT_ID'\nclient_secret = 'CLIENT_SECRET'\nusername = 'USERNAME'\npassword = 'PASSWORD'\n\nclient_auth = requests.auth.HTTPBasicAuth(client_id, client_secret)\npost_data = {\"grant_type\": \"password\", \"username\": username, \"password\": password}\nheaders = {\"User-Agent\": \"DSANProject/1.0 by u/Haunting_River_226\"}\n\nresponse = requests.post(\"https://www.reddit.com/api/v1/access_token\", auth=client_auth, data=post_data, headers=headers)\nresponse_data = response.json()\n\n\nWith this call to the API, the response returns an access token as well as more token information. I will use the access token and token type to construct my API requests.\n\n\nCode\naccess_token = response_data['access_token']\ntoken_type = response_data['token_type']\n\n\nNow, I can use my access token to construct a header to use for all of my API calls.\n\n\nCode\nheaders = {\"Authorization\": str(token_type + access_token), \"User-Agent\": \"DSANProject/1.0 by u/Haunting_River_226\"}\n\n\nNow to get the data, I have chosen three subreddits that will be relevant: 1. r/Psychosis 2. r/schizophrenia 3. r/weed Each of these subreddits relate to cannabis and/or psychosis, and I will be analyzing the text to determine if and how these topics intersect in public conversation.\nIn order to get recent data, I will be pulling the top 10,000 posts from the previous year (October 12, 2022 - October 12, 2023). I use the /top end point to get the top posts in a given subreddit. The Reddit API pulls only the first 100 results from a subreddit, but I can get more than 100 results by using the after parameter and setting it equal to the after key in the response JSON. This starts by pulling the first 100 posts, then gets the next 100 posts, and so on until we have reached 10,000.\nThe Reddit API also has stringent limits on the number of requests made per minute, so I’ll use a sleep function that limits the API requests to 10 per minute.\nI will start with the r/Psychosis subreddit.\n\n\nCode\nimport time\n\npost_id = \"\"\ndata = {}\nfor i in range(0, 100):\n    time.sleep(6)\n    response = requests.get(\"https://oauth.reddit.com/r/Psychosis/top.json\", params={'t': 'year', 'limit': 100, 'after': post_id}, headers=headers)\n    res = response.json()\n    data[i] = res\n    post_id = res[\"data\"][\"after\"][3:]\n\n\nNext, we will repeat this process to get data from r/schizophrenia.\n\n\nCode\npost_id = \"\"\ndata_schizophrenia = {}\nfor i in range(0, 100):\n    time.sleep(6)\n    response = requests.get(\"https://oauth.reddit.com/r/schizophrenia/top.json\", params={'t': 'year', 'limit': 100, 'after': post_id}, headers=headers)\n    if(response.status_code != 200):\n        print(i)\n        print(response.status_code)\n    res = response.json()\n    data_schizophrenia[i] = res\n    post_id = res[\"data\"][\"after\"][3:]\n\n\nFinally, we will repeat this process once more to get data from r/weed.\n\n\nCode\npost_id = \"\"\ndata_cannabis = {}\nfor i in range(0, 100):\n    time.sleep(6)\n    response = requests.get(\"https://oauth.reddit.com/r/weed/top.json\", params={'t': 'year', 'limit': 100, 'after': post_id}, headers=headers)\n    res = response.json()\n    data_cannabis[i] = res\n    post_id = res[\"data\"][\"after\"][3:]\n\n\nLet’s save the data to limit calls to the API. We’ll save each dictionary as a JSON file to preserve it’s data structure. We will work on cleaning this data in the Data Cleaning page of this website.\n\n\nCode\nimport json\n\nwith open('reddit_psychosis_data.json', 'w') as json_file:\n        json.dump(data, json_file, indent=4)\n\nwith open('reddit_schizophrenia_data.json', 'w') as json_file:\n        json.dump(data_schizophrenia, json_file, indent=4)\n\nwith open('reddit_cannabis_data.json', 'w') as json_file:\n        json.dump(data_cannabis, json_file, indent=4)\n\n\n\n\n\nTo get a more academic perspective on the link between psychosis and cannabis, we will also pull data from Wikipedia. We will use R and the WikipediR package to get data from Wikipedia.\n\n\nCode\nlibrary(WikipediR)\n\n\n\n\nCode\nlong_term_cannabis_backlinks &lt;- page_backlinks(\n    \"en\",\n    \"wikipedia\",\n    page = \"Long-term effects of cannabis\",\n    limit = 500\n)\nlong_term_cannabis_links &lt;- page_links(\n    \"en\",\n    \"wikipedia\",\n    page = \"Long-term effects of cannabis\",\n    limit = 500,\n    namespaces = 0\n)\nlong_term_cannabis &lt;- page_content(\n    \"en\",\n    \"wikipedia\",\n    page_name = \"Long-term effects of cannabis\"\n)\n\n\n\n\nCode\nlong_term_cannabis_links$query$pages$`25905247`$links[[500]]\n\n\n\n    $ns\n        0\n    $title\n        'Effects of cannabis'\n\n\n\n\n\nCode\nlong_term_cannabis_backlinks$query$backlinks[[500]]\n\n\n\n    $pageid\n        53053428\n    $ns\n        0\n    $title\n        'San Marcos Seven'\n\n\n\nNow that we have the forward and back links for the “Long-term effects of cannabis” page, let’s get the content of the forward and back links to create our corpus.\n\n\nCode\nlibrary(tidyverse)\n\nwiki_data &lt;- tibble(\n    title = long_term_cannabis$parse$title,\n    text = long_term_cannabis$parse$text$`*`,\n    link = \"main\"\n)\n\n\n\n\nCode\nfor(i in 1:500) {\n    page_title = long_term_cannabis_links$query$pages$`25905247`$links[[i]]$title\n    tryCatch({\n        page_details &lt;- page_content(\n            \"en\",\n            \"wikipedia\",\n            page_name = page_title\n        )\n    },\n    error = function(e) {\n        print(paste0(\"error with \", page_title))\n    })\n\n    wiki_data &lt;- wiki_data %&gt;%\n        add_row(\n            title = page_details$parse$title,\n            text = page_details$parse$text$`*`,\n            link = \"link\"\n        )\n}\nfor(i in 1:500) {\n    page_id = long_term_cannabis_backlinks$query$backlinks[[i]]$page_id\n    tryCatch({\n        page_details &lt;- page_content(\n            \"en\",\n            \"wikipedia\",\n            page = page_id\n        )\n    },\n    error = function(e) {\n        print(paste0(\"error with \", page_id))\n    })\n    wiki_data &lt;- wiki_data %&gt;%\n        add_row(\n            title = page_details$parse$title,\n            text = page_details$parse$text$`*`,\n            link = \"back\"\n        )\n}\n\n\n\n\nCode\nwiki_data %&gt;% nrow()\n\n\n1001\n\n\n\n\nCode\nwiki_data %&gt;% \n    write_csv(file = \"wikipedia_scrape.csv\")\n\n\n\n\nCode\nsave(wiki_data, file = \"wikipedia_scrape.Rdata\")"
  },
  {
    "objectID": "data-collection/data-collection.html#reddit",
    "href": "data-collection/data-collection.html#reddit",
    "title": "Data Collection",
    "section": "",
    "text": "In order to answer questions regarding public sentiment on cannabis usage and its ties to psychosis and schizophrenia, we will get text from Reddit. Reddit functions as a public forum on a large variety of topics, making it a good source for text data featuring discussions on cannabis, schizophrenia, and psychosis.\nTo get data from the Reddit API, I first made a user account and registered an app. This allowed me to generate a client ID and client secret for my app. My Reddit username and password are also necessary to gain access to the API.\nTo get started getting data from the Reddit API, I generate an access token using a basic HTTP GET with the requests package in Python. Note that I have removed my personal information from this code.\n\n\nCode\nimport requests\nimport requests.auth\n\nclient_id = 'CLIENT_ID'\nclient_secret = 'CLIENT_SECRET'\nusername = 'USERNAME'\npassword = 'PASSWORD'\n\nclient_auth = requests.auth.HTTPBasicAuth(client_id, client_secret)\npost_data = {\"grant_type\": \"password\", \"username\": username, \"password\": password}\nheaders = {\"User-Agent\": \"DSANProject/1.0 by u/Haunting_River_226\"}\n\nresponse = requests.post(\"https://www.reddit.com/api/v1/access_token\", auth=client_auth, data=post_data, headers=headers)\nresponse_data = response.json()\n\n\nWith this call to the API, the response returns an access token as well as more token information. I will use the access token and token type to construct my API requests.\n\n\nCode\naccess_token = response_data['access_token']\ntoken_type = response_data['token_type']\n\n\nNow, I can use my access token to construct a header to use for all of my API calls.\n\n\nCode\nheaders = {\"Authorization\": str(token_type + access_token), \"User-Agent\": \"DSANProject/1.0 by u/Haunting_River_226\"}\n\n\nNow to get the data, I have chosen three subreddits that will be relevant: 1. r/Psychosis 2. r/schizophrenia 3. r/weed Each of these subreddits relate to cannabis and/or psychosis, and I will be analyzing the text to determine if and how these topics intersect in public conversation.\nIn order to get recent data, I will be pulling the top 10,000 posts from the previous year (October 12, 2022 - October 12, 2023). I use the /top end point to get the top posts in a given subreddit. The Reddit API pulls only the first 100 results from a subreddit, but I can get more than 100 results by using the after parameter and setting it equal to the after key in the response JSON. This starts by pulling the first 100 posts, then gets the next 100 posts, and so on until we have reached 10,000.\nThe Reddit API also has stringent limits on the number of requests made per minute, so I’ll use a sleep function that limits the API requests to 10 per minute.\nI will start with the r/Psychosis subreddit.\n\n\nCode\nimport time\n\npost_id = \"\"\ndata = {}\nfor i in range(0, 100):\n    time.sleep(6)\n    response = requests.get(\"https://oauth.reddit.com/r/Psychosis/top.json\", params={'t': 'year', 'limit': 100, 'after': post_id}, headers=headers)\n    res = response.json()\n    data[i] = res\n    post_id = res[\"data\"][\"after\"][3:]\n\n\nNext, we will repeat this process to get data from r/schizophrenia.\n\n\nCode\npost_id = \"\"\ndata_schizophrenia = {}\nfor i in range(0, 100):\n    time.sleep(6)\n    response = requests.get(\"https://oauth.reddit.com/r/schizophrenia/top.json\", params={'t': 'year', 'limit': 100, 'after': post_id}, headers=headers)\n    if(response.status_code != 200):\n        print(i)\n        print(response.status_code)\n    res = response.json()\n    data_schizophrenia[i] = res\n    post_id = res[\"data\"][\"after\"][3:]\n\n\nFinally, we will repeat this process once more to get data from r/weed.\n\n\nCode\npost_id = \"\"\ndata_cannabis = {}\nfor i in range(0, 100):\n    time.sleep(6)\n    response = requests.get(\"https://oauth.reddit.com/r/weed/top.json\", params={'t': 'year', 'limit': 100, 'after': post_id}, headers=headers)\n    res = response.json()\n    data_cannabis[i] = res\n    post_id = res[\"data\"][\"after\"][3:]\n\n\nLet’s save the data to limit calls to the API. We’ll save each dictionary as a JSON file to preserve it’s data structure. We will work on cleaning this data in the Data Cleaning page of this website.\n\n\nCode\nimport json\n\nwith open('reddit_psychosis_data.json', 'w') as json_file:\n        json.dump(data, json_file, indent=4)\n\nwith open('reddit_schizophrenia_data.json', 'w') as json_file:\n        json.dump(data_schizophrenia, json_file, indent=4)\n\nwith open('reddit_cannabis_data.json', 'w') as json_file:\n        json.dump(data_cannabis, json_file, indent=4)"
  },
  {
    "objectID": "data-collection/data-collection.html#wikipedia",
    "href": "data-collection/data-collection.html#wikipedia",
    "title": "Data Collection",
    "section": "",
    "text": "To get a more academic perspective on the link between psychosis and cannabis, we will also pull data from Wikipedia. We will use R and the WikipediR package to get data from Wikipedia.\n\n\nCode\nlibrary(WikipediR)\n\n\n\n\nCode\nlong_term_cannabis_backlinks &lt;- page_backlinks(\n    \"en\",\n    \"wikipedia\",\n    page = \"Long-term effects of cannabis\",\n    limit = 500\n)\nlong_term_cannabis_links &lt;- page_links(\n    \"en\",\n    \"wikipedia\",\n    page = \"Long-term effects of cannabis\",\n    limit = 500,\n    namespaces = 0\n)\nlong_term_cannabis &lt;- page_content(\n    \"en\",\n    \"wikipedia\",\n    page_name = \"Long-term effects of cannabis\"\n)\n\n\n\n\nCode\nlong_term_cannabis_links$query$pages$`25905247`$links[[500]]\n\n\n\n    $ns\n        0\n    $title\n        'Effects of cannabis'\n\n\n\n\n\nCode\nlong_term_cannabis_backlinks$query$backlinks[[500]]\n\n\n\n    $pageid\n        53053428\n    $ns\n        0\n    $title\n        'San Marcos Seven'\n\n\n\nNow that we have the forward and back links for the “Long-term effects of cannabis” page, let’s get the content of the forward and back links to create our corpus.\n\n\nCode\nlibrary(tidyverse)\n\nwiki_data &lt;- tibble(\n    title = long_term_cannabis$parse$title,\n    text = long_term_cannabis$parse$text$`*`,\n    link = \"main\"\n)\n\n\n\n\nCode\nfor(i in 1:500) {\n    page_title = long_term_cannabis_links$query$pages$`25905247`$links[[i]]$title\n    tryCatch({\n        page_details &lt;- page_content(\n            \"en\",\n            \"wikipedia\",\n            page_name = page_title\n        )\n    },\n    error = function(e) {\n        print(paste0(\"error with \", page_title))\n    })\n\n    wiki_data &lt;- wiki_data %&gt;%\n        add_row(\n            title = page_details$parse$title,\n            text = page_details$parse$text$`*`,\n            link = \"link\"\n        )\n}\nfor(i in 1:500) {\n    page_id = long_term_cannabis_backlinks$query$backlinks[[i]]$page_id\n    tryCatch({\n        page_details &lt;- page_content(\n            \"en\",\n            \"wikipedia\",\n            page = page_id\n        )\n    },\n    error = function(e) {\n        print(paste0(\"error with \", page_id))\n    })\n    wiki_data &lt;- wiki_data %&gt;%\n        add_row(\n            title = page_details$parse$title,\n            text = page_details$parse$text$`*`,\n            link = \"back\"\n        )\n}\n\n\n\n\nCode\nwiki_data %&gt;% nrow()\n\n\n1001\n\n\n\n\nCode\nwiki_data %&gt;% \n    write_csv(file = \"wikipedia_scrape.csv\")\n\n\n\n\nCode\nsave(wiki_data, file = \"wikipedia_scrape.Rdata\")"
  },
  {
    "objectID": "data-collection/data-collection.html#the-behavioral-sequelae-of-cannabis-use-in-health-people",
    "href": "data-collection/data-collection.html#the-behavioral-sequelae-of-cannabis-use-in-health-people",
    "title": "Data Collection",
    "section": "The Behavioral Sequelae of Cannabis Use in Health People",
    "text": "The Behavioral Sequelae of Cannabis Use in Health People\nThe first dataset comes from Sorkhou, Bedder, and George (2021) in The Behavioral Sequelae of Cannabis Use in Health People: A Systematic Review. This data was gathered as a collection of longitundial studies on the “cannabis-related adverse behavioral outcomes.”\nThe data comes in the form of a word document, so we can use the docxtractr package in R to extract the table. We will clean this table in the next step.\n\n\nCode\nlibrary(docxtractr)\n\ntable_as_docx &lt;- read_docx(\"../data/raw_data/Table_1_The Behavioral Sequelae of Cannabis Use in Healthy People_ A Systematic Review.DOCX\")\ntbl_out &lt;- docx_extract_tbl(table_as_docx)\ntbl_out %&gt;% write_csv(\"../data/behavioral_sequelae.csv\")"
  },
  {
    "objectID": "data-collection/data-collection.html#cannabis-research-article",
    "href": "data-collection/data-collection.html#cannabis-research-article",
    "title": "Data Collection",
    "section": "Cannabis Research Article",
    "text": "Cannabis Research Article\nThe next dataset comes from “cannabis research article” by baklaci (2023). This data was created to study the differences in cannabis usage between users with PEs and users without PEs.\nThis data comes in the form of and SPSS file, .sav, so we can read it using the haven package in R.\n\n\nCode\nlibrary(haven)\n\ncannabis_research_data &lt;- read_sav(\"../data/raw_data/dataset.sav\")\ncannabis_research_data %&gt;% write_csv(\"../data/cannabis_research_data.csv\")"
  },
  {
    "objectID": "data-collection/data-collection.html#cannabinoid-use-in-psychotic-patients-impacts-inflammatory-levels-and-their-association-with-psychosis-severity",
    "href": "data-collection/data-collection.html#cannabinoid-use-in-psychotic-patients-impacts-inflammatory-levels-and-their-association-with-psychosis-severity",
    "title": "Data Collection",
    "section": "Cannabinoid use in psychotic patients impacts inflammatory levels and their association with psychosis severity",
    "text": "Cannabinoid use in psychotic patients impacts inflammatory levels and their association with psychosis severity\nThe next dataset comes from Gibson et al. (2020) and their research on the impact of cannabinoid usage on psychotic patients. This data is easily parsed as an excel file using readxl.\n\n\nCode\nlibrary(readxl)\n\ncannabinoid &lt;- read_excel(\"../data/raw_data/DataforSumbission_FINAL.xlsx\")"
  },
  {
    "objectID": "data-collection/data-collection.html#cannabis-use-schizotypy-and-kamin-blocking-performance",
    "href": "data-collection/data-collection.html#cannabis-use-schizotypy-and-kamin-blocking-performance",
    "title": "Data Collection",
    "section": "Cannabis Use, Schizotypy and Kamin Blocking Performance",
    "text": "Cannabis Use, Schizotypy and Kamin Blocking Performance\nNext, we will utilize a data set that was used by Dawes et al. (2021) to study cannabis usage and schizotypy and their relationship with Kamin blocking. We will again use docxtractr to read in this data.\n\n\nCode\ndocx_table_2 &lt;- read_docx(\"../data/raw_data/Table_2_Cannabis Use, Schizotypy and Kamin Blocking Performance.DOCX\")\nkamin_blocking &lt;- docx_table_2 %&gt;%\n    docx_extract_tbl() %&gt;%\n    write_csv(\"../data/kamin_blocking.csv\")"
  },
  {
    "objectID": "data-collection/data-collection.html#cannabis-use-in-male-and-female-first-episode-of-non-affective-psychosis-patients-long-term-clinical-neuropsychological-and-functional-differences",
    "href": "data-collection/data-collection.html#cannabis-use-in-male-and-female-first-episode-of-non-affective-psychosis-patients-long-term-clinical-neuropsychological-and-functional-differences",
    "title": "Data Collection",
    "section": "Cannabis use in male and female first episode of non-affective psychosis patients: Long-term clinical, neuropsychological and functional differences",
    "text": "Cannabis use in male and female first episode of non-affective psychosis patients: Long-term clinical, neuropsychological and functional differences\nIn our final dataset, Setién-Suero et al. (2017) aim to study the difference in men and women in the link between cannabis usage and psychosis. Setién-Suero et. al. provide two datasets that will be utilized in this analysis.\n\n\nCode\nread_sav(\"../data/raw_data/S1File.sav\") %&gt;%\n    write_csv(\"../data/s1file.csv\")\n\nread_sav(\"../data/raw_data/S2File.sav\") %&gt;%\n    write_csv(\"../data/s2file.csv\")\n\n\nNow that we have collected an ample amount of data, we can move on to data cleaning."
  },
  {
    "objectID": "eda/eda.html",
    "href": "eda/eda.html",
    "title": "Data Exploration",
    "section": "",
    "text": "In order to do machine learning with our data, we need to develop a thorough understanding of the data and the relationships between the variables. We will do Exploratory Data Analysis (EDA) to understand the data and to prepare for modeling.\n\n\nTo begin, we can explore the textual data gathered that relates to psychosis and cannabis. This text data will be essential for answering questions regarding public sentiment related to the impact of cannabis on psychosis and/or schizophrenia.\nWe’ll start by cleaning up the text data in order to make visualizations and analyze the data.\n\n\nCode\nlibrary(tidyverse, quietly = TRUE, warn.conflicts = FALSE)\n\nwiki_data &lt;- read_csv(\"../data/clean_data/wiki_cleaned_text.csv\")\n\n\nRows: 397 Columns: 2\n-- Column specification --------------------------------------------------------\nDelimiter: \",\"\nchr (2): link, text\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nCode\nreddit_data &lt;- read_csv(\n        \"../data/clean_data/reddit_cleaned_text.csv\"\n    )\n\n\nRows: 30000 Columns: 2\n-- Column specification --------------------------------------------------------\nDelimiter: \",\"\nchr (2): label, text\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nCode\nlibrary(ggwordcloud)\nlibrary(tidytext)\nlibrary(wordcloud)\n\n\n\n\nCode\nwiki_data &lt;- wiki_data %&gt;%\n    mutate(cannabis_label = case_when(\n        str_detect(text, \"cannabis|marijuana|weed|THC|CBD\") ~ \"cannabis\",\n        TRUE ~ \"no cannabis\"\n    )) %&gt;%\n    mutate(schiz_label = case_when(\n        str_detect(text, \"psychosis|psychotic|schizophrenia|schizophrenic|schizo|schizoaffective|schizotypal|schizoid|schiz\") ~ \"psychosis\",\n        TRUE ~ \"no psychosis\"\n    )) %&gt;%\n    mutate(label = case_when(\n        cannabis_label == \"cannabis\" & schiz_label == \"psychosis\" ~ \"cannabis and psychosis\",\n        cannabis_label == \"cannabis\" & schiz_label == \"no psychosis\" ~ \"cannabis\",\n        cannabis_label == \"no cannabis\" & schiz_label == \"psychosis\" ~ \"psychosis\",\n        TRUE ~ \"no cannabis and no psychosis\"\n    ))\n\n\n\n\nCode\ntidy_wiki &lt;- wiki_data %&gt;%\n    distinct() %&gt;%\n    filter(!is.na(text)) %&gt;%\n    unnest_tokens(word, text) %&gt;%\n    group_by(label, word) %&gt;%\n    summarise(n = n()) %&gt;%\n    ungroup() %&gt;%\n    group_by(word) %&gt;%\n    mutate(word_count = sum(n)) %&gt;%\n    ungroup() %&gt;%\n    arrange(-word_count)\n\n\n`summarise()` has grouped output by 'label'. You can override using the\n`.groups` argument.\n\n\n\n\nCode\ntidy_wiki &lt;- tidy_wiki %&gt;%\n  anti_join(stop_words) %&gt;%\n  filter(word != \"numbr\")\n\n\nJoining with `by = join_by(word)`\n\n\nNow that the Wikipedia data is clean, let’s visualize it to get a deeper understanding of the data.\n\n\nCode\nlibrary(wordcloud)\n\ntidy_wiki %&gt;%\n    select(word, word_count) %&gt;%\n    distinct() %&gt;%\n    with(wordcloud(word, word_count, max.words = 200, colors = brewer.pal(8, \"Dark2\")))\n\n\n\n\n\nThis wordcloud indicates that the Wikipedia text refers to cannabis as a drug, emphasizing medical language and medical effects of the substance. There does not appear to be any mention of psychosis or schizophrenia in the text, indicating that it is not a common topic of discussion on the Wikipedia pages relevant to cannabis usage.\n\n\nCode\ntidy_wiki %&gt;%\n    filter(stringi::stri_enc_isascii(word)) %&gt;%\n    group_by(label) %&gt;%\n    mutate(m = n / sum(n)) %&gt;%\n    arrange(-m) %&gt;%\n    slice_head(n = 50) %&gt;%\n    ggplot(aes(label = word, size = m, color = label)) +\n    geom_text_wordcloud_area(rm_outside = TRUE) +\n    scale_size_area(max_size = 20) +\n    theme_minimal() +\n    facet_wrap(~label)\n\n\n\n\n\nHere, we visualize the text based on whether it mentions cannabis, psychosis, neither, or both. We notice that texts mentioning psychosis tend to have language that relates more to addiction and science as it relates to cannabis. This indicates that conversations around mental health and cannabis tend to be more scientific in nature.\n\n\nCode\ntidy_wiki %&gt;%\n    mutate(nchar = nchar(word)) %&gt;%\n    group_by(label) %&gt;%\n    summarize(mean = mean(nchar))\n\n\n\nA tibble: 4 x 2\n\n\nlabel\nmean\n\n\n&lt;chr&gt;\n&lt;dbl&gt;\n\n\n\n\ncannabis\n7.696249\n\n\ncannabis and psychosis\n7.767887\n\n\nno cannabis and no psychosis\n7.631243\n\n\npsychosis\n8.038816\n\n\n\n\n\nWe see here that the average word length is very similar across all found categories. However, it tends to be slightly higher when psychosis is mentioned. This may indicate a more scientific discussion, as scientific terms tend to be longer than average, but this is somewhat speculative.\n\n\nCode\nwiki_data %&gt;%\n    distinct() %&gt;%\n    filter(!is.na(text)) %&gt;%\n    mutate(sentence_length = stringr::str_count(text, \"\\\\S+\")) %&gt;%\n    group_by(label) %&gt;%\n    summarize(mean = mean(sentence_length), median = median(sentence_length), n = n())\n\n\n\nA tibble: 4 x 4\n\n\nlabel\nmean\nmedian\nn\n\n\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;int&gt;\n\n\n\n\ncannabis\n783.2434\n266.5\n152\n\n\ncannabis and psychosis\n2391.0417\n2006.5\n24\n\n\nno cannabis and no psychosis\n388.6061\n69.0\n198\n\n\npsychosis\n2205.4706\n1381.0\n17\n\n\n\n\n\nTexts that mentions psychosis are significantly longer in word count that texts that do not mention psychosis. This suggests that these conversations may be much more nuanced and detailed than conversations that do not mention psychosis. Another factor may be that we have less texts that mention psychosis than those that do not, so the average word count may be skewed.\n\n\nCode\ntidy_wiki %&gt;%\n    group_by(label) %&gt;%\n    summarise(n = n())\n\n\n\nA tibble: 4 x 2\n\n\nlabel\nn\n\n\n&lt;chr&gt;\n&lt;int&gt;\n\n\n\n\ncannabis\n12902\n\n\ncannabis and psychosis\n7673\n\n\nno cannabis and no psychosis\n10812\n\n\npsychosis\n6183\n\n\n\n\n\nThis table shows overall word counts by category. This is not very useful since the categories are not evenly distributed.\n\n\nCode\ntidy_wiki %&gt;%\n    group_by(label) %&gt;%\n    mutate(total = sum(n)) %&gt;%\n    ggplot(aes(n/total, fill = label)) +\n    geom_histogram(show.legend = FALSE) +\n    facet_wrap(~label, ncol = 2, scales = \"free_y\")\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nAs expected, words with low frequency are much more common across all 4 categories than words with high frequency. This follows Zipf’s law. Texts that mention psychosis appear to have slightly more repeated language than texts that do not mention psychosis.\n\n\nCode\ntidy_wiki %&gt;%\n  group_by(label) %&gt;%\n  mutate(total = sum(n)) %&gt;%\n  arrange(label, -n) %&gt;%\n  mutate(rank = row_number(), term_freq = n/total) %&gt;%\n  ungroup() %&gt;%\n  ggplot(aes(rank, term_freq, color = label)) + \n  geom_line(size = 1, alpha = 0.8, show.legend = FALSE) +\n  scale_x_log10() +\n  scale_y_log10()\n\n\n\n\n\nAgain, we see a very clear display of Zipf’s law, indicating the inverse nature between term frequency and rank. This means that the most common words are much more common than the least common words. We see here that this remains true across all categories in our data.\n\n\nCode\ntidy_wiki %&gt;%\n  group_by(label) %&gt;%\n  slice_max(n, n = 15) %&gt;%\n  ungroup() %&gt;%\n  ggplot(aes(n, fct_reorder(word, n), fill = label)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~label, ncol = 2, scales = \"free\")\n\n\n\n\n\nWhen looking at the top most frequent terms for each category of Wikipedia article, we see that texts that mention cannabis and psychosis focus on the effects of cannabis and THC specificallly. It is also notable that texts which only mention psychosis also mention drugs, indicating the high prevalence of drug use in individuals with psychosis. Texts that mention only cannabis include “law” as a top word, indicting discussions about the legality of cannabis usage.\n\n\nCode\ntidy_wiki %&gt;%\n    select(word, word_count) %&gt;%\n    distinct() %&gt;%\n    filter(word_count &gt;= 600) %&gt;%\n    ggplot() +\n    geom_bar(aes(y = fct_reorder(word, word_count), x = word_count), stat = \"identity\")\n\n\n\n\n\nOverall, we see the theme of “effect”, indicating that the text is focused on identifying the impact of cannabis usage. This indicates that these texts will be helpful in identifying public sentiment around the impact of cannabis on psychosis and schizophrenia, as texts are focused on the impact of cannabis usage.\nNow, let’s take a deeper look at our Reddit data.\n\n\nCode\ntidy_reddit &lt;- reddit_data %&gt;%\n    distinct() %&gt;%\n    filter(!is.na(text)) %&gt;%\n    unnest_tokens(word, text) %&gt;%\n    group_by(label, word) %&gt;%\n    summarise(n = n()) %&gt;%\n    ungroup() %&gt;%\n    group_by(word) %&gt;%\n    mutate(word_count = sum(n)) %&gt;%\n    ungroup() %&gt;%\n    arrange(-word_count)\n\n\n`summarise()` has grouped output by 'label'. You can override using the\n`.groups` argument.\n\n\n\n\nCode\ntidy_reddit &lt;- tidy_reddit %&gt;%\n  anti_join(stop_words) %&gt;%\n  filter(word != \"numbr\")\n\n\nJoining with `by = join_by(word)`\n\n\n\n\nCode\ntidy_reddit %&gt;%\n    select(word, word_count) %&gt;%\n    distinct() %&gt;%\n    with(wordcloud(word, word_count, max.words = 100, colors = brewer.pal(8, \"Dark2\")))\n\n\n\n\n\nImmediately, we can sense a difference in tone between Wikipedia data and Reddit data. These texts are much more casual and anecdotal, rather than clinical and scientific. This indicates that Reddit data will be helpful in identifying public sentiment around the impact of cannabis on psychosis and schizophrenia, as the word “feel” is one of the most commonly used.\n\n\nCode\nggplot(\n  tidy_reddit %&gt;% filter(stringi::stri_enc_isascii(word)) %&gt;% filter(n &gt;= 2),\n  aes(\n    label = word, size = n, color = label\n  )\n) +\n  geom_text_wordcloud_area(rm_outside = TRUE) +\n  scale_size_area(max_size = 20) +\n  theme_minimal() +\n  facet_wrap(~label)\n\n\nWarning message in wordcloud_boxes(data_points = points_valid_first, boxes = boxes, :\n\"Some words could not fit on page. They have been removed.\"\n\n\n\n\n\nHere we see the breakdown of the most common words across the subreddits. It is clear that the “r/weed” subreddit consists largely of stories about being high. The “r/schizophrenia” and “r/Psychosis” subreddits include stories of delusions and express emotions. It will be interesting to understand the overlap between these three subreddits.\n\n\nCode\ntidy_reddit %&gt;%\n    mutate(nchar = nchar(word)) %&gt;%\n    group_by(label) %&gt;%\n    summarize(mean = mean(nchar), median = median(nchar))\n\n\n\nA tibble: 3 x 3\n\n\nlabel\nmean\nmedian\n\n\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nPsychosis\n6.411458\n6\n\n\nschizophrenia\n6.291242\n6\n\n\nweed\n5.555046\n5\n\n\n\n\n\nThe average word count is highest for “r/Psychosis”, followed by “r/schizophrenia”, and then “r/weed”. This indicates that the texts in “r/Psychosis” are more detailed than those in “r/weed”. This seems to indicate that individuals are telling detailed stories about their experiences with psychosis and schizophrenia on the “r/Psychosis” and “r/schizophrenia” subreddits.\n\n\nCode\nreddit_data %&gt;%\n    distinct() %&gt;%\n    filter(!is.na(text)) %&gt;%\n    mutate(sentence_length = stringr::str_count(text, \"\\\\S+\")) %&gt;%\n    group_by(label) %&gt;%\n    summarize(mean = mean(sentence_length), median = median(sentence_length))\n\n\n\nA tibble: 3 x 3\n\n\nlabel\nmean\nmedian\n\n\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nPsychosis\n26.979381\n5\n\n\nschizophrenia\n10.802083\n4\n\n\nweed\n8.262626\n4\n\n\n\n\n\nThis analysis confirms that individuals are writing many more words in “r/Psychosis” than in the other two subreddits. It is interesting that “r/Psychosis” has a much higher word count than “r/schizophrenia”, even though I would expect them to be similar. We also see that the means are all significantly higher than the medians, indicating that there are a few outliers of very long text in the data.\n\n\nCode\ntidy_reddit %&gt;%\n    group_by(label) %&gt;%\n    summarise(n = n())\n\n\n\nA tibble: 3 x 2\n\n\nlabel\nn\n\n\n&lt;chr&gt;\n&lt;int&gt;\n\n\n\n\nPsychosis\n960\n\n\nschizophrenia\n491\n\n\nweed\n436\n\n\n\n\n\nWe see here that the total number of words used across all the texts is almost twice as high in “r/Psychosis”. This again points to the fact that people are writing more on “r/Psychosis” than on the other two subreddits, so they are using a more diverse subset of words.\n\n\nCode\ntidy_reddit %&gt;%\n    filter(stringi::stri_enc_isascii(word)) %&gt;%\n    group_by(label) %&gt;%\n    arrange(-n) %&gt;%\n    slice_head(n = 10) %&gt;%\n    ggplot(aes(x = n, y = word, fill = label)) +\n    geom_bar(stat = \"identity\") +\n    theme_minimal() +\n    facet_wrap(~label, scales = \"free_y\")\n\n\n\n\n\nFrom this visual of the top words in each subreddit, we see that “r/Psychosis” tends to focus on feelings and tell stories of experiences with psychosis. “r/schizophrenia” tends to focus on symptoms and experiences with medication. “r/weed” tends to focus on the experience of being high and partaking in cannabis usage.\n\n\nCode\ntidy_reddit %&gt;%\n    group_by(label) %&gt;%\n    mutate(total = sum(n)) %&gt;%\n    ggplot(aes(n/total, fill = label)) +\n    geom_histogram(show.legend = FALSE) +\n    facet_wrap(~label, ncol = 3, scales = \"free_y\")\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nHere we see again that the most common words are much more common than the least common words, indicating Zipf’s law. We also see that the texts in “r/Psychosis” have more repeated language than the other two subreddits, probably due to the fact that people are writing more in “r/Psychosis”.\n\n\nCode\ntidy_reddit %&gt;%\n  group_by(label) %&gt;%\n  mutate(total = sum(n)) %&gt;%\n  arrange(label, -n) %&gt;%\n  mutate(rank = row_number(), term_freq = n/total) %&gt;%\n  ungroup() %&gt;%\n  ggplot(aes(rank, term_freq, color = label)) + \n  geom_line(size = 1, alpha = 0.8, show.legend = FALSE) +\n  scale_x_log10() +\n  scale_y_log10()\n\n\n\n\n\nOnce again, we see that for each subreddit, the most common words are much more common than the least common words.\nNow that we have a better understanding of the text data, we can begin to explore our numerical data.\n\n\n\nIn order to understand our numerical data, we will analyze relationships through numerical summaries and visualizations. We will also look at correlation between variables in our data in order to understand the relationships between variables and remove highly correlated variables from our data.\n\n\nCode\ns1 &lt;- read_csv(\"../data/clean_data/s1file_clean.csv\")\n\n\nRows: 549 Columns: 24\n-- Column specification --------------------------------------------------------\nDelimiter: \",\"\ndbl (24): Code, Program, sex, age, age_psychosis, famhis, hospita, dui, dup,...\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nCode\ns2 &lt;- read_csv(\"../data/clean_data/s2file_clean.csv\")\n\n\nRows: 477 Columns: 23\n-- Column specification --------------------------------------------------------\nDelimiter: \",\"\ndbl (23): code, Program, sex, age, agepsychosis, fampsic, hospita, dui, dup,...\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nNow that we have the s1 and s2 files, let’s figure out how to join them. Using setdiff below, we see that s2 has a few columns not in s1.\n\n\nCode\nsetdiff(names(s1), names(s2))\n\n\n\n'Code''age_psychosis''famhis''levelsocioeco''dasgl0''Diagnosisobinnary''insight'\n\n\n\n\nCode\nsetdiff(names(s2), names(s1))\n\n\n\n'code''agepsychosis''fampsic''levelecon''cds0''Diagnosisbinario'\n\n\n\n\nCode\nfull_data &lt;- s1 %&gt;% \n    rename(\n        code = Code,\n        diagnosis = Diagnosisobinnary,\n        family_history = famhis\n    ) %&gt;%\n    left_join(\n        s2 %&gt;%\n            rename(\n                diagnosis = Diagnosisbinario,\n                age_psychosis = agepsychosis,\n                family_history = fampsic,\n                levelsocioeco = levelecon\n            )\n    )\n\n\nJoining with `by = join_by(code, Program, sex, age, age_psychosis,\nfamily_history, hospita, dui, dup, levelsocioeco, urbanarea, livingwithparents,\nunmarried, unemployed, years_edu, CannabisBinary, SAPS0, SANS0, Psychoticdim0,\nDisorganizeddim0, Negativedimen0, diagnosis)`\n\n\nWe perform a left_join of the two datasets since s2 is a subset of s1 with additional columns. We can see that the s2 columns are added to the s1 columns in our new joined dataset.\n\n\nCode\nfull_data %&gt;% head()\n\n\n\nA tibble: 6 x 25\n\n\ncode\nProgram\nsex\nage\nage_psychosis\nfamily_history\nhospita\ndui\ndup\nlevelsocioeco\n...\nCannabisBinary\nSAPS0\nSANS0\nPsychoticdim0\nDisorganizeddim0\nNegativedimen0\ndasgl0\ndiagnosis\ninsight\ncds0\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n...\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n1\n1\n22.24658\n22.16324\n2\n2\n2.0\n1.0\n1\n...\n1\n6\n6\n4\n2\n5\n2\n1\n1\nNA\n\n\n2\n1\n0\n32.67945\n32.63779\n1\n2\n0.5\n0.5\n2\n...\n1\n9\n5\n8\n1\n4\n0\n1\n1\nNA\n\n\n3\n1\n0\n30.23562\n30.15228\n2\n1\n36.0\n1.0\nNA\n...\n1\n11\n2\n4\n7\n0\nNA\n1\nNA\nNA\n\n\n4\n1\n0\n24.80274\n24.46941\n2\n2\n18.0\n4.0\n2\n...\n1\n14\n10\n8\n6\n7\n0\n0\n2\nNA\n\n\n5\n1\n0\n22.38082\n21.38082\n2\n1\n28.0\n12.0\n1\n...\n1\n11\n22\n4\n7\n18\n2\n0\n2\nNA\n\n\n6\n1\n0\n17.07671\n16.99338\n2\n1\n1.0\n1.0\n2\n...\n1\n10\n5\n4\n6\n5\n0\n1\n2\nNA\n\n\n\n\n\n\n\nCode\nfull_data &lt;- full_data %&gt;%\n    mutate(\n        across(\n            c(code, Program, sex, family_history, diagnosis, \n            hospita, levelsocioeco, urbanarea, livingwithparents,\n            unmarried, unemployed, CannabisBinary\n            ),\n            ~ as.factor(.x)\n        )\n    )\n\n\n\n\nCode\nfull_data %&gt;%\n    ggplot() +\n    geom_bar(aes(x = diagnosis, fill = diagnosis)) +\n    theme_minimal()\n\n\n\n\n\nHere we see that the data is well distributed between individuals with a diagnosis of schizophrenia and those without. This is important because we want to make sure that our data is not skewed towards one group or the other. Evenly distributed data will help us build better models and make better predictions.\n\n\nCode\nfull_data %&gt;%\n    ggplot() +\n    geom_bar(aes(x = diagnosis, fill = sex))\n\n\n\n\n\nOur data is fairly evenly distributed between genders. The patients without schizophrenia are slightly skewed male.\n\n\nCode\nfull_data %&gt;%\n    ggplot() +\n    geom_bar(aes(x = diagnosis, fill = CannabisBinary))\n\n\n\n\n\nIn this plot, it is clear that the spread of individuals using cannabis is even across both groups. This will help us make clear predictions and gain a better understanding of the impact of cannabis on diagnosis by having all groups evenly represented.\n\n\nCode\nlibrary(corrplot)\nfull_data %&gt;%\n    select(-cds0) %&gt;%\n    select(where(is.numeric)) %&gt;%\n    # fill in all missing values with the mean\n    mutate(across(where(is.numeric), ~ ifelse(is.na(.x), mean(.x, na.rm = TRUE), .x))) %&gt;%\n    # normalize all numeric variables\n    mutate(across(where(is.numeric), ~ (.x - min(.x)) / (max(.x) - min(.x)))) %&gt;%\n    cor() %&gt;%\n    corrplot.mixed(order = 'AOE', upper = 'circle', tl.col = 'black')\n\n\n\n\n\nThrough a correlation plot of all numeric variables, we see that very few variables are correlated. This is great because it means that our features are relatively independent of each other, which will help us build better models.\nThere are two sets of highly correlated features: Negativedimen0 and SANS0; and age_psychosis and age. The two age variables are obviously clearly correlated because the data was collected shortly after the onset of pychosis, so we will remove age. We also remove SANS0 due to low interpretability.\n\n\nCode\nfull_data %&gt;%\n    select(-cds0) %&gt;%\n    select(where(is.factor)) %&gt;%\n    drop_na() %&gt;%\n    mutate(across(where(is.factor), ~ as.numeric(.x))) %&gt;%\n    # normalize all numeric variables\n    mutate(across(where(is.numeric), ~ (.x - min(.x)) / (max(.x) - min(.x)))) %&gt;%\n    cor() %&gt;%\n    corrplot.mixed(order = 'AOE', upper = 'circle', tl.col = 'black')\n\n\n\n\n\nNext, we visualize the correlation between all the categorical variables as numeric variables. We see that most of the features are not highly correlated. Program and code are correlated, but we will remove both of these variables before modeling as they are both ID variables rather than features.\n\n\nCode\nfull_data &lt;- full_data %&gt;%\n    select(-SANS0, -age) %&gt;%\n    mutate(across(where(is.numeric), ~ ifelse(is.na(.x), mean(.x, na.rm = TRUE), .x)))\n\n\nNext, we will look at a full numeric summary of all variables. This tells us the mean, median, and quartiles of each numeric variable as well as the distribution of factor variables. it is notable that most of the factor variables are fairly evenly distributed, which is good for modeling. Some numeric variables are highly skewed, such as dup, dui, and family_history. We will normalize the data to lessen the impact of these outliers. These are not significant enough to remove from the data.\n\n\nCode\nfull_data %&gt;% summary()\n\n\n      code     Program sex     age_psychosis   family_history hospita   \n 1      :  1   1:174   0:311   Min.   :14.81   1   :127       1   :379  \n 2      :  1   2: 20   1:238   1st Qu.:21.65   2   :420       2   :169  \n 3      :  1   3:203           Median :26.92   NA's:  2       NA's:  1  \n 4      :  1   4:152           Mean   :28.92                            \n 5      :  1                   3rd Qu.:34.25                            \n 6      :  1                   Max.   :59.80                            \n (Other):543                                                            \n      dui              dup         levelsocioeco urbanarea  livingwithparents\n Min.   :  0.10   Min.   :  0.06   1   :282      1   :386   1   :273         \n 1st Qu.:  2.00   1st Qu.:  1.00   2   :249      2   :154   2   :270         \n Median : 10.00   Median :  3.00   NA's: 18      NA's:  9   NA's:  6         \n Mean   : 21.88   Mean   : 12.51                                             \n 3rd Qu.: 24.00   3rd Qu.: 12.00                                             \n Max.   :288.00   Max.   :240.00                                             \n                                                                             \n unmarried  unemployed   years_edu     CannabisBinary     SAPS0      \n 1   :397   1   :235   Min.   : 6.00   0:236          Min.   : 1.00  \n 2   :147   2   :308   1st Qu.: 8.00   1:313          1st Qu.:10.00  \n NA's:  5   NA's:  6   Median :10.00                  Median :14.00  \n                       Mean   :10.13                  Mean   :13.76  \n                       3rd Qu.:12.00                  3rd Qu.:17.00  \n                       Max.   :17.00                  Max.   :25.00  \n                                                                     \n Psychoticdim0    Disorganizeddim0 Negativedimen0       dasgl0      diagnosis\n Min.   : 0.000   Min.   : 0.000   Min.   : 0.000   Min.   :0.000   0:278    \n 1st Qu.: 5.000   1st Qu.: 4.000   1st Qu.: 0.000   1st Qu.:0.000   1:271    \n Median : 7.000   Median : 5.000   Median : 2.000   Median :1.000            \n Mean   : 7.405   Mean   : 6.358   Mean   : 4.785   Mean   :1.412            \n 3rd Qu.:10.000   3rd Qu.: 9.000   3rd Qu.: 8.000   3rd Qu.:3.000            \n Max.   :10.000   Max.   :15.000   Max.   :20.000   Max.   :5.000            \n                                                                             \n    insight           cds0    \n Min.   :1.000   Min.   : NA  \n 1st Qu.:1.000   1st Qu.: NA  \n Median :2.000   Median : NA  \n Mean   :1.568   Mean   :NaN  \n 3rd Qu.:2.000   3rd Qu.: NA  \n Max.   :2.000   Max.   : NA  \n                 NA's   :549  \n\n\n\n\nCode\nfull_data %&gt;%\n    ggplot(aes(x = age_psychosis, y = years_edu, color = CannabisBinary)) +\n    geom_jitter(alpha = 0.8) +\n    facet_wrap(~CannabisBinary) +\n    geom_smooth()\n\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nHere we attempt to see if there is any relationship between the age of psychosis, the number of years of education, and cannabis use. There does not appear to be any relationship between these variables, indicating that cannabis use is not strongly predicted by age of psychosis or years of education.\n\n\nCode\nfull_data %&gt;%\n    ggplot(aes(x = age_psychosis, y = dui, color = diagnosis)) +\n    geom_jitter(alpha = 0.8) +\n    facet_wrap(~diagnosis) +\n    geom_smooth()\n\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nHere we examine the relationship between the age of psychosis, DUI, and diagnosis. Interestingly, we see that individuals with no schizophrenia diagnosis are more likely to have a DUI than individuals with a schizophrenia diagnosis. This is surprising, as we would expect individuals with schizophrenia to be more likely to have a DUI. This may be due to the fact that individuals with schizophrenia are less likely to drive.\n\n\nCode\nfull_data %&gt;%\n    ggplot(aes(x = age_psychosis, y = dui, color = CannabisBinary)) +\n    geom_jitter(alpha = 0.8) +\n    facet_wrap(~CannabisBinary)\n\n\n\n\n\nNext, we examine the relationship between the age of psychosis, DUI, and cannabis use. We see that individuals with a DUI are more likely to use cannabis than individuals without a DUI. This is not surprising, as we would expect individuals who use cannabis to be more likely to use other substances as well, or they may have received a DUI for driving under the influence of cannabis.\n\n\nCode\nfull_data %&gt;%\n    ggplot() +\n    geom_boxplot(aes(x = diagnosis, y = age_psychosis, fill = diagnosis))\n\n\n\n\n\nHere we examine the age of pychosis onset and the diagnosis. It appears that individuals with a schizophrenia diagnosis tend to have a slightly earlier age of psychosis onset than individuals without a schizophrenia diagnosis. This is not surprising, as schizophrenia tends to be diagnosed in early adulthood.\n\n\nCode\nfull_data %&gt;%\n    ggplot() +\n    geom_boxplot(aes(x = CannabisBinary, y = age_psychosis, fill = CannabisBinary))\n\n\n\n\n\nHere we see that cannabis users tend to have a significantly earlier age of psychosis onset than non-cannabis users. This is some of the most interesting evidence in support of my research, as it indicates that cannabis use may be related to the onset of psychosis.\n\n\nCode\nfull_data %&gt;%\n    ggplot() +\n    geom_histogram(aes(x = age_psychosis, fill = sex), bins = 20) +\n    facet_wrap(~sex)\n\n\n\n\n\nHere we see that men tend to have an earlier age of onset of psychosis.\n\n\nCode\nfull_data %&gt;%\n    ggplot() +\n    geom_histogram(aes(x = age_psychosis, fill = CannabisBinary), bins = 20) +\n    facet_wrap(~CannabisBinary)\n\n\n\n\n\nOnce more, we see that cannabis usage is linked to an earlier age of psychosis onset. This is strong evidence in support of my research, and it indicates that cannabis usage may be related to the onset of psychosis.\n\n\nCode\nfull_data %&gt;%\n    drop_na(levelsocioeco) %&gt;%\n    ggplot() +\n    geom_bar(aes(x = levelsocioeco, fill = CannabisBinary), bins = 20) +\n    facet_wrap(~CannabisBinary)\n\n\nWarning message in geom_bar(aes(x = levelsocioeco, fill = CannabisBinary), bins = 20):\n\"Ignoring unknown parameters: `bins`\"\n\n\n\n\n\nThis visual shows that there is not a strong relationship between socio economic status and cannabis usage. This is not surprising, as cannabis usage is common both among the wealthy and the poor.\n\n\nCode\nfull_data %&gt;%\n    group_by(CannabisBinary, diagnosis) %&gt;%\n    summarize(n = n()) %&gt;%\n    ungroup() %&gt;%\n    group_by(CannabisBinary) %&gt;%\n    mutate(diagnosis_gvn_cannabis = n / sum(n)) %&gt;%\n    ungroup() %&gt;%\n    group_by(diagnosis) %&gt;%\n    mutate(cannabis_gvn_diagnosis = n / sum(n)) %&gt;%\n    ungroup()\n\n\n`summarise()` has grouped output by 'CannabisBinary'. You can override using\nthe `.groups` argument.\n\n\n\nA tibble: 4 x 5\n\n\nCannabisBinary\ndiagnosis\nn\ndiagnosis_gvn_cannabis\ncannabis_gvn_diagnosis\n\n\n&lt;fct&gt;\n&lt;fct&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n0\n0\n119\n0.5042373\n0.4280576\n\n\n0\n1\n117\n0.4957627\n0.4317343\n\n\n1\n0\n159\n0.5079872\n0.5719424\n\n\n1\n1\n154\n0.4920128\n0.5682657\n\n\n\n\n\nIn this table, we see that there are about even numbers of individuals with schizophrenia who use cannabis, individuals with schizophrenia who do not use cannabis, other cannabis users, and individuals who do not use cannabis in the data. All four groups have experienced psychosis, so we can use this data to understand the impact of cannabis on psychosis. Here we see that probabilistically, the chance of using cannabis given a schizophrenia diagnosis is about 0.57, while the chance of having a schizophrenia diagnosis given cannabis usage is about 0.49. Neither of these probabilities indicate a very strong relationship between cannabis usage and schizophrenia diagnosis.\n\n\nCode\nfull_data %&gt;% write_csv(\"../data/clean_data/full_data.csv\")\n\n\n\n\n\nIn my data exploration, I utilized the tidyverse, tidytext, ggplot2 to understand my data. These tools developed by Posit have very thorough documentation and similar APIs, making them quick to learn, user friendly, and customizable. ggplot2 follows the grammar of graphics, making plots easy to build and modify based on the requirements.\nFor the text data, I analyzed the word counts, word length, and word frequency across different categorizations. For the Reddit data, I used subreddit title as the label. For the Wikipedia pages, I categorized them based on whether they mentioned cannabis, psychosis, both, or neither. I was able to understand the themes of each different category as well as tone through the analysis.\nFor the record data, I used dplyr to join the two datasets and ggplot2 to visualize the data. It was clear that a relationship between age of psychosis onset and cannabis usage exists, but the relationship between cannabis usage and diagnosis was less clear. It would be interesting to explore a wider dataset with more individuals, including individuals who have not experienced psychosis, to see if the relationship between cannabis usage and diagnosis is more clear.\nMy hypothesis was that cannabis usage is related to the onset of psychosis. My analysis of the record data supports this hypothesis, as I found that cannabis users tend to have an earlier age of psychosis onset than non-cannabis users. This is strong evidence in support of my research, and it indicates that cannabis usage may be related to the onset of psychosis. I am still interested in the sentiment of the public around the impact of cannabis on psychosis and schizophrenia, so I will continue to explore the text data to understand this."
  },
  {
    "objectID": "eda/eda.html#text-data",
    "href": "eda/eda.html#text-data",
    "title": "Data Exploration",
    "section": "",
    "text": "To begin, we can explore the textual data gathered that relates to psychosis and cannabis. This text data will be essential for answering questions regarding public sentiment related to the impact of cannabis on psychosis and/or schizophrenia.\nWe’ll start by cleaning up the text data in order to make visualizations and analyze the data.\n\n\nCode\nlibrary(tidyverse, quietly = TRUE, warn.conflicts = FALSE)\n\nwiki_data &lt;- read_csv(\"../data/clean_data/wiki_cleaned_text.csv\")\n\n\nRows: 397 Columns: 2\n-- Column specification --------------------------------------------------------\nDelimiter: \",\"\nchr (2): link, text\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nCode\nreddit_data &lt;- read_csv(\n        \"../data/clean_data/reddit_cleaned_text.csv\"\n    )\n\n\nRows: 30000 Columns: 2\n-- Column specification --------------------------------------------------------\nDelimiter: \",\"\nchr (2): label, text\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nCode\nlibrary(ggwordcloud)\nlibrary(tidytext)\nlibrary(wordcloud)\n\n\n\n\nCode\nwiki_data &lt;- wiki_data %&gt;%\n    mutate(cannabis_label = case_when(\n        str_detect(text, \"cannabis|marijuana|weed|THC|CBD\") ~ \"cannabis\",\n        TRUE ~ \"no cannabis\"\n    )) %&gt;%\n    mutate(schiz_label = case_when(\n        str_detect(text, \"psychosis|psychotic|schizophrenia|schizophrenic|schizo|schizoaffective|schizotypal|schizoid|schiz\") ~ \"psychosis\",\n        TRUE ~ \"no psychosis\"\n    )) %&gt;%\n    mutate(label = case_when(\n        cannabis_label == \"cannabis\" & schiz_label == \"psychosis\" ~ \"cannabis and psychosis\",\n        cannabis_label == \"cannabis\" & schiz_label == \"no psychosis\" ~ \"cannabis\",\n        cannabis_label == \"no cannabis\" & schiz_label == \"psychosis\" ~ \"psychosis\",\n        TRUE ~ \"no cannabis and no psychosis\"\n    ))\n\n\n\n\nCode\ntidy_wiki &lt;- wiki_data %&gt;%\n    distinct() %&gt;%\n    filter(!is.na(text)) %&gt;%\n    unnest_tokens(word, text) %&gt;%\n    group_by(label, word) %&gt;%\n    summarise(n = n()) %&gt;%\n    ungroup() %&gt;%\n    group_by(word) %&gt;%\n    mutate(word_count = sum(n)) %&gt;%\n    ungroup() %&gt;%\n    arrange(-word_count)\n\n\n`summarise()` has grouped output by 'label'. You can override using the\n`.groups` argument.\n\n\n\n\nCode\ntidy_wiki &lt;- tidy_wiki %&gt;%\n  anti_join(stop_words) %&gt;%\n  filter(word != \"numbr\")\n\n\nJoining with `by = join_by(word)`\n\n\nNow that the Wikipedia data is clean, let’s visualize it to get a deeper understanding of the data.\n\n\nCode\nlibrary(wordcloud)\n\ntidy_wiki %&gt;%\n    select(word, word_count) %&gt;%\n    distinct() %&gt;%\n    with(wordcloud(word, word_count, max.words = 200, colors = brewer.pal(8, \"Dark2\")))\n\n\n\n\n\nThis wordcloud indicates that the Wikipedia text refers to cannabis as a drug, emphasizing medical language and medical effects of the substance. There does not appear to be any mention of psychosis or schizophrenia in the text, indicating that it is not a common topic of discussion on the Wikipedia pages relevant to cannabis usage.\n\n\nCode\ntidy_wiki %&gt;%\n    filter(stringi::stri_enc_isascii(word)) %&gt;%\n    group_by(label) %&gt;%\n    mutate(m = n / sum(n)) %&gt;%\n    arrange(-m) %&gt;%\n    slice_head(n = 50) %&gt;%\n    ggplot(aes(label = word, size = m, color = label)) +\n    geom_text_wordcloud_area(rm_outside = TRUE) +\n    scale_size_area(max_size = 20) +\n    theme_minimal() +\n    facet_wrap(~label)\n\n\n\n\n\nHere, we visualize the text based on whether it mentions cannabis, psychosis, neither, or both. We notice that texts mentioning psychosis tend to have language that relates more to addiction and science as it relates to cannabis. This indicates that conversations around mental health and cannabis tend to be more scientific in nature.\n\n\nCode\ntidy_wiki %&gt;%\n    mutate(nchar = nchar(word)) %&gt;%\n    group_by(label) %&gt;%\n    summarize(mean = mean(nchar))\n\n\n\nA tibble: 4 x 2\n\n\nlabel\nmean\n\n\n&lt;chr&gt;\n&lt;dbl&gt;\n\n\n\n\ncannabis\n7.696249\n\n\ncannabis and psychosis\n7.767887\n\n\nno cannabis and no psychosis\n7.631243\n\n\npsychosis\n8.038816\n\n\n\n\n\nWe see here that the average word length is very similar across all found categories. However, it tends to be slightly higher when psychosis is mentioned. This may indicate a more scientific discussion, as scientific terms tend to be longer than average, but this is somewhat speculative.\n\n\nCode\nwiki_data %&gt;%\n    distinct() %&gt;%\n    filter(!is.na(text)) %&gt;%\n    mutate(sentence_length = stringr::str_count(text, \"\\\\S+\")) %&gt;%\n    group_by(label) %&gt;%\n    summarize(mean = mean(sentence_length), median = median(sentence_length), n = n())\n\n\n\nA tibble: 4 x 4\n\n\nlabel\nmean\nmedian\nn\n\n\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;int&gt;\n\n\n\n\ncannabis\n783.2434\n266.5\n152\n\n\ncannabis and psychosis\n2391.0417\n2006.5\n24\n\n\nno cannabis and no psychosis\n388.6061\n69.0\n198\n\n\npsychosis\n2205.4706\n1381.0\n17\n\n\n\n\n\nTexts that mentions psychosis are significantly longer in word count that texts that do not mention psychosis. This suggests that these conversations may be much more nuanced and detailed than conversations that do not mention psychosis. Another factor may be that we have less texts that mention psychosis than those that do not, so the average word count may be skewed.\n\n\nCode\ntidy_wiki %&gt;%\n    group_by(label) %&gt;%\n    summarise(n = n())\n\n\n\nA tibble: 4 x 2\n\n\nlabel\nn\n\n\n&lt;chr&gt;\n&lt;int&gt;\n\n\n\n\ncannabis\n12902\n\n\ncannabis and psychosis\n7673\n\n\nno cannabis and no psychosis\n10812\n\n\npsychosis\n6183\n\n\n\n\n\nThis table shows overall word counts by category. This is not very useful since the categories are not evenly distributed.\n\n\nCode\ntidy_wiki %&gt;%\n    group_by(label) %&gt;%\n    mutate(total = sum(n)) %&gt;%\n    ggplot(aes(n/total, fill = label)) +\n    geom_histogram(show.legend = FALSE) +\n    facet_wrap(~label, ncol = 2, scales = \"free_y\")\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nAs expected, words with low frequency are much more common across all 4 categories than words with high frequency. This follows Zipf’s law. Texts that mention psychosis appear to have slightly more repeated language than texts that do not mention psychosis.\n\n\nCode\ntidy_wiki %&gt;%\n  group_by(label) %&gt;%\n  mutate(total = sum(n)) %&gt;%\n  arrange(label, -n) %&gt;%\n  mutate(rank = row_number(), term_freq = n/total) %&gt;%\n  ungroup() %&gt;%\n  ggplot(aes(rank, term_freq, color = label)) + \n  geom_line(size = 1, alpha = 0.8, show.legend = FALSE) +\n  scale_x_log10() +\n  scale_y_log10()\n\n\n\n\n\nAgain, we see a very clear display of Zipf’s law, indicating the inverse nature between term frequency and rank. This means that the most common words are much more common than the least common words. We see here that this remains true across all categories in our data.\n\n\nCode\ntidy_wiki %&gt;%\n  group_by(label) %&gt;%\n  slice_max(n, n = 15) %&gt;%\n  ungroup() %&gt;%\n  ggplot(aes(n, fct_reorder(word, n), fill = label)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~label, ncol = 2, scales = \"free\")\n\n\n\n\n\nWhen looking at the top most frequent terms for each category of Wikipedia article, we see that texts that mention cannabis and psychosis focus on the effects of cannabis and THC specificallly. It is also notable that texts which only mention psychosis also mention drugs, indicating the high prevalence of drug use in individuals with psychosis. Texts that mention only cannabis include “law” as a top word, indicting discussions about the legality of cannabis usage.\n\n\nCode\ntidy_wiki %&gt;%\n    select(word, word_count) %&gt;%\n    distinct() %&gt;%\n    filter(word_count &gt;= 600) %&gt;%\n    ggplot() +\n    geom_bar(aes(y = fct_reorder(word, word_count), x = word_count), stat = \"identity\")\n\n\n\n\n\nOverall, we see the theme of “effect”, indicating that the text is focused on identifying the impact of cannabis usage. This indicates that these texts will be helpful in identifying public sentiment around the impact of cannabis on psychosis and schizophrenia, as texts are focused on the impact of cannabis usage.\nNow, let’s take a deeper look at our Reddit data.\n\n\nCode\ntidy_reddit &lt;- reddit_data %&gt;%\n    distinct() %&gt;%\n    filter(!is.na(text)) %&gt;%\n    unnest_tokens(word, text) %&gt;%\n    group_by(label, word) %&gt;%\n    summarise(n = n()) %&gt;%\n    ungroup() %&gt;%\n    group_by(word) %&gt;%\n    mutate(word_count = sum(n)) %&gt;%\n    ungroup() %&gt;%\n    arrange(-word_count)\n\n\n`summarise()` has grouped output by 'label'. You can override using the\n`.groups` argument.\n\n\n\n\nCode\ntidy_reddit &lt;- tidy_reddit %&gt;%\n  anti_join(stop_words) %&gt;%\n  filter(word != \"numbr\")\n\n\nJoining with `by = join_by(word)`\n\n\n\n\nCode\ntidy_reddit %&gt;%\n    select(word, word_count) %&gt;%\n    distinct() %&gt;%\n    with(wordcloud(word, word_count, max.words = 100, colors = brewer.pal(8, \"Dark2\")))\n\n\n\n\n\nImmediately, we can sense a difference in tone between Wikipedia data and Reddit data. These texts are much more casual and anecdotal, rather than clinical and scientific. This indicates that Reddit data will be helpful in identifying public sentiment around the impact of cannabis on psychosis and schizophrenia, as the word “feel” is one of the most commonly used.\n\n\nCode\nggplot(\n  tidy_reddit %&gt;% filter(stringi::stri_enc_isascii(word)) %&gt;% filter(n &gt;= 2),\n  aes(\n    label = word, size = n, color = label\n  )\n) +\n  geom_text_wordcloud_area(rm_outside = TRUE) +\n  scale_size_area(max_size = 20) +\n  theme_minimal() +\n  facet_wrap(~label)\n\n\nWarning message in wordcloud_boxes(data_points = points_valid_first, boxes = boxes, :\n\"Some words could not fit on page. They have been removed.\"\n\n\n\n\n\nHere we see the breakdown of the most common words across the subreddits. It is clear that the “r/weed” subreddit consists largely of stories about being high. The “r/schizophrenia” and “r/Psychosis” subreddits include stories of delusions and express emotions. It will be interesting to understand the overlap between these three subreddits.\n\n\nCode\ntidy_reddit %&gt;%\n    mutate(nchar = nchar(word)) %&gt;%\n    group_by(label) %&gt;%\n    summarize(mean = mean(nchar), median = median(nchar))\n\n\n\nA tibble: 3 x 3\n\n\nlabel\nmean\nmedian\n\n\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nPsychosis\n6.411458\n6\n\n\nschizophrenia\n6.291242\n6\n\n\nweed\n5.555046\n5\n\n\n\n\n\nThe average word count is highest for “r/Psychosis”, followed by “r/schizophrenia”, and then “r/weed”. This indicates that the texts in “r/Psychosis” are more detailed than those in “r/weed”. This seems to indicate that individuals are telling detailed stories about their experiences with psychosis and schizophrenia on the “r/Psychosis” and “r/schizophrenia” subreddits.\n\n\nCode\nreddit_data %&gt;%\n    distinct() %&gt;%\n    filter(!is.na(text)) %&gt;%\n    mutate(sentence_length = stringr::str_count(text, \"\\\\S+\")) %&gt;%\n    group_by(label) %&gt;%\n    summarize(mean = mean(sentence_length), median = median(sentence_length))\n\n\n\nA tibble: 3 x 3\n\n\nlabel\nmean\nmedian\n\n\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\nPsychosis\n26.979381\n5\n\n\nschizophrenia\n10.802083\n4\n\n\nweed\n8.262626\n4\n\n\n\n\n\nThis analysis confirms that individuals are writing many more words in “r/Psychosis” than in the other two subreddits. It is interesting that “r/Psychosis” has a much higher word count than “r/schizophrenia”, even though I would expect them to be similar. We also see that the means are all significantly higher than the medians, indicating that there are a few outliers of very long text in the data.\n\n\nCode\ntidy_reddit %&gt;%\n    group_by(label) %&gt;%\n    summarise(n = n())\n\n\n\nA tibble: 3 x 2\n\n\nlabel\nn\n\n\n&lt;chr&gt;\n&lt;int&gt;\n\n\n\n\nPsychosis\n960\n\n\nschizophrenia\n491\n\n\nweed\n436\n\n\n\n\n\nWe see here that the total number of words used across all the texts is almost twice as high in “r/Psychosis”. This again points to the fact that people are writing more on “r/Psychosis” than on the other two subreddits, so they are using a more diverse subset of words.\n\n\nCode\ntidy_reddit %&gt;%\n    filter(stringi::stri_enc_isascii(word)) %&gt;%\n    group_by(label) %&gt;%\n    arrange(-n) %&gt;%\n    slice_head(n = 10) %&gt;%\n    ggplot(aes(x = n, y = word, fill = label)) +\n    geom_bar(stat = \"identity\") +\n    theme_minimal() +\n    facet_wrap(~label, scales = \"free_y\")\n\n\n\n\n\nFrom this visual of the top words in each subreddit, we see that “r/Psychosis” tends to focus on feelings and tell stories of experiences with psychosis. “r/schizophrenia” tends to focus on symptoms and experiences with medication. “r/weed” tends to focus on the experience of being high and partaking in cannabis usage.\n\n\nCode\ntidy_reddit %&gt;%\n    group_by(label) %&gt;%\n    mutate(total = sum(n)) %&gt;%\n    ggplot(aes(n/total, fill = label)) +\n    geom_histogram(show.legend = FALSE) +\n    facet_wrap(~label, ncol = 3, scales = \"free_y\")\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nHere we see again that the most common words are much more common than the least common words, indicating Zipf’s law. We also see that the texts in “r/Psychosis” have more repeated language than the other two subreddits, probably due to the fact that people are writing more in “r/Psychosis”.\n\n\nCode\ntidy_reddit %&gt;%\n  group_by(label) %&gt;%\n  mutate(total = sum(n)) %&gt;%\n  arrange(label, -n) %&gt;%\n  mutate(rank = row_number(), term_freq = n/total) %&gt;%\n  ungroup() %&gt;%\n  ggplot(aes(rank, term_freq, color = label)) + \n  geom_line(size = 1, alpha = 0.8, show.legend = FALSE) +\n  scale_x_log10() +\n  scale_y_log10()\n\n\n\n\n\nOnce again, we see that for each subreddit, the most common words are much more common than the least common words.\nNow that we have a better understanding of the text data, we can begin to explore our numerical data."
  },
  {
    "objectID": "eda/eda.html#record-data",
    "href": "eda/eda.html#record-data",
    "title": "Data Exploration",
    "section": "",
    "text": "In order to understand our numerical data, we will analyze relationships through numerical summaries and visualizations. We will also look at correlation between variables in our data in order to understand the relationships between variables and remove highly correlated variables from our data.\n\n\nCode\ns1 &lt;- read_csv(\"../data/clean_data/s1file_clean.csv\")\n\n\nRows: 549 Columns: 24\n-- Column specification --------------------------------------------------------\nDelimiter: \",\"\ndbl (24): Code, Program, sex, age, age_psychosis, famhis, hospita, dui, dup,...\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nCode\ns2 &lt;- read_csv(\"../data/clean_data/s2file_clean.csv\")\n\n\nRows: 477 Columns: 23\n-- Column specification --------------------------------------------------------\nDelimiter: \",\"\ndbl (23): code, Program, sex, age, agepsychosis, fampsic, hospita, dui, dup,...\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nNow that we have the s1 and s2 files, let’s figure out how to join them. Using setdiff below, we see that s2 has a few columns not in s1.\n\n\nCode\nsetdiff(names(s1), names(s2))\n\n\n\n'Code''age_psychosis''famhis''levelsocioeco''dasgl0''Diagnosisobinnary''insight'\n\n\n\n\nCode\nsetdiff(names(s2), names(s1))\n\n\n\n'code''agepsychosis''fampsic''levelecon''cds0''Diagnosisbinario'\n\n\n\n\nCode\nfull_data &lt;- s1 %&gt;% \n    rename(\n        code = Code,\n        diagnosis = Diagnosisobinnary,\n        family_history = famhis\n    ) %&gt;%\n    left_join(\n        s2 %&gt;%\n            rename(\n                diagnosis = Diagnosisbinario,\n                age_psychosis = agepsychosis,\n                family_history = fampsic,\n                levelsocioeco = levelecon\n            )\n    )\n\n\nJoining with `by = join_by(code, Program, sex, age, age_psychosis,\nfamily_history, hospita, dui, dup, levelsocioeco, urbanarea, livingwithparents,\nunmarried, unemployed, years_edu, CannabisBinary, SAPS0, SANS0, Psychoticdim0,\nDisorganizeddim0, Negativedimen0, diagnosis)`\n\n\nWe perform a left_join of the two datasets since s2 is a subset of s1 with additional columns. We can see that the s2 columns are added to the s1 columns in our new joined dataset.\n\n\nCode\nfull_data %&gt;% head()\n\n\n\nA tibble: 6 x 25\n\n\ncode\nProgram\nsex\nage\nage_psychosis\nfamily_history\nhospita\ndui\ndup\nlevelsocioeco\n...\nCannabisBinary\nSAPS0\nSANS0\nPsychoticdim0\nDisorganizeddim0\nNegativedimen0\ndasgl0\ndiagnosis\ninsight\ncds0\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n...\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n1\n1\n22.24658\n22.16324\n2\n2\n2.0\n1.0\n1\n...\n1\n6\n6\n4\n2\n5\n2\n1\n1\nNA\n\n\n2\n1\n0\n32.67945\n32.63779\n1\n2\n0.5\n0.5\n2\n...\n1\n9\n5\n8\n1\n4\n0\n1\n1\nNA\n\n\n3\n1\n0\n30.23562\n30.15228\n2\n1\n36.0\n1.0\nNA\n...\n1\n11\n2\n4\n7\n0\nNA\n1\nNA\nNA\n\n\n4\n1\n0\n24.80274\n24.46941\n2\n2\n18.0\n4.0\n2\n...\n1\n14\n10\n8\n6\n7\n0\n0\n2\nNA\n\n\n5\n1\n0\n22.38082\n21.38082\n2\n1\n28.0\n12.0\n1\n...\n1\n11\n22\n4\n7\n18\n2\n0\n2\nNA\n\n\n6\n1\n0\n17.07671\n16.99338\n2\n1\n1.0\n1.0\n2\n...\n1\n10\n5\n4\n6\n5\n0\n1\n2\nNA\n\n\n\n\n\n\n\nCode\nfull_data &lt;- full_data %&gt;%\n    mutate(\n        across(\n            c(code, Program, sex, family_history, diagnosis, \n            hospita, levelsocioeco, urbanarea, livingwithparents,\n            unmarried, unemployed, CannabisBinary\n            ),\n            ~ as.factor(.x)\n        )\n    )\n\n\n\n\nCode\nfull_data %&gt;%\n    ggplot() +\n    geom_bar(aes(x = diagnosis, fill = diagnosis)) +\n    theme_minimal()\n\n\n\n\n\nHere we see that the data is well distributed between individuals with a diagnosis of schizophrenia and those without. This is important because we want to make sure that our data is not skewed towards one group or the other. Evenly distributed data will help us build better models and make better predictions.\n\n\nCode\nfull_data %&gt;%\n    ggplot() +\n    geom_bar(aes(x = diagnosis, fill = sex))\n\n\n\n\n\nOur data is fairly evenly distributed between genders. The patients without schizophrenia are slightly skewed male.\n\n\nCode\nfull_data %&gt;%\n    ggplot() +\n    geom_bar(aes(x = diagnosis, fill = CannabisBinary))\n\n\n\n\n\nIn this plot, it is clear that the spread of individuals using cannabis is even across both groups. This will help us make clear predictions and gain a better understanding of the impact of cannabis on diagnosis by having all groups evenly represented.\n\n\nCode\nlibrary(corrplot)\nfull_data %&gt;%\n    select(-cds0) %&gt;%\n    select(where(is.numeric)) %&gt;%\n    # fill in all missing values with the mean\n    mutate(across(where(is.numeric), ~ ifelse(is.na(.x), mean(.x, na.rm = TRUE), .x))) %&gt;%\n    # normalize all numeric variables\n    mutate(across(where(is.numeric), ~ (.x - min(.x)) / (max(.x) - min(.x)))) %&gt;%\n    cor() %&gt;%\n    corrplot.mixed(order = 'AOE', upper = 'circle', tl.col = 'black')\n\n\n\n\n\nThrough a correlation plot of all numeric variables, we see that very few variables are correlated. This is great because it means that our features are relatively independent of each other, which will help us build better models.\nThere are two sets of highly correlated features: Negativedimen0 and SANS0; and age_psychosis and age. The two age variables are obviously clearly correlated because the data was collected shortly after the onset of pychosis, so we will remove age. We also remove SANS0 due to low interpretability.\n\n\nCode\nfull_data %&gt;%\n    select(-cds0) %&gt;%\n    select(where(is.factor)) %&gt;%\n    drop_na() %&gt;%\n    mutate(across(where(is.factor), ~ as.numeric(.x))) %&gt;%\n    # normalize all numeric variables\n    mutate(across(where(is.numeric), ~ (.x - min(.x)) / (max(.x) - min(.x)))) %&gt;%\n    cor() %&gt;%\n    corrplot.mixed(order = 'AOE', upper = 'circle', tl.col = 'black')\n\n\n\n\n\nNext, we visualize the correlation between all the categorical variables as numeric variables. We see that most of the features are not highly correlated. Program and code are correlated, but we will remove both of these variables before modeling as they are both ID variables rather than features.\n\n\nCode\nfull_data &lt;- full_data %&gt;%\n    select(-SANS0, -age) %&gt;%\n    mutate(across(where(is.numeric), ~ ifelse(is.na(.x), mean(.x, na.rm = TRUE), .x)))\n\n\nNext, we will look at a full numeric summary of all variables. This tells us the mean, median, and quartiles of each numeric variable as well as the distribution of factor variables. it is notable that most of the factor variables are fairly evenly distributed, which is good for modeling. Some numeric variables are highly skewed, such as dup, dui, and family_history. We will normalize the data to lessen the impact of these outliers. These are not significant enough to remove from the data.\n\n\nCode\nfull_data %&gt;% summary()\n\n\n      code     Program sex     age_psychosis   family_history hospita   \n 1      :  1   1:174   0:311   Min.   :14.81   1   :127       1   :379  \n 2      :  1   2: 20   1:238   1st Qu.:21.65   2   :420       2   :169  \n 3      :  1   3:203           Median :26.92   NA's:  2       NA's:  1  \n 4      :  1   4:152           Mean   :28.92                            \n 5      :  1                   3rd Qu.:34.25                            \n 6      :  1                   Max.   :59.80                            \n (Other):543                                                            \n      dui              dup         levelsocioeco urbanarea  livingwithparents\n Min.   :  0.10   Min.   :  0.06   1   :282      1   :386   1   :273         \n 1st Qu.:  2.00   1st Qu.:  1.00   2   :249      2   :154   2   :270         \n Median : 10.00   Median :  3.00   NA's: 18      NA's:  9   NA's:  6         \n Mean   : 21.88   Mean   : 12.51                                             \n 3rd Qu.: 24.00   3rd Qu.: 12.00                                             \n Max.   :288.00   Max.   :240.00                                             \n                                                                             \n unmarried  unemployed   years_edu     CannabisBinary     SAPS0      \n 1   :397   1   :235   Min.   : 6.00   0:236          Min.   : 1.00  \n 2   :147   2   :308   1st Qu.: 8.00   1:313          1st Qu.:10.00  \n NA's:  5   NA's:  6   Median :10.00                  Median :14.00  \n                       Mean   :10.13                  Mean   :13.76  \n                       3rd Qu.:12.00                  3rd Qu.:17.00  \n                       Max.   :17.00                  Max.   :25.00  \n                                                                     \n Psychoticdim0    Disorganizeddim0 Negativedimen0       dasgl0      diagnosis\n Min.   : 0.000   Min.   : 0.000   Min.   : 0.000   Min.   :0.000   0:278    \n 1st Qu.: 5.000   1st Qu.: 4.000   1st Qu.: 0.000   1st Qu.:0.000   1:271    \n Median : 7.000   Median : 5.000   Median : 2.000   Median :1.000            \n Mean   : 7.405   Mean   : 6.358   Mean   : 4.785   Mean   :1.412            \n 3rd Qu.:10.000   3rd Qu.: 9.000   3rd Qu.: 8.000   3rd Qu.:3.000            \n Max.   :10.000   Max.   :15.000   Max.   :20.000   Max.   :5.000            \n                                                                             \n    insight           cds0    \n Min.   :1.000   Min.   : NA  \n 1st Qu.:1.000   1st Qu.: NA  \n Median :2.000   Median : NA  \n Mean   :1.568   Mean   :NaN  \n 3rd Qu.:2.000   3rd Qu.: NA  \n Max.   :2.000   Max.   : NA  \n                 NA's   :549  \n\n\n\n\nCode\nfull_data %&gt;%\n    ggplot(aes(x = age_psychosis, y = years_edu, color = CannabisBinary)) +\n    geom_jitter(alpha = 0.8) +\n    facet_wrap(~CannabisBinary) +\n    geom_smooth()\n\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nHere we attempt to see if there is any relationship between the age of psychosis, the number of years of education, and cannabis use. There does not appear to be any relationship between these variables, indicating that cannabis use is not strongly predicted by age of psychosis or years of education.\n\n\nCode\nfull_data %&gt;%\n    ggplot(aes(x = age_psychosis, y = dui, color = diagnosis)) +\n    geom_jitter(alpha = 0.8) +\n    facet_wrap(~diagnosis) +\n    geom_smooth()\n\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nHere we examine the relationship between the age of psychosis, DUI, and diagnosis. Interestingly, we see that individuals with no schizophrenia diagnosis are more likely to have a DUI than individuals with a schizophrenia diagnosis. This is surprising, as we would expect individuals with schizophrenia to be more likely to have a DUI. This may be due to the fact that individuals with schizophrenia are less likely to drive.\n\n\nCode\nfull_data %&gt;%\n    ggplot(aes(x = age_psychosis, y = dui, color = CannabisBinary)) +\n    geom_jitter(alpha = 0.8) +\n    facet_wrap(~CannabisBinary)\n\n\n\n\n\nNext, we examine the relationship between the age of psychosis, DUI, and cannabis use. We see that individuals with a DUI are more likely to use cannabis than individuals without a DUI. This is not surprising, as we would expect individuals who use cannabis to be more likely to use other substances as well, or they may have received a DUI for driving under the influence of cannabis.\n\n\nCode\nfull_data %&gt;%\n    ggplot() +\n    geom_boxplot(aes(x = diagnosis, y = age_psychosis, fill = diagnosis))\n\n\n\n\n\nHere we examine the age of pychosis onset and the diagnosis. It appears that individuals with a schizophrenia diagnosis tend to have a slightly earlier age of psychosis onset than individuals without a schizophrenia diagnosis. This is not surprising, as schizophrenia tends to be diagnosed in early adulthood.\n\n\nCode\nfull_data %&gt;%\n    ggplot() +\n    geom_boxplot(aes(x = CannabisBinary, y = age_psychosis, fill = CannabisBinary))\n\n\n\n\n\nHere we see that cannabis users tend to have a significantly earlier age of psychosis onset than non-cannabis users. This is some of the most interesting evidence in support of my research, as it indicates that cannabis use may be related to the onset of psychosis.\n\n\nCode\nfull_data %&gt;%\n    ggplot() +\n    geom_histogram(aes(x = age_psychosis, fill = sex), bins = 20) +\n    facet_wrap(~sex)\n\n\n\n\n\nHere we see that men tend to have an earlier age of onset of psychosis.\n\n\nCode\nfull_data %&gt;%\n    ggplot() +\n    geom_histogram(aes(x = age_psychosis, fill = CannabisBinary), bins = 20) +\n    facet_wrap(~CannabisBinary)\n\n\n\n\n\nOnce more, we see that cannabis usage is linked to an earlier age of psychosis onset. This is strong evidence in support of my research, and it indicates that cannabis usage may be related to the onset of psychosis.\n\n\nCode\nfull_data %&gt;%\n    drop_na(levelsocioeco) %&gt;%\n    ggplot() +\n    geom_bar(aes(x = levelsocioeco, fill = CannabisBinary), bins = 20) +\n    facet_wrap(~CannabisBinary)\n\n\nWarning message in geom_bar(aes(x = levelsocioeco, fill = CannabisBinary), bins = 20):\n\"Ignoring unknown parameters: `bins`\"\n\n\n\n\n\nThis visual shows that there is not a strong relationship between socio economic status and cannabis usage. This is not surprising, as cannabis usage is common both among the wealthy and the poor.\n\n\nCode\nfull_data %&gt;%\n    group_by(CannabisBinary, diagnosis) %&gt;%\n    summarize(n = n()) %&gt;%\n    ungroup() %&gt;%\n    group_by(CannabisBinary) %&gt;%\n    mutate(diagnosis_gvn_cannabis = n / sum(n)) %&gt;%\n    ungroup() %&gt;%\n    group_by(diagnosis) %&gt;%\n    mutate(cannabis_gvn_diagnosis = n / sum(n)) %&gt;%\n    ungroup()\n\n\n`summarise()` has grouped output by 'CannabisBinary'. You can override using\nthe `.groups` argument.\n\n\n\nA tibble: 4 x 5\n\n\nCannabisBinary\ndiagnosis\nn\ndiagnosis_gvn_cannabis\ncannabis_gvn_diagnosis\n\n\n&lt;fct&gt;\n&lt;fct&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n0\n0\n119\n0.5042373\n0.4280576\n\n\n0\n1\n117\n0.4957627\n0.4317343\n\n\n1\n0\n159\n0.5079872\n0.5719424\n\n\n1\n1\n154\n0.4920128\n0.5682657\n\n\n\n\n\nIn this table, we see that there are about even numbers of individuals with schizophrenia who use cannabis, individuals with schizophrenia who do not use cannabis, other cannabis users, and individuals who do not use cannabis in the data. All four groups have experienced psychosis, so we can use this data to understand the impact of cannabis on psychosis. Here we see that probabilistically, the chance of using cannabis given a schizophrenia diagnosis is about 0.57, while the chance of having a schizophrenia diagnosis given cannabis usage is about 0.49. Neither of these probabilities indicate a very strong relationship between cannabis usage and schizophrenia diagnosis.\n\n\nCode\nfull_data %&gt;% write_csv(\"../data/clean_data/full_data.csv\")"
  },
  {
    "objectID": "eda/eda.html#summary",
    "href": "eda/eda.html#summary",
    "title": "Data Exploration",
    "section": "",
    "text": "In my data exploration, I utilized the tidyverse, tidytext, ggplot2 to understand my data. These tools developed by Posit have very thorough documentation and similar APIs, making them quick to learn, user friendly, and customizable. ggplot2 follows the grammar of graphics, making plots easy to build and modify based on the requirements.\nFor the text data, I analyzed the word counts, word length, and word frequency across different categorizations. For the Reddit data, I used subreddit title as the label. For the Wikipedia pages, I categorized them based on whether they mentioned cannabis, psychosis, both, or neither. I was able to understand the themes of each different category as well as tone through the analysis.\nFor the record data, I used dplyr to join the two datasets and ggplot2 to visualize the data. It was clear that a relationship between age of psychosis onset and cannabis usage exists, but the relationship between cannabis usage and diagnosis was less clear. It would be interesting to explore a wider dataset with more individuals, including individuals who have not experienced psychosis, to see if the relationship between cannabis usage and diagnosis is more clear.\nMy hypothesis was that cannabis usage is related to the onset of psychosis. My analysis of the record data supports this hypothesis, as I found that cannabis users tend to have an earlier age of psychosis onset than non-cannabis users. This is strong evidence in support of my research, and it indicates that cannabis usage may be related to the onset of psychosis. I am still interested in the sentiment of the public around the impact of cannabis on psychosis and schizophrenia, so I will continue to explore the text data to understand this."
  },
  {
    "objectID": "work-in-progress.html",
    "href": "work-in-progress.html",
    "title": "Work in Progress",
    "section": "",
    "text": "Work in Progress\nThis website is still being built. Check back later for more!"
  },
  {
    "objectID": "data-cleaning/data-cleaning.html",
    "href": "data-cleaning/data-cleaning.html",
    "title": "Data Cleaning",
    "section": "",
    "text": "Now that we have gathered a reasonable amount of textual and record data, we can begin the data cleaning process. Our ultimate goal is to do statistical modeling wtih our data, so we need to clean the data. Generally, we will follow the principles of tidy data (Wickham 2014) in cleaning our data."
  },
  {
    "objectID": "data-cleaning/data-cleaning.html#reddit-data",
    "href": "data-cleaning/data-cleaning.html#reddit-data",
    "title": "Data Cleaning",
    "section": "Reddit Data",
    "text": "Reddit Data\nRecall that the Reddit data was returned as a JSON. We retrieved 10,000 text posts for each of three different text files. Our goal is to turn each of these JSON files into an individual dataframe. From there, we can transform the data into a Bag of Words, Document Term Matrix, or any other helpful format.\nWe will use pandas and json to parse this data into a desired output. First let’s read in the data.\n\n\nCode\nimport pandas as pd\nimport json\n\nwith open(\"../data/raw_data/reddit_psychosis_data.json\") as f:\n    reddit_psychosis = json.load(f)\nwith open('../data/raw_data/reddit_cannabis_data.json') as f:\n    reddit_cannabis = json.load(f)\nwith open(\"../data/raw_data/reddit_schizophrenia_data.json\") as f:\n    reddit_schizophrenia = json.load(f)\n\n\nFrom the data pull, we know that each of these JSON files has 100 elements, each with 100 posts.\nLet’s look at the structure of one element to identify how we can extract the title and text information.\n\n\nCode\nreddit_psychosis['0'].keys()\n\n\ndict_keys(['kind', 'data'])\n\n\nFrom here, we see that the posts are within the parameter children.\n\n\nCode\nlen(reddit_psychosis['0']['data']['children'])\n\n\n100\n\n\nIt looks like these are the 100 posts we’re looking for. Now we’ll extract the title and text from each of these children elements.\n\n\nCode\nreddit_psychosis['0']['data']['children'][1]['data'].keys()\n\n\ndict_keys(['approved_at_utc', 'subreddit', 'selftext', 'author_fullname', 'saved', 'mod_reason_title', 'gilded', 'clicked', 'title', 'link_flair_richtext', 'subreddit_name_prefixed', 'hidden', 'pwls', 'link_flair_css_class', 'downs', 'thumbnail_height', 'top_awarded_type', 'hide_score', 'name', 'quarantine', 'link_flair_text_color', 'upvote_ratio', 'author_flair_background_color', 'subreddit_type', 'ups', 'total_awards_received', 'media_embed', 'thumbnail_width', 'author_flair_template_id', 'is_original_content', 'user_reports', 'secure_media', 'is_reddit_media_domain', 'is_meta', 'category', 'secure_media_embed', 'link_flair_text', 'can_mod_post', 'score', 'approved_by', 'is_created_from_ads_ui', 'author_premium', 'thumbnail', 'edited', 'author_flair_css_class', 'author_flair_richtext', 'gildings', 'post_hint', 'content_categories', 'is_self', 'mod_note', 'created', 'link_flair_type', 'wls', 'removed_by_category', 'banned_by', 'author_flair_type', 'domain', 'allow_live_comments', 'selftext_html', 'likes', 'suggested_sort', 'banned_at_utc', 'url_overridden_by_dest', 'view_count', 'archived', 'no_follow', 'is_crosspostable', 'pinned', 'over_18', 'preview', 'all_awardings', 'awarders', 'media_only', 'can_gild', 'spoiler', 'locked', 'author_flair_text', 'treatment_tags', 'visited', 'removed_by', 'num_reports', 'distinguished', 'subreddit_id', 'author_is_blocked', 'mod_reason_by', 'removal_reason', 'link_flair_background_color', 'id', 'is_robot_indexable', 'report_reasons', 'author', 'discussion_type', 'num_comments', 'send_replies', 'whitelist_status', 'contest_mode', 'mod_reports', 'author_patreon_flair', 'author_flair_text_color', 'permalink', 'parent_whitelist_status', 'stickied', 'url', 'subreddit_subscribers', 'created_utc', 'num_crossposts', 'media', 'is_video'])\n\n\n\n\nCode\ntext = reddit_psychosis['0']['data']['children'][1]['data']['selftext']\ntitle = reddit_psychosis['0']['data']['children'][0]['data']['title']\n\n\nSince we have discovered the structure of this data, we can extract the text info for all of the posts. Let’s loop through all three files to get the data in a data frame.\nFirst, we can define function to loop through each of our JSON files.\n\n\nCode\ndef parse_reddit_json(reddit_json):\n    text_list = []\n    title_list = []\n    subreddit_list = []\n    for i in range(0, 100):\n        index = str(i)\n        for j in range(0, 100):\n            text_list.append(reddit_json[index]['data']['children'][j]['data']['selftext'])\n            title_list.append(reddit_json[index]['data']['children'][j]['data']['title'])\n            subreddit_list.append(reddit_json[index]['data']['children'][j]['data']['subreddit'])\n\n    return text_list, title_list, subreddit_list\n\n\nNow, the function parses each of the JSON files and outputs a list of the title of each post and the text contents of each post. All posts have titles, but not all posts have additional text.\n\n\nCode\npsy_text, psy_title, psy_sub = parse_reddit_json(reddit_psychosis)\nschiz_text, schiz_title, schiz_sub = parse_reddit_json(reddit_schizophrenia)\ncannabis_text, cannabis_title, cannabis_sub = parse_reddit_json(reddit_cannabis)\n\n\nNow that we have these lists, we can combine them into a pandas dataframe where each row is one post on Reddit.\n\n\nCode\ntext = psy_text + schiz_text + cannabis_text\ntitle = psy_title + schiz_title + cannabis_title\nsub = psy_sub + schiz_sub + cannabis_sub\n\nreddit_df = pd.DataFrame({'text': text, 'title': title, 'subreddit': sub})\nreddit_df.head()\n\n\n\n\n\n\n\n\n\ntext\ntitle\nsubreddit\n\n\n\n\n0\n3 years post-psychosis in recovery some days c...\nfirst time smiling on camera in... 3 years!\nPsychosis\n\n\n1\n\nI quit my meds lmfao\nPsychosis\n\n\n2\n\nI hate it here\nPsychosis\n\n\n3\n\nart by me. I thought it kinda visualized how I...\nPsychosis\n\n\n4\n\nBut I’m still god and this is neither a joke a...\nPsychosis\n\n\n\n\n\n\n\nNow we have a data frame of labeled text objects that will be easy to work with for modeling.\nTo take this data a step further, we can use a spacy pipeline to clean the text. This will do some standard cleaning to deal with things like emails, numbers, extra white space, and punctuation.\n\n\nCode\nreddit_df['all_text'] = reddit_df['title'] + \" \" + reddit_df['text']\ntext_list = list(reddit_df[['subreddit', 'all_text']].to_records(index = False))\n\n\n\n\nCode\n# Code adapted from DSAN 5800 Lab 2 by Dr. Larson\nimport spacy\n\npipeline = spacy.load('en_core_web_sm')\n\nimport re\nfrom spacy.language import Language\n\n# http://emailregex.com/\nemail_re = r\"\"\"(?:[a-z0-9!#$%&'*+/=?^_`{|}~-]+(?:\\.[a-z0-9!#$%&'*+/=?^_`{|}~-]+)*|\"(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21\\x23-\\x5b\\x5d-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])*\")@(?:(?:[a-z0-9](?:[a-z0-9-]*[a-z0-9])?\\.)+[a-z0-9](?:[a-z0-9-]*[a-z0-9])?|\\[(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?|[a-z0-9-]*[a-z0-9]:(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21-\\x5a\\x53-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])+)\\])\"\"\"\n\n# replace = [ (pattern-to-replace, replacement),  ...]\nreplace = [\n    (r\"&lt;a[^&gt;]*&gt;(.*?)&lt;/a&gt;\", r\"\\1\"),  # Matches most URLs\n    (email_re, \"email\"),            # Matches emails\n    (r\"(?&lt;=\\d),(?=\\d)\", \"\"),        # Remove commas in numbers\n    (r\"\\d+\", \"numbr\"),              # Map digits to special token &lt;numbr&gt;\n    (r\"[\\t\\n\\r\\*\\.\\@\\,\\-\\/]\", \" \"), # Punctuation and other junk\n    (r\"\\s+\", \" \")                   # Strips extra whitespace\n]\n\nfor i, (label, text) in enumerate(text_list):\n    new_text = text  # Make a copy of the original text\n    for repl in replace:\n        new_text = re.sub(repl[0], repl[1], new_text)\n    text_list[i] = (label, new_text)\n\n@Language.component(\"DSAN5000\")\ndef DSAN5000_preprocess(doc):\n    tokens = [token for token in doc \n              if not any((token.is_stop, token.is_punct))]\n    tokens = [token.lemma_.lower().strip() for token in tokens]\n    tokens = [token for token in tokens if token]\n    return pipeline.make_doc(\" \".join(tokens))\n\npipeline.add_pipe(\"DSAN5000\")\n\n\n&lt;function __main__.DSAN5000_preprocess(doc)&gt;\n\n\nWe can use this pipeline to clean up the text data.\n\n\nCode\ntext_list_clean = [(label, pipeline(text)) for label, text in text_list]\n\n\n\n\nCode\ntext_list_clean[0:5]\n\n\n[('Psychosis',\n  time smile camera numbr year numbr year post psychosis recovery day complete nightmare nightmare u hold long time knock grow strong find breakthrough moment lt;numbr trust friend dm support ❤ ️),\n ('Psychosis', quit med lmfao),\n ('Psychosis', hate),\n ('Psychosis', art think kinda visualize feel),\n ('Psychosis', god joke crisis delusion realization understanding true nature)]\n\n\n\n\nCode\nreddit_cleaned_df = pd.DataFrame(text_list_clean, columns=['label', 'text'])\nreddit_cleaned_df.to_csv('../data/clean_data/reddit_cleaned_text.csv', index=False)\n\n\nNow that our data is relatively clean, we can construct a bag of words using CountVectorizer.\n\n\nCode\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport numpy as np\n\nvectorizer = CountVectorizer()\nbow = vectorizer.fit_transform(text_list_clean)\n\n\nWe can see the most popular words in the corpus:\n\n\nCode\nvectorizer.vocabulary_"
  },
  {
    "objectID": "data-cleaning/data-cleaning.html#wikipedia-data",
    "href": "data-cleaning/data-cleaning.html#wikipedia-data",
    "title": "Data Cleaning",
    "section": "Wikipedia Data",
    "text": "Wikipedia Data\nNext, let’s clean up our Wikipedia data. The Wikipedia API returned a complex nested R object. We already extracted the HTML from this R object and stored it in a csv, but we really want the main text of each webpage.\nWe will use rvest to “harvest” the data from each HTML and store all of the information in a tibble.\n\n\nCode\nlibrary(rvest)\n\nload(\"../data/raw_data/wikipedia_scrape.Rdata\")\nwiki_data %&gt;% names()\n\n\n\n'title''text''link'\n\n\nThe HTML data is stored in the text column of the data frame. Let’s take the first element to parse out the text from each webpage.\n\n\nCode\nfirst &lt;- wiki_data$text[1]\nfirst %&gt;% \n    read_html() %&gt;%\n    html_element(\"body\") %&gt;% \n    html_element(\"div\") %&gt;% \n    html_elements(\"p\") %&gt;% \n    html_text() %&gt;% \n    head()\n\n\n\n'The long-term effects of cannabis have been the subject of ongoing debate. Because cannabis is illegal in most countries, clinical research presents a challenge and there is limited evidence from which to draw conclusions.[1] In 2017, the U.S. National Academies of Sciences, Engineering, and Medicine issued a report summarizing much of the published literature on health effects of cannabis, into categories regarded as conclusive, substantial, moderate, limited and of no or insufficient evidence to support an association with a particular outcome.[2]''Cannabis is the most widely used illicit drug in the Western world.[3] In the United States, 10-20% of those who begin the use of cannabis daily will later become dependent.[4][5] Cannabis use can lead to addiction, which is defined as \"when the person cannot stop using the drug even though it interferes with many aspects of his or her life.\"[5][6][7][8] Cannabis use disorder is defined in the fifth revision of the Diagnostic and Statistical Manual of Mental Disorders (DSM-5) as a condition requiring treatment.[3] A 2012 review of cannabis use and dependency in the United States by Danovitch et al said that \"42% of persons over age 12 have used cannabis at least once in their lifetime, 11.5% have used within the past year, and 1.8% have met diagnostic criteria for cannabis abuse or dependence within the past year. Among individuals who have ever used cannabis, conditional dependence (the proportion who go on to develop dependence) is 9%.\" Although no medication is known to be effective in combating dependency, combinations of psychotherapy such as cognitive behavioural therapy and motivational enhancement therapy have achieved some success.[9]''Cannabis dependence develops in 9% of users, significantly less than that of heroin, cocaine, alcohol, and prescribed anxiolytics,[10] but slightly higher than that for psilocybin, mescaline, or LSD. Dependence on cannabis tends to be less severe than that observed with cocaine, opiates, and alcohol.[11] A 2018 academic review, published in partnership with Canopy Growth, discussed the limitations of current studies of therapeutic and non-therapeutic cannabis use, and further stated that the nature of dependence formation among regular marijuana consumers has declined since 2002.[12]''Cambridge University published a study in 2015 that showed the surprising fact that in England and Wales, the use of cannabis had decreased. Although there was a reported decrease in use, the need for addiction treatment was surging. The study looked more in depth on how the potency of the cannabis affected someone\\'s dependence on the drug. They tested three different levels of potency and found that the most potent cannabis had the highest amount of dependence. Researchers believe that this is because of the high that the participants felt after using. The lower potency strains did not give users the same high, which made them not desire or in turn depend on that strain as much.[13]''Acute cannabis intoxication has been shown to negatively affect attention, psychomotor task ability, and short-term memory.[14][15] Studies of chronic cannabis users have demonstrated, although inconsistently, a long-lasting effect on the attention span, memory function, and cognitive abilities of moderate-dose, long-term users. Once cannabis use is discontinued for several months, these effects disappear, unless the user started consuming during adolescence. It is speculated that this is due to neurotoxic effects of cannabis interfering with critical brain development.[16][17]''Chronic use of cannabis during adolescence, a time when the brain is still developing, is correlated in the long term with lower IQ and cognitive deficits. It is not clear, though, if cannabis use causes the problems or if the causality is in the reverse. Recent studies have shown that IQ deficits existed in some subjects before chronic cannabis use, suggesting that lower IQ may instead be a risk factor for cannabis addiction.[18][6][19]'\n\n\nThroughout exploring the structure of the HTML file in Wikipedia, we see that there is a simple way to get all paragraph text from each page. We just need to pull all the text from the &lt;p&gt; tags on each page.\nNow, we will loop through the tibble to convert the HTML string into a plain text string representing the paragraphs on the Wikipedia page.\n\n\nCode\ntext_column &lt;- list()\nfor(i in 1:nrow(wiki_data)) {\n    html &lt;- wiki_data$text[i]\n    text_list &lt;- html %&gt;%\n        read_html() %&gt;%\n        html_element(\"body\") %&gt;%\n        html_elements(\"p\") %&gt;%\n        html_text()\n    text_list &lt;- paste0(text_list, collapse = \" \")\n    text_column &lt;- append(text_column, text_list)\n}\n\n\nNow we have a list of all the text from each of these HTML files. Let’s add this as a new column to our tibble and get rid of the huge HTML strings to decrease our memory footprint.\n\n\nCode\nlibrary(tidyverse)\n\nwiki_data &lt;- wiki_data %&gt;%\n    select(-text) %&gt;%\n    tibble::add_column(raw_text = text_column)\n\n\n\n\nCode\nwiki_data %&gt;%\n    unnest(raw_text) %&gt;%\n    distinct() %&gt;%\n    write_csv(\"../data/clean_data/wiki_data.csv\")\n\n\nSimilar to the Reddit data, let’s clean this text by running it through the pipeline we defined earlier.\n\n\nCode\nimport pandas as pd\n\nwiki_data = pd.read_csv('../data/clean_data/wiki_data.csv')\n\n\n\n\nCode\nwiki_data.head()\n\n\n\n\n\n\n\n\n\ntitle\nlink\nraw_text\nall_text\n\n\n\n\n0\nLong-term effects of cannabis\nmain\nThe long-term effects of cannabis have been th...\nLong-term effects of cannabis The long-term ef...\n\n\n1\n(C6)-CP 47,497\nlink\n(C6)-CP 47,497 (CP 47,497 dimethylhexyl homolo...\n(C6)-CP 47,497 (C6)-CP 47,497 (CP 47,497 dimet...\n\n\n2\n(C9)-CP 47,497\nlink\n(C9)-CP 47,497 (CP 47,497 dimethylnonyl homolo...\n(C9)-CP 47,497 (C9)-CP 47,497 (CP 47,497 dimet...\n\n\n3\n11-Hydroxy-Delta-8-THC\nlink\n11-Hydroxy-Delta-8-tetrahydrocannabinol (11-OH...\n11-Hydroxy-Delta-8-THC 11-Hydroxy-Delta-8-tetr...\n\n\n4\n11-Hydroxy-THC\nlink\n11-Hydroxy-Δ9-tetrahydrocannabinol (11-OH-Δ9-T...\n11-Hydroxy-THC 11-Hydroxy-Δ9-tetrahydrocannabi...\n\n\n\n\n\n\n\n\n\nCode\nwiki_data['all_text'] = wiki_data['title'] + \" \" + wiki_data['raw_text']\ntext_list = list(wiki_data[['link', 'all_text']].to_records(index = False))\n\n\n\n\nCode\nfor i, (label, text) in enumerate(text_list):\n    new_text = text\n    for repl in replace:\n        new_text = re.sub(repl[0], repl[1], new_text)\n    text_list[i] = (label, new_text)\n\n\n\n\nCode\ntext_list_clean = [(label, pipeline(text)) for label, text in text_list]\n\n\n\n\nCode\ntext_list_clean[0:5]\n\n\n[('main',\n  long term effect cannabis long term effect cannabi subject ongoing debate cannabis illegal country clinical research present challenge limited evidence draw conclusion numbr numbr u s national academies sciences engineering medicine issue report summarize publish literature health effect cannabi category regard conclusive substantial moderate limited insufficient evidence support association particular outcome numbr cannabis widely illicit drug western world numbr united states numbr numbr% begin use cannabis daily later dependent numbr][numbr cannabis use lead addiction define person stop drug interfere aspect life numbr][numbr][numbr][numbr cannabi use disorder define fifth revision diagnostic statistical manual mental disorders dsm numbr condition require treatment numbr numbr review cannabis use dependency united states danovitch et al say numbr% person age numbr cannabi lifetime numbr numbr% past year numbr numbr% meet diagnostic criterion cannabis abuse dependence past year individual cannabi conditional dependence proportion develop dependence numbr% medication know effective combat dependency combination psychotherapy cognitive behavioural therapy motivational enhancement therapy achieve success numbr cannabis dependence develop numbr% user significantly heroin cocaine alcohol prescribed anxiolytic numbr slightly high psilocybin mescaline lsd dependence cannabis tend severe observe cocaine opiate alcohol numbr numbr academic review publish partnership canopy growth discuss limitation current study therapeutic non therapeutic cannabis use state nature dependence formation regular marijuana consumer decline numbr numbr cambridge university publish study numbr show surprising fact england wales use cannabi decrease report decrease use need addiction treatment surge study look depth potency cannabi affect dependence drug test different level potency find potent cannabis high dependence researcher believe high participant feel low potency strain user high desire turn depend strain numbr acute cannabis intoxication show negatively affect attention psychomotor task ability short term memory numbr][numbr study chronic cannabis user demonstrate inconsistently long lasting effect attention span memory function cognitive ability moderate dose long term user cannabis use discontinue month effect disappear user start consume adolescence speculate neurotoxic effect cannabi interfere critical brain development numbr][numbr chronic use cannabis adolescence time brain develop correlate long term low iq cognitive deficit clear cannabi use cause problem causality reverse recent study show iq deficit exist subject chronic cannabis use suggest low iq instead risk factor cannabis addiction numbr][numbr][numbr prospective cohort study take place numbr numbr investigate association cannabi use neuropsychological decline subject test point life administer multiple different neuropsychological test author conclude cannabis intoxication find affect attention psychomotor task ability short term memory numbr][numbr find intoxicate user face difficulty have false memory numbr use cannabis heavily show affect working memory network function large amount cannabis time associate hyperactivity network working memory task finding show people use cannabis daily basis need additional effort order perform certain task numbr][numbr][numbr cannabis contain numbr different cannabinoid compound display psychoactive effect distinguished cannabinoid tetrahydrocannabinol thc cannabidiol cbd thc primary psychoactive agent numbr][numbr effect thc cbd salient psychosis anxiety numbr accord national academies sciences engineering medicine substantial evidence statistical association cannabi use development schizophrenia chronic psychosis high risk potentially frequent user numbr possible connection psychosis cannabis controversial observational study suggest correlation establish causative effect cannabis long term psychiatric health numbr medical evidence strongly suggest long term use cannabis people begin use early age display high tendency mental health problem physical development disorder causal link prove available datum numbr risk appear acute adolescent user numbr numbr review author conclude long term cannabis use increase risk psychosis people certain genetic environmental vulnerability cause psychosis important predisposing factor genetic liability childhood trauma urban upbringing numbr review year conclude cannabis use cause permanent psychological disorder user cognitive impairment anxiety paranoia increase risk psychosis key predisposing variable include age exposure frequency use potency cannabis individual susceptibility numbr researcher maintain exist strong association schizophrenia cannabis use cannabi use predict transition subsequent psychiatric illness factor involve include genetic environment time period initiation duration cannabis use underlie psychiatric pathology precede drug use combine use psychoactive drug numbr temporal relationship cannabis psychosis review numbr author propose b]ecause longitudinal work indicate cannabis use precede psychotic symptom reasonable assume causal relationship cannabis psychosis work need address possibility gene environment correlation numbr numbr meta analysis publish association study cover range dose habit show cannabis use associate significantly increase risk psychosis allege dose response relationship exist level cannabis use risk psychosis risk increase numbr fold daily use analysis adequate establish causal link numbr numbr meta analysis find cannabi use predict transition psychosis meet criterion abuse dependence drug numbr numbr review conclude exist evidence cannabi cause psychosis early heavy cannabis use factor likely find risk develop psychosis numbr opposing view express suzanne gage coauthor review literature available numbr regard epidemiologic evidence cannabis use psychosis strong warrant public health message cannabi use increase risk psychotic disorder caution additional study need determine size effect numbr public health message subsequently issue august numbr surgeon general united states numbr review gage et al state association cannabis schizophrenia causal magnitude estimate study date equate schizophrenia lifetime risk approximately numbr% regular cannabis user risk broad psychotic outcome great imply numbr% regular cannabis user develop schizophrenia risk great high genetic risk use particularly potent strain cannabis numbr numbr express term odd ratio study find daily cannabis use associate increase odd psychotic disorder compare user adjust odd ratio numbr numbr numbr% ci numbr numbr numbr numbr increase nearly time increase odd daily use high potency type cannabis numbr numbr numbr numbr numbr numbr numbr calculate increase odd ratio[numbr mean schizophrenia specifically numbr review place lifetime morbid risk narrowly define schizophrenia numbr numbr% numbr location translate substantial population attributable risk assume causality high potency cannabis type long available numbr% case episode psychosis prevent europe rise numbr% london numbr% amsterdam numbr numbr meta analysis find numbr% people cannabi induce psychosis transition schizophrenia find comparatively high hallucinogen numbr% amphetamine numbr% numbr numbr study note general population statistic increase psychosis incidence rate developed country numbr year despite fold increase cannabis use rate quote macleod et al numbr cannabis use appear increase substantially young people past numbr year numbr% report use numbr numbr numbr% report use numbr britain sweden relation use schizophrenia truly causal relative risk fold incidence schizophrenia double numbr population trend schizophrenia incidence suggest incidence stable slightly decrease relevant time period numbr note cannabis high thc cbd ratio produce high incidence psychological effect cbd antipsychotic neuroprotective property act antagonist effect thc studies examine effect high ratio cbd thc unclear extent laboratory study translate type cannabi real life user numbr][numbr research suggest cbd safely reduce symptom psychosis general numbr numbr review examine psychological therapy add people schizophrenia cannabis numbr clear evidence long term use cannabis increase risk psychosis regardless confound factor particularly people genetic risk factor numbr previous section family history psychosis administration pure thc clinical setting demonstrate elicit transient psychotic symptom numbr][numbr][numbr][numbr cannabis use precipitate new onset panic attack depersonalization derealization symptom simultaneously association cannabi use depersonalisation derealisation disorder study depersonalization define dissociative symptom feel like outside observer respect thought body sensation derealization mark feeling unreality detachment surrounding environment experience remote unfamiliar numbr individual experience depersonalisation derealisation symptom prior cannabis use report effect cannabi calm symptom depersonalisation derealisation disorder manageable regular use numbr attention give association cannabi use depression accord australian national drug alcohol research centre possible cannabi user depression likely access treatment psychosis numbr finding marijuana relationship depressive disorder scatter show cannabis use benefit detrimental overall mental health sufficient evidence exist show reduction cannabis use improve anxiety depression sleep quality numbr numbr review suggest cannabis show improve mood depression diagnose patient numbr indicative longitudinal relationship cannabis reduction improvement anxiety depression anxiety depression find increase susceptibility marijuana use numbr desire alleviate symptom experience marijuana use chronic user use anxiolytic purpose develop dependency cannabi make difficult cope anxiety drug absent teenage cannabis user difference general population incidence major depressive disorder mdd association exist early exposure couple continue use adult life increase incidence mdd adulthood numbr cannabis user age increase risk develop depression heavy user seemingly have high risk numbr heavy marijuana use adolescence associate deficit cognition recent study assess change neuropsychological functioning result long term cannabis use follow group adolescent age numbr numbr baseline numbr year period researchers find day use correlate decrease inhibitory control visuospatial ability contrary exist cross sectional study show marijuana use adolescence associate poor cognitive functioning association long term cannabis use memory processing speed numbr study show correlation memory cannabis use find important know study look association cannabis use poor neurocognitive function find extended abstinence marijuana lead improvement cognitive deficit decrease cognition result marijuana use reversible february numbr systematic review meta analysis find cannabis consumption adolescence associate increase risk develop depression suicidal behavior later life find effect anxiety numbr longitudinal study assess association long term use mental health group individual participate drug base treatment depression researcher find compare non user patient medically non medically experience improvement depressive symptom increase suicidal ideation additionally non medically likely visit psychiatrist numbr research investigate finding non medical marijuana use serve barrier treatment seek behavior mania mental illness mark period great excitement euphoria delusion overactivity numbr common cannabis user hit point high lead paranoia anxiety increase heart rate strain drug effect individual use effect guarantee case review report adult user marijuana induce mania previous psychiatric history numbr participant previously diagnose bipolar disorder worsen occurrence mania symptom numbr show diagnose psychiatrically stable develop mania symptom influence cannabis adolescent cannabis user difference peer suicidal ideation rate suicide attempt continue use cannabis adult life exhibit increase incidence multiple contributory factor implicate numbr general population weak indirect association appear exist suicidal behaviour cannabis consumption psychotic non psychotic user numbr remain unclear regular cannabis use increase risk suicide numbr cannabis use risk factor suicidality suicide attempt characterize additional risk factor include mood disorder alcohol use stress personal problem poor support numbr gateway drug hypothesis assert use soft drug cannabis tobacco alcohol ultimately lead use hard drug release dopamine cbnumbr receptor cannabinoid enter body enforce drug seek behavior addition gateway framework peer clustering theory say friendship influence drug seek behavior friend use influence drug rewarding high potential abuse numbr large scale longitudinal study uk new zealand numbr numbr show association cannabi use increase probability later disorder use drug numbr][numbr][numbr time marijuana gateway hypothesis study publish study use marijuana show reliable gateway cause illicit drug use numbr social factor environment influence drug use abuse make gateway effect cannabis different differ social circumstance study look association drug injection cannabis use street involve youth find cannabis use associate slow time injection initiation numbr injection initiation lead pattern injection initiation eventually lead addiction numbr numbr literature review say exposure cannabis associate disease liver particularly co exist hepatitis c lung heart vasculature author caution evidence need research consider prove causal association marijuana physical health condition numbr researcher concern increase legalization lead increase use turn new strategy rehabilitation minimize harm cannabis body numbr study conflict long term cannabis use cause persistent structural change human twin study show significant difference user non user twin pair numbr study demonstrate chronic use affect white matter hippocampal volume brain healthy non psychotic patient large amount cannabinoid numbr receptor present numbr][numbr long term cannabis user risk develop cannabinoid hyperemesis syndrome chs characterize recurrent bout intense vomiting mechanism chs poorly understand contrary antiemetic property cannabi cannabinoid numbr acute effect cannabi use human include dose dependent increase heart rate typically accompany mild increase blood pressure lie postural hypotension drop blood pressure stand effect vary depend relative concentration different cannabinoid affect cardiovascular function cannabigerol smoking cannabis decrease exercise tolerance numbr cardiovascular effect lead health issue majority young healthy user contrary heart attack myocardial infarction stroke adverse cardiovascular event occur association use cannabis use people cardiovascular disease pose health risk lead increase cardiac work increase catecholamine level impair blood oxygen carrying capacity production carboxyhemoglobin numbr numbr review examine relation cancer cannabi find little direct evidence cannabinoid find cannabi include thc carcinogenic cannabinoid mutagenic accord ames test cannabis smoke find carcinogenic rodent mutagenic ames test correlating cannabi use development human cancer problematic difficulty quantify cannabis use unmeasured confounder cannabinoid potential cancer treatment numbr accord numbr literature review cannabis carcinogenic methodological limitation study make difficult establish link cannabis use cancer risk numbr author bladder cancer link habitual cannabis use risk cancer head neck long term numbr year user numbr gordon colleague say appear increase risk cancer particularly head neck lung bladder cancer use marijuana period time length time risk increase uncertain numbr limited number study look effect smoke cannabi respiratory system numbr chronic heavy cannabis smoking associate cough production sputum wheezing symptom chronic bronchitis numbr regular cannabis use show cause significant abnormality lung function numbr regular cannabis smoker pathological change lung cell similar precede development lung cancer tobacco smoker numbr gordon colleague numbr literature review say unfortunately methodological limitation review study include selection bias small sample size limited generalizability lack adjustment tobacco smoking limit ability attribute cancer risk solely marijuana use numbr review study adjust age tobacco use say risk lung cancer adjust tobacco use period time risk increase uncertain numbr numbr review specifically examine effect cannabis lung conclude f]inding limited number design epidemiological study suggest increase risk development lung upper airway cancer light moderate use evidence mix concern possible carcinogenic risk heavy long term use numbr numbr international lung cancer consortium find significant additional lung cancer risk tobacco user smoke cannabi find increase risk cannabis smoker use tobacco conclude o]ur pool result show significant association intensity duration cumulative consumption cannabis smoke risk lung cancer overall smoker caution o]ur result preclude possibility cannabi exhibit association lung cancer risk extremely high dosage author support study call attention evolve mean cannabis consumption specifically respiratory risk differ use water pipe vaporizer consume oral preparation numbr cannabis smoke contain thousand organic inorganic chemical include carcinogen tobacco smoke numbr numbr special report british lung foundation conclude cannabis smoking link adverse effect include bronchitis lung cancer numbr identify cannabis smoke carcinogen say awareness danger low compare high awareness danger smoke tobacco particularly young user say increase risk cannabis cigarette draw large puff smoke hold numbr cannabis smoke list california proposition numbr warning list carcinogen numbr leave pure thc numbr numbr review find association head neck cancer lifetime cannabis smoking numbr numbr literature review gordon colleague conclude inhale cannabis associate lung disease numbr tashkin numbr review find clear link chronic obstructive pulmonary disease numbr smoking cannabis link adverse respiratory effect include chronic cough wheeze sputum production acute bronchitis numbr suggest common practice inhale cannabis smoke deeply hold breath lead pneumothorax case report involve immunocompromise patient pulmonary infection aspergillosis attribute smoke cannabi contaminate fungus transmission tuberculosis link cannabis inhalation technique share water pipe hotboxing numbr method cannabis consumption smoking consider harmful inhalation smoke organic material cause health problem e g cough sputum isoprene help modulate slow reaction rate contribute significantly differ quality partial combustion product source numbr][numbr male cannabis use associate reduced fertility decrease sperm count numbr initial epigenetic study show male cannabi use cause widespread dna methylation change sperm result low rate fertilization high rate miscarriage numbr sperm dna methylation alteration cannabis extract exposure evident offspring rat numbr important prenatal cannabis exposure associate neuropsychiatric disorder rate autism increase u s particularly state cannabi legal numbr study release national academies sciences engineering medicine cite significant evidence statistical link mother smoke cannabi pregnancy low birth weight baby numbr cannabis consumption pregnancy associate restriction growth fetus miscarriage cognitive deficit offspring numbr majority research concentrate adverse effect alcohol evidence prenatal exposure cannabis effect develop brain associate deficit language attention area cognitive performance delinquent behavior adolescence numbr report prepare australian national council drugs conclude cannabi cannabinoid contraindicate pregnancy interact endocannabinoid system numbr fatal overdose associate cannabis use report numbr small number study conduct evidence insufficient long term elevate risk mortality cause motor vehicle accident suicide possible respiratory brain cancer interest researcher study able consistent increase mortality cause numbr),\n ('link',\n  cnumbr cp numbr cnumbr cp numbr cp numbr dimethylhexyl homologue synthetic cannabinoid cp numbr homologue numbr systematic numbr numbrs numbrr numbr hydroxycyclohexyl numbr numbr dimethylhexyl)phenol numbr cannabinoid relate article stub help wikipedia expand),\n ('link',\n  cnumbr cp numbr cnumbr cp numbr cp numbr dimethylnonyl homologue synthetic cannabinoid cp numbr homologue numbr systematic numbr numbrs numbrr numbr hydroxycyclohexyl numbr numbr dimethylnonyl)phenol cannabinoid relate article stub help wikipedia expand),\n ('link',\n  numbr hydroxy delta numbr thc numbr hydroxy delta numbr tetrahydrocannabinol numbr oh δnumbr thc alternatively number numbr oh δnumbr thc active metabolite δnumbr thc psychoactive cannabinoid find small amount cannabis isomer numbr oh δnumbr thc produce metabolic pathway cannabinoid metabolite discover numbr numbr retain psychoactive effect animal study high potency δnumbr thc low potency numbr oh δnumbr thc widespread legal use semi synthetic δnumbr thc certain jurisdiction δnumbr thc remain illegal numbr oh δnumbr thc important metabolite distinguish use legal δnumbr thc illegal δnumbr thc numbr][numbr][numbr][numbr][numbr cannabinoid relate article stub help wikipedia expand),\n ('link',\n  numbr hydroxy thc numbr hydroxy δnumbr tetrahydrocannabinol numbr oh δnumbr thc alternatively number numbr oh δnumbr thc usually refer numbr hydroxy thc main active metabolite tetrahydrocannabinol thc form body δnumbr thc consume numbr][numbr cannabis consumption thc metabolize inside body cytochrome pnumbr enzyme cypnumbrcnumbr cypnumbranumbr numbr hydroxy thc metabolize dehydrogenase cypnumbrcnumbr enzyme form numbr numbr carboxy thc thc cooh inactive cbnumbr receptors;[numbr glucuronidate form numbr δnumbr tetrahydrocannabinol numbr carboxylic acid glucuronide δnumbr thc cooh glu)[numbr excrete fece urine numbr compound thc assay drug test numbr numbr hydroxy thc form consumption thc inhalation vape smoking oral mouth edible sublingual use level numbr hydroxy thc typically high eat compare inhalation numbr][numbr like δnumbr thc numbr hydroxy thc partial agonist cannabinoid receptor cbnumbr significantly high bind affinity ki = numbr numbr nm compare δnumbr thc ki = numbr nm numbr respect camp inhibition cbnumbr display similar efficacy δnumbr thc ecnumbr = numbr nm vs ecnumbr = numbr numbr nm respectively low maximum response emax = numbr% vs emax = numbr% numbr analysis university rhode island cannabinoid find numbr oh δnumbr thc numbrrd high numbrc like protease inhibitor activity covid numbr cannabinoid test study high antiviral drug gcnumbr numbr% numbr oh δnumbr thc vs numbr% gcnumbr numbr)]\n\n\n\n\nCode\nwiki_cleaned_df = pd.DataFrame(text_list_clean, columns=['link', 'text'])\nwiki_cleaned_df.to_csv('../data/clean_data/wiki_cleaned_text.csv', index=False)\n\n\nNow we have our text data from Wikipedia in a data frame with labels representing whether the page was a forward or backward link for the page titled “Long-term effects of cannabis.”"
  },
  {
    "objectID": "data-cleaning/data-cleaning.html#cannabinoids",
    "href": "data-cleaning/data-cleaning.html#cannabinoids",
    "title": "Data Cleaning",
    "section": "Cannabinoids",
    "text": "Cannabinoids\nWe can clean Gibson et al. (2020) data set on cannabinoids and their impact on individuals with psychosis. We will start again by visualizing missing data in our table.\n\n\nCode\nlibrary(readxl)\ncannabinoid &lt;- read_excel(\"../data/raw_data/DataforSumbission_FINAL.xlsx\", skip = 1)\n\ncannabinoid %&gt;% vis_dat()\n\n\nNew names:\n* `` -&gt; `...1`\n* `` -&gt; `...2`\n\n\n\n\n\nOverall, the completeness of this data is pretty good. We again remove the columns with a high rate of incompleteness. We can also drop rows with missing data because we see a pattern where the missing data tends to collect in rows.\n\n\nCode\ncannabinoid &lt;- cannabinoid %&gt;%\n    select(where(~ sum(is.na(.)) &lt; 20))\n    \ncannabinoid &lt;- cannabinoid %&gt;%\n    mutate(na_count = apply(., 1, function(x) sum(is.na(x)))) %&gt;%\n    filter(na_count &lt; 20) \n    \ncannabinoid %&gt;%\n    vis_dat()\n\n\n\n\n\nI notice that there seem to be a lot of character columns. Let’s check to make sure that these columns are standardized and do some cleaning up.\n\n\nCode\ncannabinoid &lt;- cannabinoid %&gt;% select(-`...1`)\n\n\n\n\nCode\nname_list &lt;- cannabinoid %&gt;%\n    select(where(is.character)) %&gt;%\n    names()\n\n\n\n\nCode\nfor(name in name_list) {\n    cannabinoid %&gt;%\n        select(all_of(name)) %&gt;% \n        distinct() %&gt;%\n        print(n = 10)\n}\n\n\n# A tibble: 3 x 1\n  Group_Toxicology    \n  &lt;chr&gt;               \n1 Cannabinoid-Positive\n2 Cannabinoid-Negative\n3 119                 \n# A tibble: 3 x 1\n  Toxicology_Synthetic_Cannabinoid\n  &lt;chr&gt;                           \n1 Synthetic Cannabinoid Negative  \n2 Synthetic Cannabinoid Positive  \n3 119                             \n# A tibble: 5 x 1\n  Batch \n  &lt;chr&gt; \n1 NA    \n2 DEC 17\n3 MAR 18\n4 JUL 18\n5 107   \n# A tibble: 6 x 1\n  RaceEthnicity\n  &lt;chr&gt;        \n1 Hispanic     \n2 Black        \n3 White        \n4 Asian        \n5 Unknown      \n6 119          \n# A tibble: 6 x 1\n  MaritalStatus               \n  &lt;chr&gt;                       \n1 \"Single\\u200e/Never married\"\n2  NA                         \n3 \"Widowed\\u200e/Divorced (?)\"\n4 \"Married\"                   \n5 \"0\"                         \n6 \"116\"                       \n# A tibble: 6 x 1\n  Education               \n  &lt;chr&gt;                   \n1 Non-high school graduate\n2 High school graduate    \n3 Some college            \n4 College graduate        \n5 NA                      \n6 114                     \n# A tibble: 5 x 1\n  WorkStatus \n  &lt;chr&gt;      \n1 Not working\n2 NA         \n3 Full time  \n4 Part time  \n5 111        \n# A tibble: 7 x 1\n  LivingSituation                                                      \n  &lt;chr&gt;                                                                \n1 \"Shelter\\u200e/homeless\"                                             \n2  NA                                                                  \n3 \"Independent Housing (own\\u200e/rent)\"                               \n4 \"Other\"                                                              \n5 \"Living with family\\u200e/friend\\u200e/domestic partner\\u200e/spouse\"\n6 \"Transitional Housing\\u200e (SRO, residential housing)\"              \n7 \"113\"                                                                \n# A tibble: 5 x 1\n  LivingSit_Combined                    \n  &lt;chr&gt;                                 \n1 \"Shelter\\u200e/homeless\"              \n2  NA                                   \n3 \"Independent Housing (own\\u200e/rent)\"\n4 \"Other\"                               \n5 \"113\"                                 \n# A tibble: 7 x 1\n  HouseholdSupport                    \n  &lt;chr&gt;                               \n1  NA                                 \n2 \"Self-support\"                      \n3 \"Government support (SSD\\u200e/SSI)\"\n4 \"Parental\\u200e/guardian support\"   \n5 \"Other\"                             \n6 \"Spouse\\u200e/partner support\"      \n7 \"108\"                               \n# A tibble: 5 x 1\n  Clinician_Admission_PANSS\n  &lt;chr&gt;                    \n1 AB                       \n2 HE                       \n3 DD                       \n4 CM                       \n5 119                      \n# A tibble: 5 x 1\n  Clinician_Discharge_PANSS\n  &lt;chr&gt;                    \n1 NA                       \n2 HE                       \n3 DD                       \n4 CM                       \n5 117                      \n# A tibble: 6 x 1\n  `Mini MDD Dx`   \n  &lt;chr&gt;           \n1 Past            \n2 None            \n3 Current and Past\n4 NA              \n5 Current         \n6 104             \n# A tibble: 6 x 1\n  `Mini Mania Dx` \n  &lt;chr&gt;           \n1 None            \n2 Current and Past\n3 NA              \n4 Past            \n5 Current         \n6 100             \n# A tibble: 3 x 1\n  `Pyschotic diagnosis at discharge`\n  &lt;chr&gt;                             \n1 Yes                               \n2 No                                \n3 119                               \n# A tibble: 3 x 1\n  `Mood diagnosis at discharge`\n  &lt;chr&gt;                        \n1 No                           \n2 Yes                          \n3 119                          \n# A tibble: 4 x 1\n  `History of mood disorder`\n  &lt;chr&gt;                     \n1 0                         \n2 Mania                     \n3 Depression                \n4 119                       \n# A tibble: 9 x 1\n  `Psychotic disoder dx at discharge` \n  &lt;chr&gt;                               \n1 Psychotic Disorder NOS              \n2 Schizoaffective Disorder            \n3 Bipolar Disorder                    \n4 Schizophrenia                       \n5 MDD with Psychotic Features         \n6 Substance Induced Psychotic Disorder\n7 None                                \n8 Other Psychotic Disorder            \n9 119                                 \n# A tibble: 3 x 1\n  `If patient required a PRN (extra) medication for agitation in CPEP`\n  &lt;chr&gt;                                                               \n1 No                                                                  \n2 Yes                                                                 \n3 119                                                                 \n# A tibble: 3 x 1\n  `If patient required a PRN (extra) medication for agitation in Unit`\n  &lt;chr&gt;                                                               \n1 No                                                                  \n2 Yes                                                                 \n3 119                                                                 \n# A tibble: 3 x 1\n  `If patient required a PRN (extra) medication for agitation in Unit or CPEP`\n  &lt;chr&gt;                                                                       \n1 No                                                                          \n2 Yes                                                                         \n3 119                                                                         \n# A tibble: 4 x 1\n  SMOKER\n  &lt;chr&gt; \n1 Yes   \n2 No    \n3 NA    \n4 117   \n# A tibble: 3 x 1\n  Sex   \n  &lt;chr&gt; \n1 Male  \n2 Female\n3 119   \n# A tibble: 4 x 1\n  `Smoker, no missing values (median)`\n  &lt;chr&gt;                               \n1 NA                                  \n2 Yes                                 \n3 No                                  \n4 107                                 \n# A tibble: 4 x 1\n  `If patient recieved antipsychotics before study blood draw`\n  &lt;chr&gt;                                                       \n1 NA                                                          \n2 Yes                                                         \n3 No                                                          \n4 113                                                         \n# A tibble: 4 x 1\n  `If patient recieved benzos before study blood draw`\n  &lt;chr&gt;                                               \n1 NA                                                  \n2 Yes                                                 \n3 No                                                  \n4 113                                                 \n# A tibble: 3 x 1\n  `Medical condition known to effect cytokine levels`\n  &lt;chr&gt;                                              \n1 No condition                                       \n2 Condition present                                  \n3 119                                                \n# A tibble: 4 x 1\n  Cocaine \n  &lt;chr&gt;   \n1 Negative\n2 Missing \n3 Positive\n4 119     \n# A tibble: 4 x 1\n  PCP     \n  &lt;chr&gt;   \n1 Positive\n2 Negative\n3 Missing \n4 119     \n# A tibble: 4 x 1\n  Opiates \n  &lt;chr&gt;   \n1 Negative\n2 Missing \n3 Positive\n4 119     \n# A tibble: 4 x 1\n  Benzos  \n  &lt;chr&gt;   \n1 Negative\n2 Missing \n3 Positive\n4 119     \n# A tibble: 4 x 1\n  Barbituates\n  &lt;chr&gt;      \n1 Negative   \n2 Missing    \n3 Positive   \n4 119        \n# A tibble: 4 x 1\n  Methadone\n  &lt;chr&gt;    \n1 Negative \n2 Missing  \n3 Positive \n4 119      \n# A tibble: 4 x 1\n  Amphetamines\n  &lt;chr&gt;       \n1 Negative    \n2 Missing     \n3 Positive    \n4 119         \n# A tibble: 4 x 1\n  Alcohol \n  &lt;chr&gt;   \n1 Negative\n2 Missing \n3 Positive\n4 119     \n# A tibble: 3 x 1\n  All_Other_Tox\n  &lt;chr&gt;        \n1 Positive     \n2 None         \n3 119          \n# A tibble: 56 x 1\n   `Medical Conditions`                                                         \n   &lt;chr&gt;                                                                        \n 1 None                                                                         \n 2 Hypothyroidism                                                               \n 3 Pt had leukocytosis axillary abscess drained while on unit and then a short ~\n 4 DM Type 2, HTN, CVA, HCV                                                     \n 5 Recently post-partum                                                         \n 6 AIDS                                                                         \n 7 Asthma, herpes, HTN                                                          \n 8 HTN                                                                          \n 9 None recorded but taking asthma medications                                  \n10 HIV                                                                          \n# i 46 more rows\n# A tibble: 3 x 1\n  `Patient Discharged on First Generation Antipsychotic`\n  &lt;chr&gt;                                                 \n1 No                                                    \n2 Yes                                                   \n3 119                                                   \n# A tibble: 3 x 1\n  `Patient Discharged on Second Generation Antipsychotic`\n  &lt;chr&gt;                                                  \n1 Yes                                                    \n2 No                                                     \n3 119                                                    \n# A tibble: 3 x 1\n  `Patient Discharged on Any Antipsychotic`\n  &lt;chr&gt;                                    \n1 Yes                                      \n2 No                                       \n3 119                                      \n# A tibble: 3 x 1\n  `Patient Discharged on Clozapine`\n  &lt;chr&gt;                            \n1 No                               \n2 Yes                              \n3 119                              \n# A tibble: 3 x 1\n  `Patient Discharged on Injectable Antipsychotic`\n  &lt;chr&gt;                                           \n1 Yes                                             \n2 No                                              \n3 119                                             \n# A tibble: 3 x 1\n  `Patient Discharged on Mood Stabilizer`\n  &lt;chr&gt;                                  \n1 No                                     \n2 Yes                                    \n3 119                                    \n# A tibble: 3 x 1\n  `Patient Discharged on Antidepressant`\n  &lt;chr&gt;                                 \n1 No                                    \n2 Yes                                   \n3 119                                   \n# A tibble: 3 x 1\n  `Patient Discharged on Anxioytic`\n  &lt;chr&gt;                            \n1 No                               \n2 Yes                              \n3 119                              \n# A tibble: 3 x 1\n  `Patient Discharged on Medication to Prevent Relapse`\n  &lt;chr&gt;                                                \n1 No                                                   \n2 Yes                                                  \n3 119                                                  \n# A tibble: 3 x 1\n  `Patient Discharged on Benzotropine or Equivalent`\n  &lt;chr&gt;                                             \n1 No                                                \n2 Yes                                               \n3 119                                               \n# A tibble: 3 x 1\n  `Patient Discharged on Propanolol for Akathisia`\n  &lt;chr&gt;                                           \n1 No                                              \n2 Yes                                             \n3 119                                             \n# A tibble: 3 x 1\n  `Patient Discharged on Benzotropine or Propanolol`\n  &lt;chr&gt;                                             \n1 No                                                \n2 Yes                                               \n3 119                                               \n\n\nGenerally, these character columns are very clean. However, I want to get rid of the encoding issue causing \\u200e to appear.\n\n\nCode\ncannabinoid %&gt;%\n    filter(if_any(where(is.character), ~str_detect(., \"\\u200e\"))) %&gt;%\n    slice_head(n = 1)\n\n\n\nA tibble: 1 x 104\n\n\nID\nGroup_Toxicology\nToxicology_Synthetic_Cannabinoid\nBatch\nRaceEthnicity\nMaritalStatus\nEducation\nNumberChildren\nWorkStatus\nLivingSituation\n...\nLogIL10_scaled\nLogIL12_scaled\nLogIL1b_scaled\nLogIL2_scaled\nLogIL21_scaled\nLogIL6_scaled\nLogIL8_scaled\nLogTNFa_scaled\nLogsIL2Ra_scaled\nLogCRP_scaled\n\n\n&lt;dbl&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n...\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1000\nCannabinoid-Positive\nSynthetic Cannabinoid Negative\nNA\nHispanic\nSingle&lt;U+200E&gt;/Never married\nNon-high school graduate\n0\nNot working\nShelter&lt;U+200E&gt;/homeless\n...\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\n\n\nCode\ncannabinoid &lt;- cannabinoid %&gt;%\n    mutate(across(where(is.character), ~str_replace_all(., \"\\u200e\", \"\")))\n\ncannabinoid %&gt;%\n    slice_head(n = 1)\n\n\n\nA tibble: 1 x 104\n\n\nID\nGroup_Toxicology\nToxicology_Synthetic_Cannabinoid\nBatch\nRaceEthnicity\nMaritalStatus\nEducation\nNumberChildren\nWorkStatus\nLivingSituation\n...\nLogIL10_scaled\nLogIL12_scaled\nLogIL1b_scaled\nLogIL2_scaled\nLogIL21_scaled\nLogIL6_scaled\nLogIL8_scaled\nLogTNFa_scaled\nLogsIL2Ra_scaled\nLogCRP_scaled\n\n\n&lt;dbl&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n...\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1000\nCannabinoid-Positive\nSynthetic Cannabinoid Negative\nNA\nHispanic\nSingle/Never married\nNon-high school graduate\n0\nNot working\nShelter/homeless\n...\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\nI also notice that there is one row where the values are numeric rather than character. It might be a total row or an error. Let’s get rid of that row.\n\n\nCode\n# Confirm that this is only one row\ncannabinoid %&gt;%\n    filter(Group_Toxicology == 119) %&gt;%\n    nrow() %&gt;%\n    print()\n\n# Remove the row\ncannabinoid &lt;- cannabinoid %&gt;%\n    filter(Group_Toxicology != 119)\n\n\n[1] 1\n\n\n\n\nCode\ncannabinoid %&gt;%\n    write_csv(\"../data/cannabinoid_clean.csv\")\n\n\nNow that our data is clean, we can move on to some exploratory data analysis.\n\n\nCode\nresearch_data &lt;- read_csv(\"../data/cannabis_research_data.csv\")\n\n\nRows: 40 Columns: 56\n-- Column specification --------------------------------------------------------\nDelimiter: \",\"\nchr  (4): AdıSoyadı, MeslekDiğerseBelirtiniz, Tanı, TanıDigerseBelirt\ndbl (51): Cinsiyeti, Yaş, MedeniDurumu, Çocuk, EgitimDurumu, EğitimSüresi, M...\nnum  (1): CIDI\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nCode\nlibrary(deeplr)\nname_list &lt;- research_data %&gt;%\n    names()\n\n\n\n\nCode\ntranslated_names &lt;- translate2(\n    text = name_list,\n    target_lang = \"EN\",\n    source_lang = \"TR\"\n    auth_key = \"e323baa6-a35d-44dd-7508-2b9259af05f5:fx\"\n)\n\n\n\n\nCode\nnames(research_data) &lt;- translated_names\n\n\n\n\nCode\nresearch_data %&gt;%\n    select(where(is.character))\n\n\n\nA tibble: 40 x 4\n\n\nNameSurname\nOccupationOtherSpecify\nDiagnosis\nDiagnoseIfDigerseDetermine\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n\n\n\n\nB.&lt;U+00C7&gt; \nNA \nNA \nNA \n\n\nC.B.T \nNA \nAlkol Kulan&lt;U+0131&gt;m Bozuklu&lt;U+011F&gt;u\nNA \n\n\nE.K \nNA \nAlkol Kulan&lt;U+0131&gt;m Bozuklu&lt;U+011F&gt;u\nNA \n\n\nB.&lt;U+00D6&gt; \nNA \nGe&lt;U+00E7&gt;irilmi&lt;U+015F&gt; Depresyon \nkumar \n\n\nE.S \nNA \nAlkol Kulan&lt;U+0131&gt;m Bozuklu&lt;U+011F&gt;u\nDEHB \n\n\nM.A.T \nNA \nGe&lt;U+00E7&gt;irilmi&lt;U+015F&gt; Depresyon \nNA \n\n\nR.K \nOto Y&lt;U+0131&gt;k \nGe&lt;U+00E7&gt;irilmi&lt;U+015F&gt; Depresyon \nNA \n\n\nB.P \nNA \nDEHB \nGe&lt;U+00E7&gt;irilmi&lt;U+015F&gt; Depresy\n\n\nB.E\nNA\nNA\nNA\n\n\nY \n&lt;U+0130&gt;&lt;U+015F&gt;letm \nPersistent Depressive Dis \nAlkol Kullan&lt;U+0131&gt;m Boz \n\n\nH \nNA \nDEHB Ge&lt;U+00E7&gt;irilmi&lt;U+015F&gt; Depresy\nNA \n\n\n&lt;U+00D6&gt; \nNA \nNA \nNA \n\n\nM\nNA\nDEHB\nNA\n\n\nM \nNA \nDEHB Alkol kullan&lt;U+0131&gt;m \nGe&lt;U+00E7&gt;irilmi&lt;U+015F&gt; Depresy\n\n\nN \nNA \nDEHB Ge&lt;U+00E7&gt; Depresyon \nKumar Oynama Boz \n\n\nM.T \nNA \nDEHB Kumar Oynama Boz \nGe&lt;U+00E7&gt;ieilmi&lt;U+015F&gt; Depres\n\n\nY.G \nGarson \nAlkol Kullan&lt;U+0131&gt;m Boz \nMajor Depresyon \n\n\nM.D\nEsnaf\nNA\nNA\n\n\nB.K\nNA\nNA\nNA\n\n\nK.A\nNA\nNA\nNA\n\n\nA.A \nNA \nAlkol Kullan&lt;U+0131&gt;m Boz \nNA \n\n\nM.S \nNA \nDEHB Ge&lt;U+00E7&gt;irilmi&lt;U+015F&gt; Depresy\nNA \n\n\nH.H.G\nEsnaf\nDEHB\nNA\n\n\nO.B.K\nNA\nNA\nNA\n\n\nM.K \nNA \nDEHB Ge&lt;U+00E7&gt;irilmi&lt;U+015F&gt; Depresy\nNA \n\n\nH.K \nNA \nGe&lt;U+00E7&gt;irilmi&lt;U+015F&gt; Depresyon \nAlkol Kullan&lt;U+0131&gt;m Boz \n\n\nP.&lt;U+015E&gt; \nBarista \nGe&lt;U+00E7&gt;irilmi&lt;U+015F&gt; Depresyon \nFobi \n\n\nY\nNA\nNA\nNA\n\n\nB.E \nNA \nPTSB Ge&lt;U+00E7&gt;irilmi&lt;U+015F&gt; Depres\nNA \n\n\nH.A\nNA\nNA\nNA\n\n\nB.G \nNA \nDEHB Ge&lt;U+00E7&gt;irilmi Depresyo \nNA \n\n\nE.&lt;U+00DC&gt; \nNA \nNA \nNA \n\n\nF.A \nNA \nYayg&lt;U+0131&gt;n Ank Motor Tik \nNA \n\n\nA.&lt;U+015E&gt; \nNA \nDEHB \nNA \n\n\nK.M \n&lt;U+0130&gt;&lt;U+015F&gt;&lt;U+00E7&gt;i\nNA \nNA \n\n\nC.C \n&lt;U+0130&gt;&lt;U+015F&gt;&lt;U+00E7&gt;i\nDEHB Alkol Kullan&lt;U+0131&gt;m Boz \nNA \n\n\nS.&lt;U+00C7&gt; \nNA \nGe&lt;U+00E7&gt;irilmi&lt;U+015F&gt; Depresyon \nNA \n\n\n&lt;U+0130&gt;.C.&lt;U+00C7&gt;\nNA \nDEHB Ge&lt;U+00E7&gt;irilmi&lt;U+015F&gt; Depresy\nNA \n\n\nH.Y \nNA \nGe&lt;U+00E7&gt;irilmi&lt;U+015F&gt; Depresyon \nNA \n\n\nE\nNA\nNA\nNA\n\n\n\n\n\n\n\nCode\ndiagnosis_translated &lt;- research_data$Diagnosis %&gt;%\n    unique() %&gt;%\n    stringi::stri_enc_toutf8() %&gt;%\n    na.omit(.) %&gt;%\n    translate2(\n        source_lang = \"TR\"\n        auth_key = \"e323baa6-a35d-44dd-7508-2b9259af05f5:fx\"\n    )\n\n\n\n\nCode\nnew_name_joiner &lt;- cbind(research_data$Diagnosis %&gt;%\n    unique() %&gt;%\n    na.omit(.), diagnosis_translated) %&gt;%\n    as_tibble()\n\n\n\n\nCode\nresearch_data &lt;- research_data %&gt;%\n    left_join(new_name_joiner, by = join_by(Diagnosis == V1))\n\n\n\n\nCode\ndiagnoseifdigersedetermine_translated &lt;- research_data$DiagnoseIfDigerseDetermine %&gt;%\n    unique() %&gt;%\n    stringi::stri_enc_toutf8() %&gt;%\n    na.omit(.) %&gt;%\n    translate2(\n        source_lang = \"TR\"\n        auth_key = \"e323baa6-a35d-44dd-7508-2b9259af05f5:fx\"\n    )\n\n\n\n\nCode\nnew_name_joiner2 &lt;- cbind(research_data$DiagnoseIfDigerseDetermine %&gt;%\n    unique() %&gt;%\n    na.omit(.), diagnoseifdigersedetermine_translated) %&gt;%\n    as_tibble()\n\nresearch_data &lt;- research_data %&gt;%\n    left_join(new_name_joiner2, by = join_by(DiagnoseIfDigerseDetermine == V1))\n\n\n\n\nCode\nresearch_data %&gt;%\n    select(where(is.character)) %&gt;%\n    select(-NameSurname, -OccupationOtherSpecify)\n\n\n\nA tibble: 40 x 4\n\n\nDiagnosis\nDiagnoseIfDigerseDetermine\ndiagnosis_translated\ndiagnoseifdigersedetermine_translated\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n\n\n\n\nNA\nNA\nNA\nNA\n\n\nAlkol Kulan&lt;U+0131&gt;m Bozuklu&lt;U+011F&gt;u\nNA \nAlcohol Use Disorder \nNA \n\n\nAlkol Kulan&lt;U+0131&gt;m Bozuklu&lt;U+011F&gt;u\nNA \nAlcohol Use Disorder \nNA \n\n\nGe&lt;U+00E7&gt;irilmi&lt;U+015F&gt; Depresyon \nkumar \nPrevious Depression \ngambling \n\n\nAlkol Kulan&lt;U+0131&gt;m Bozuklu&lt;U+011F&gt;u\nDEHB \nAlcohol Use Disorder \nADHD \n\n\nGe&lt;U+00E7&gt;irilmi&lt;U+015F&gt; Depresyon \nNA \nPrevious Depression \nNA \n\n\nGe&lt;U+00E7&gt;irilmi&lt;U+015F&gt; Depresyon \nNA \nPrevious Depression \nNA \n\n\nDEHB \nGe&lt;U+00E7&gt;irilmi&lt;U+015F&gt; Depresy\nADHD \nSpent Depresy \n\n\nNA\nNA\nNA\nNA\n\n\nPersistent Depressive Dis \nAlkol Kullan&lt;U+0131&gt;m Boz \nPersistent Depressive Dis\nAlcohol Use Boz\n\n\nDEHB Ge&lt;U+00E7&gt;irilmi&lt;U+015F&gt; Depresy\nNA \nADHD Past Depresy \nNA \n\n\nNA\nNA\nNA\nNA\n\n\nDEHB\nNA\nADHD\nNA\n\n\nDEHB Alkol kullan&lt;U+0131&gt;m \nGe&lt;U+00E7&gt;irilmi&lt;U+015F&gt; Depresy\nADHD Alcohol use \nSpent Depresy \n\n\nDEHB Ge&lt;U+00E7&gt; Depresyon \nKumar Oynama Boz \nADHD Late Depression \nGambling Boz \n\n\nDEHB Kumar Oynama Boz \nGe&lt;U+00E7&gt;ieilmi&lt;U+015F&gt; Depres\nADHD Gambling Boz \nLate Depres \n\n\nAlkol Kullan&lt;U+0131&gt;m Boz \nMajor Depresyon \nAlcohol Use Boz \nMajor Depression\n\n\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\n\n\nAlkol Kullan&lt;U+0131&gt;m Boz \nNA \nAlcohol Use Boz \nNA \n\n\nDEHB Ge&lt;U+00E7&gt;irilmi&lt;U+015F&gt; Depresy\nNA \nADHD Past Depresy \nNA \n\n\nDEHB\nNA\nADHD\nNA\n\n\nNA\nNA\nNA\nNA\n\n\nDEHB Ge&lt;U+00E7&gt;irilmi&lt;U+015F&gt; Depresy\nNA \nADHD Past Depresy \nNA \n\n\nGe&lt;U+00E7&gt;irilmi&lt;U+015F&gt; Depresyon \nAlkol Kullan&lt;U+0131&gt;m Boz \nPrevious Depression \nAlcohol Use Boz\n\n\nGe&lt;U+00E7&gt;irilmi&lt;U+015F&gt; Depresyon \nFobi \nPrevious Depression \nPhobia \n\n\nNA\nNA\nNA\nNA\n\n\nPTSB Ge&lt;U+00E7&gt;irilmi&lt;U+015F&gt; Depres\nNA \nPTSD Past Depression \nNA \n\n\nNA\nNA\nNA\nNA\n\n\nDEHB Ge&lt;U+00E7&gt;irilmi Depresyo \nNA \nADHD History of Depression\nNA \n\n\nNA\nNA\nNA\nNA\n\n\nYayg&lt;U+0131&gt;n Ank Motor Tik \nNA \nCommon Ank Motor Tic \nNA \n\n\nDEHB\nNA\nADHD\nNA\n\n\nNA\nNA\nNA\nNA\n\n\nDEHB Alkol Kullan&lt;U+0131&gt;m Boz \nNA \nADHD Alcohol Use Impaired\nNA \n\n\nGe&lt;U+00E7&gt;irilmi&lt;U+015F&gt; Depresyon \nNA \nPrevious Depression \nNA \n\n\nDEHB Ge&lt;U+00E7&gt;irilmi&lt;U+015F&gt; Depresy\nNA \nADHD Past Depresy \nNA \n\n\nGe&lt;U+00E7&gt;irilmi&lt;U+015F&gt; Depresyon \nNA \nPrevious Depression \nNA \n\n\nNA\nNA\nNA\nNA\n\n\n\n\n\nThus, we sucessfully translate all the relevant data into English in order to utilize it for analysis. However, we notice that there are some inconsistencies - let’s clean up these translated columns so we can use them as classes.\n\n\nCode\nresearch_data %&gt;%\n    select(diagnoseifdigersedetermine_translated, DiagnoseIfDigerseDetermine) %&gt;%\n    distinct()\n\n\n\nA tibble: 9 x 2\n\n\ndiagnoseifdigersedetermine_translated\nDiagnoseIfDigerseDetermine\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n\n\n\n\nNA\nNA\n\n\ngambling\nkumar\n\n\nADHD\nDEHB\n\n\nSpent Depresy \nGe&lt;U+00E7&gt;irilmi&lt;U+015F&gt; Depresy\n\n\nAlcohol Use Boz\nAlkol Kullan&lt;U+0131&gt;m Boz \n\n\nGambling Boz\nKumar Oynama Boz\n\n\nLate Depres \nGe&lt;U+00E7&gt;ieilmi&lt;U+015F&gt; Depres\n\n\nMajor Depression\nMajor Depresyon\n\n\nPhobia\nFobi\n\n\n\n\n\nA quick google search shows that “Boz” is likely short for “Bozukluk” which means “Disorder”. Similarly, “Depres” and “Depresy” are likely short versions of “Depression”. We also find that “Spent” most likely means “Previous”.\n\n\nCode\nresearch_data &lt;- research_data %&gt;%\n    mutate(diagnoseifdigersedetermine_translated = case_when(\n        diagnoseifdigersedetermine_translated == \"Spent Depresy\" ~ \"Previous Depression\",\n        diagnoseifdigersedetermine_translated == \"Alcohol Use Boz\" ~ \"Alcohol Use Disorder\",\n        diagnoseifdigersedetermine_translated == \"Gambling Boz\" ~ \"Gambling Disorder\",\n        diagnoseifdigersedetermine_translated == \"Late Depres\" ~ \"Previous Depression\",\n        TRUE ~ diagnoseifdigersedetermine_translated\n        )\n    )\n\n\n\n\nCode\nresearch_data %&gt;%\n    select(diagnosis_translated, Diagnosis) %&gt;%\n    distinct()\n\n\n\nA tibble: 14 x 2\n\n\ndiagnosis_translated\nDiagnosis\n\n\n&lt;chr&gt;\n&lt;chr&gt;\n\n\n\n\nNA\nNA\n\n\nAlcohol Use Disorder \nAlkol Kulan&lt;U+0131&gt;m Bozuklu&lt;U+011F&gt;u\n\n\nPrevious Depression \nGe&lt;U+00E7&gt;irilmi&lt;U+015F&gt; Depresyon \n\n\nADHD\nDEHB\n\n\nPersistent Depressive Dis\nPersistent Depressive Dis\n\n\nADHD Past Depresy \nDEHB Ge&lt;U+00E7&gt;irilmi&lt;U+015F&gt; Depresy\n\n\nADHD Alcohol use \nDEHB Alkol kullan&lt;U+0131&gt;m \n\n\nADHD Late Depression \nDEHB Ge&lt;U+00E7&gt; Depresyon \n\n\nADHD Gambling Boz\nDEHB Kumar Oynama Boz\n\n\nAlcohol Use Boz \nAlkol Kullan&lt;U+0131&gt;m Boz \n\n\nPTSD Past Depression \nPTSB Ge&lt;U+00E7&gt;irilmi&lt;U+015F&gt; Depres\n\n\nADHD History of Depression\nDEHB Ge&lt;U+00E7&gt;irilmi Depresyo \n\n\nCommon Ank Motor Tic \nYayg&lt;U+0131&gt;n Ank Motor Tik \n\n\nADHD Alcohol Use Impaired\nDEHB Alkol Kullan&lt;U+0131&gt;m Boz \n\n\n\n\n\n\n\nCode\nresearch_data &lt;- research_data %&gt;%\n    mutate(diagnosis_translated = case_when(\n        diagnosis_translated == \"Persistent Depressive Dis\" ~ \"Persistent Depressive Disorder\",\n        diagnosis_translated == \"ADHD Past Depresy\" ~ \"ADHD, Previous Depression\",\n        diagnosis_translated == \"ADHD Alcohol use\" ~ \"ADHD, Alcohol Use Disorder\",\n        diagnosis_translated == \"ADHD Late Depression\" ~ \"ADHD, Previous Depression\",\n        diagnosis_translated == \"ADHD Gambling Boz\" ~ \"ADHD, Gambling Disorder\",\n        diagnosis_translated == \"Alcohol Use Boz\" ~ \"Alcohol Use Disorder\",\n        diagnosis_translated == \"PTSD Past Depression\" ~ \"PTSD, Previous Depression\",\n        diagnosis_translated == \"ADHD History of Depression\" ~ \"ADHD, Previous Depression\",\n        diagnosis_translated == \"ADHD Alcohol Use Impaired\" ~ \"ADHD, Alcohol Use Disorder\",\n        TRUE ~ diagnosis_translated\n    ))\n\n\n\n\nCode\nwrite_csv(research_data, file = \"../data/cannabis_research_data_clean.csv\")\n\n\n\n\nCode\nbehavioral_sequelae &lt;- read_csv(\"../data/behavioral_sequelae.csv\")\n\n\nRows: 124 Columns: 8\n-- Column specification --------------------------------------------------------\nDelimiter: \",\"\nchr (8): X.Author..Year., Participants..age..participant.group..sample.size....\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nCode\nlibrary(xml2)\n\nbehavioral_sequelae %&gt;%\n    # select(-X.Author..Year.) %&gt;%\n    # filter(str_detect(Relevant.Findings, \"schiz|psych\")) %&gt;%\n    mutate(X.Author..Year. = str_extract(X.Author..Year., \"&lt;EndNote&gt;.*&lt;/EndNote&gt;\")) %&gt;%\n    filter(!is.na(X.Author..Year.)) %&gt;%\n    rowwise() %&gt;%\n    mutate(\n        auths = list(\n            xml2::read_xml(X.Author..Year.) %&gt;%\n            xml2::xml_find_first(\".//record\") %&gt;%\n            xml2::xml_find_all(\".//contributors//authors\") %&gt;%\n            xml2::xml_contents() %&gt;%\n            xml2::xml_text()),\n        title = xml2::read_xml(X.Author..Year.) %&gt;%\n            xml2::xml_find_first(\".//record\") %&gt;%\n            xml2::xml_find_all(\".//title\") %&gt;%\n            xml2::xml_contents() %&gt;%\n            xml2::xml_text()\n    ) %&gt;%\n    ungroup() %&gt;%\n    mutate(n = str_extract(Participants..age..participant.group..sample.size., \"N=( )?\\\\d*\")) %&gt;%\n    mutate(n = str_extract(n, \"\\\\d+\")) %&gt;%\n    mutate(n = as.double(n)) %&gt;%\n    select(auths, title, n, Relevant.Findings) %&gt;%\n    arrange(-n)\n\n\n\nA tibble: 118 x 4\n\n\nauths\ntitle\nn\nRelevant.Findings\n\n\n&lt;list&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n\n\n\n\nZammit, Stanley , Allebeck, Peter , Andreasson, Sven, Lundberg, Ingvar, Lewis, Glyn\nSelf reported cannabis use as a risk factor for schizophrenia in Swedish conscripts of 1969: historical cohort study\n50087\n-Cannabis use at baseline was a significant risk factor for development of schizophrenia in a dose-dependent manner\n\n\nManrique-Garcia, Edison, Zammit, Stanley , Dalman, Christina , Hemmingsson, Tomas , Allebeck, Peter\nCannabis use and depression: a longitudinal study of a national cohort of Swedish conscripts\n45087\n-Overall, cannabis use was not a significant risk factor for future depression or completed suicide-Cannabis use was a significant risk factor schizoaffective disorder\n\n\nBlanco, Carlos , Hasin, Deborah S , Wall, Melanie M , Fl&lt;U+00F3&gt;rez-Salamanca, Ludwing, Hoertel, Nicolas , Wang, Shuai , Kerridge, Bradley T , Olfson, Mark \nCannabis use and risk of psychiatric disorders: prospective evidence from a US national longitudinal study \n34663\n-Cannabis use at baseline did not predict any mood or anxiety disorder at follow-up \n\n\nPatte, Karen A , Qian, Wei , Leatherdale, Scott T\nMarijuana and alcohol use as predictors of academic achievement: a longitudinal analysis among youth in the COMPASS study\n26475\n-Frequent cannabis use at baseline was a significant risk factor for poor school engagement and academic performance at follow-up\n\n\nHarder, Valerie S, Morral, Andrew R , Arkes, Jeremy\nMarijuana use and depression among adults: Testing for causal associations\n8759\n-Overall cannabis use at baseline was not a significant risk factor for depression after adjusting for covariates-Heavy cannabis use at baseline was a significant risk factor for depression after adjusting for covariates\n\n\nDanielsson, Anna-Karin, Lundin, Andreas , Agardh, Emilie , Allebeck, Peter , Forsell, Yvonne\nCannabis use, depression and anxiety: A 3-year prospective population-based study\n8598\n- Cannabis use was not a risk factor for any anxiety disorder or major depressive disorder\n\n\nBoccio, Cashen M, Beaver, Kevin M\nExamining the influence of adolescent marijuana use on adult intelligence: Further evidence in the causation versus spuriousness debate\n6584\n-Cannabis use in adolescence (13-22) was a risk factor for a 1-2-point decline in IQ by the 5-yr follow-up (18-26)\n\n\nMustonen, Antti , Niemel&lt;U+00E4&gt;, Solja , Nordstr&lt;U+00F6&gt;m, Tanja , Murray, Graham K , M&lt;U+00E4&gt;ki, Pirjo , J&lt;U+00E4&gt;&lt;U+00E4&gt;skel&lt;U+00E4&gt;inen, Erika, Miettunen, Jouko \nAdolescent cannabis use, baseline prodromal symptoms and the risk of psychosis \n6534\n-Adolescent cannabis use was a significant risk factor for psychosis at follow-up even after adjusting for possible covariates \n\n\nMiettunen, Jouko , T&lt;U+00F6&gt;rm&lt;U+00E4&gt;nen, Sari, Murray, Graham K , Jones, Peter B , M&lt;U+00E4&gt;ki, Pirjo , Ebeling, Hanna , Moilanen, Irma , Taanila, Anja , Heinimaa, Markus , Joukamaa, Matti \nAssociation of cannabis use with prodromal symptoms of psychosis in adolescence \n6330\nCannabis use before the age of 15 was a significant risk factor for reporting prodromal psychosis symptoms at follow-up \n\n\nVan Os, J , Bak, Maarten, Hanssen, M , Bijl, RV , De Graaf, R , Verdoux, H\nCannabis use and psychosis: a longitudinal population-based study\n4848\n-Cannabis use at baseline was a significant risk factor for psychosis and diagnosis of a psychotic disorder at follow-ups\n\n\nMokrysz, Claire , Landy, Rebecca , Gage, Suzanna H , Munafo, Marcus R , Roiser, Jonathan P, Curran, H Valerie\nAre IQ and educational outcomes in teenagers related to their cannabis use? A prospective cohort study\n4621\n-Frequent cannabis use was not a significant risk factor for lowered IQ or educational performance in comparison to occasional cannabis use or abstinence\n\n\nScott, J Cobb , Wolf, Daniel H , Calkins, Monica E, Bach, Emily C , Weidner, Jennifer, Ruparel, Kosha , Moore, Tyler M , Jones, Jason D , Jackson, Chad T , Gur, Raquel E\nCognitive functioning of adolescent and young adult cannabis users in the Philadelphia Neurodevelopmental Cohort\n4568\n-Frequent cannabis users demonstrated impairments in executive control in comparison to occasional and non-users-Younger age of first cannabis use was associated with greater impairment\n\n\nVan Laar, Margriet , Van Dorsselaer, Saskia, Monshouwer, Karin , De Graaf, Ron\nDoes cannabis use predict the first incidence of mood and anxiety disorders in the adult population?\n3854\n-Cannabis use at baseline was a moderate risk factor for depression-Cannabis use at baseline was a strong risk factor for bipolar disorder\n\n\nHayatbakhsh, Mohammad R, Najman, Jake M , Jamrozik, Konrad , Mamun, Abdullah A , Alati, Rosa , Bor, William\nCannabis and anxiety and depression in young adults: a large prospective study\n3239\n-Cannabis use before 15 years was a significant risk factor for anxiety and depression\n\n\nMeier, Madeline H , Caspi, Avshalom , Danese, Andrea , Fisher, Helen L , Houts, Renate , Arseneault, Louise, Moffitt, Terrie E \nAssociations between adolescent cannabis use and neuropsychological decline: a longitudinal co&lt;U+2010&gt;twin control study \n2332\n-Cannabis use was not associated with IQ decline-Twins who used cannabis more frequently than their co-twin demonstrated impairments in one test of working memory \n\n\nGriffith-Lendering, MFH, Huijbregts, Stephen CJ , Mooijaart, A , Vollebergh, WAM , Swaab, H\nCannabis use and development of externalizing and internalizing behaviour problems in early adolescence: A TRAILS study\n2230\n-Cannabis use was not a risk factor for anxiety or depression by age 16\n\n\nGriffith-Lendering, MF , Wigman, JT , Prince van Leeuwen, Andrea, Huijbregts, SC , Huizink, Anja C , Ormel, Johan , Verhulst, Frank C , van Os, Jim , Swaab, Hanna , Vollebergh, Wilma AM \nCannabis use and vulnerability for psychosis in early adolescence&lt;U+2013&gt;a TRAILS study \n2120\n-Cannabis use at age 16 predicted psychosis vulnerability at age 19 -Psychosis vulnerability at age 13 predicted cannabis use at ages 16 and 19 \n\n\nDegenhardt, Louisa, Coffey, Carolyn , Romaniuk, Helena , Swift, Wendy , Carlin, John B , Hall, Wayne D , Patton, George C\nThe persistence of the association between adolescent cannabis use and common mental disorders into young adulthood\n1943\n-Adolescent cannabis use was not a risk factor for depression -Daily cannabis use in adolescence was a risk factor for general anxiety disorder\n\n\nKuepper, Rebecca , van Os, Jim , Lieb, Roselind , Wittchen, Hans-Ulrich , H&lt;U+00F6&gt;fler, Michael, Henquet, C&lt;U+00E9&gt;cile\nContinued cannabis use and risk of incidence and persistence of psychotic symptoms: 10 year follow-up cohort study \n1923\n-Cannabis use at baseline was a significant risk factor for developing psychotic symptoms at follow-up \n\n\nGage, SH , Hickman, M , Heron, J , Munaf&lt;U+00F2&gt;, MR, Lewis, G , Macleod, J , Zammit, Stanley \nAssociations of cannabis and cigarette use with psychotic experiences at age 18: findings from the Avon Longitudinal Study of Parents and Children\n1756\n-Cannabis use was associated with a greater amount of psychotic experiences at age 16, but the relationship was attenuated after controlling for cigarette smoking frequency \n\n\nGage, Suzanne H , Hickman, Matthew , Heron, Jon , Munaf&lt;U+00F2&gt;, Marcus R, Lewis, Glyn , Macleod, John , Zammit, Stanley \nAssociations of cannabis and cigarette use with depression and anxiety at age 18: findings from the Avon Longitudinal Study of Parents and Children\n1756\n- Cannabis use was a significant risk factor for depression after adjustment for confounders-Cannabis use was not a significant risk factor for anxiety after adjustment for confounders \n\n\nMeier, M.H., , Caspi, A. , Ambler, A. , Harrington, H., Houts, R. , Keefe, R. S. , McDonald, K. , Ward, A. , Poulton, R. , Moffitt, T. E.\nPersistent cannabis users show neuropsychological decline from childhood to midlife\n1748\n-Persistent cannabis use was associated with neuropsychological and IQ decline - Cannabis cessation did not fully restore neurocognitive functioning in adolescent-onset cannabis users\n\n\nDespina, Bolanis , Massimiliano, Orri , Natalie, Castellanos-Ryan , Johanne, Renaud , Tina, Montreuil , Michel, Boivin , Frank, Vitaro , Tremblay, Richard E , Gustavo, Turecki , C&lt;U+00F4&gt;t&lt;U+00E9&gt;, M Sylvana\nCannabis use, depression and suicidal ideation in adolescence: direction of associations in a population based cohort \n1606\n-Cannabis use at baseline was not a risk factor for depression or suicide ideation-Depression at age 15 was a significant risk factor for subsequent weekly cannabis use \n\n\nPatton, George C , Coffey, Carolyn , Carlin, John B , Degenhardt, Louisa, Lynskey, Michael , Hall, Wayne\nCannabis use and mental health in young people: cohort study\n1601\n- Daily cannabis use in girls was a significant risk factor for reporting depression and anxiety at follow-up-Weekly cannabis use was a significant risk factor for reporting depression and anxiety at follow-up\n\n\nFerdinand, Robert F , Sondeijker, Frouke , Van Der Ende, Jan , Selten, Jean&lt;U+2010&gt;Paul, Huizink, Anja , Verhulst, Frank C \nCannabis use predicts future psychotic symptoms, and vice versa \n1580\n- Cannabis use was a significant risk factor for development of psychotic symptoms at follow-up- Psychotic symptoms in individuals who never used cannabis was a significant risk factor for future cannabis use \n\n\nStiby, Alexander I , Hickman, Matthew , Munaf&lt;U+00F2&gt;, Marcus R, Heron, Jon , Yip, Vikki L , Macleod, John \nAdolescent cannabis and tobacco use and educational outcomes at age 16: birth cohort study \n1155\n-Both cannabis and tobacco use at age 15 were associated with subsequent adverse educational outcomes by final follow-up \n\n\nBoden, Joseph M , Dhakal, Bhubaneswor, Foulds, James A , Horwood, L John \nLife&lt;U+2010&gt;course trajectories of cannabis use: a latent class analysis of a New Zealand birth cohort \n1065\n-Heavy and persistent cannabis use in adulthood was a risk factor for major depression, anxiety disorder, suicidal ideation, psychotic symptoms, welfare dependence, and lower SES \n\n\nFergusson, David M , Horwood, L John , Ridder, Elizabeth M\nTests of causal linkages between cannabis use and psychotic symptoms\n1055\n-Daily cannabis use was a significant risk factor for development of psychotic symptoms at 21 and 25-\n\n\nBechtold, Jordan, Hipwell, Alison , Lewis, David A , Loeber, Rolf , Pardini, Dustin\nConcurrent and sustained cumulative effects of adolescent marijuana use on subclinical psychotic symptoms\n1009\n-For each year adolescent boys engaged in regularcannabis use, their expected odds of subsequent subclinicalpsychotic symptoms rose by 21% and their expected odds ofexperiencing subsequent subclinical paranoia or hallucinations rose by 133% and 92%\n\n\nEpstein, Marina , Hill, Karl G , Nevell, Alyssa M , Guttmannova, Katarina, Bailey, Jennifer A , Abbott, Robert D , Kosterman, Rick , Hawkins, J David\nTrajectories of marijuana use from adolescence into adulthood: environmental and individual correlates\n808\n-Cannabis use in adolescence was a significant risk factor for adverse mental health, educational, and economic outcomes\n\n\n...\n...\n...\n...\n\n\nPadula, Claudia B , Schweinsburg, Alecia D, Tapert, Susan F\nSpatial working memory performance and fMRI activation interaction in abstinent adolescent marijuana users\n34\n-No behavior differences between groups emerged\n\n\nWesley, Michael J, Hanlon, Colleen A, Porrino, Linda J\nPoor decision-making by chronic marijuana users is associated with decreased functional responsiveness to negative consequences\n32\n- Cannabis users demonstrated impairments in decision-making in comparison to non-users\n\n\nDesrosiers, Nathalie A, Ramaekers, Johannes G , Chauchard, Emeline , Gorelick, David A , Huestis, Marilyn A\nSmoked cannabis' psychomotor and neurocognitive effects in occasional and frequent smokers\n25\n-Occasional cannabis users demonstrated impairment in psychomotor function and divided attention -Occasional and regular users did not demonstrate impairments in working memory or risk-taking during intoxication\n\n\nHatchard, T, Fried, PA , Hogan, MJ , Cameron, I , Smith, AM\nMarijuana use impacts cognitive interference: an fMRI investigation in young adults performing the counting Stroop task\n24\n-No behavior differences between groups emerged\n\n\nHunault, Claudine C , B&lt;U+00F6&gt;cker, Koen BE, Stellato, RK , Kenemans, J Leon , de Vries, Irma , Meulenbelt, Jan \nAcute subjective effects after smoking joints containing up to 69 mg &lt;U+0394&gt;9-tetrahydrocannabinol in recreational users: a randomized, crossover clinical trial \n24\n-THC-users reported significantly increased feelings of anxiety up to 8hr post-smoking-THC users reported significant impairment in memory, concentration, and alertness \n\n\nRamaekers, Johannes Gerardus, Kauert, G , Theunissen, EL , Toennes, Stefan W , Moeller, MR\nNeurocognitive performance during acute THC intoxication in heavy and occasional cannabis users\n24\n-Intoxication led to significant impairments in executive functioning in both groups- Intoxication led to significant impairments in divided attention, motor control, and motor inhibition among occasional users- Intoxication led to significant impairments in motor inhibition among heavy users\n\n\nSmith, Andra M , Longo, Carmelinda A, Fried, Peter A , Hogan, Matthew J , Cameron, Ian\nEffects of marijuana on visuospatial working memory: an fMRI study in young adults\n24\n-No behavior differences between groups emerged\n\n\nTheunissen, Eef L , Kauert, Gerold F , Toennes, Stefan W , Moeller, Manfred R , Sambeth, Anke , Blanchard, Mathieu M , Ramaekers, Johannes G\nNeurophysiological functioning of occasional and heavy cannabis users during THC intoxication\n24\n-THC intoxication led to significant impairments in divided attention and inhibitory control\n\n\nBoggs, Douglas L , Cortes-Briones, Jose A , Surti, Toral , Luddy, Christina , Ranganathan, Mohini , Cahill, John D , Sewell, Andrew R , D&lt;U+2019&gt;Souza, Deepak C, Skosnik, Patrick D \nThe dose-dependent psychomotor effects of intravenous delta-9-tetrahydrocannabinol (&lt;U+0394&gt;9-THC) in humans \n23\nIntoxicated participants did not demonstrate impairments in sustained attentions-intoxicated participants demonstrated dose-dependent deficits in fine motor control and timing \n\n\nBolla, K. I., Brown, K. , Eldreth, D. , Tate, K. , Cadet, J. L.\nDose-related neurocognitive effects of marijuana use\n22\n-Heavy cannabis users demonstrated impairments in memory and executive function despite abstinence\n\n\nBolla, K. I., , Eldreth, D. A., , Matochik, J. A.,, Cadet, J. L.\nNeural substrates of faulty decision-making in abstinent marijuana users\n22\n- Cannabis users demonstrated impairment in decision-making in comparison to controls\n\n\nD&lt;U+2019&gt;Souza, Deepak Cyril, Abi-Saab, Walid Michel , Madonick, Steven , Forselius-Bielen, Kimberlee , Doersch, Anne , Braley, Gabriel , Gueorguieva, Ralitza , Cooper, Thomas B , Krystal, John Harrison \nDelta-9-tetrahydrocannabinol effects in schizophrenia: implications for cognition, psychosis, and addiction \n22\n- THC increased anxious and positive and negative symptoms -THC did not lead to impairments in verbal memory but did lead to impairments in working memory \n\n\nKanayama, Gen , Rogowska, Jadwiga , Pope, Harrison G , Gruber, Staci A , Yurgelun-Todd, Deborah A\nSpatial working memory in heavy cannabis users: a functional magnetic resonance imaging study\n22\n- No behavior differences between groups emerged\n\n\nMorrison, PD, Stone, JM \nSynthetic delta&lt;U+2010&gt;9&lt;U+2010&gt;tetrahydrocannabinol elicits schizophrenia&lt;U+2010&gt;like negative symptoms which are distinct from sedation \n22\n-Under intoxication, participants reported significantly more negative symptoms than in placebo conditions \n\n\nMorrison, PD, Zois, V , McKeown, DA , Lee, TD , Holt, DW , Powell, JF , Kapur, S , Murray, RM\nThe acute effects of synthetic intravenous [Delta] 9-tetrahydrocannabinol on psychosis, mood and cognitive functioning\n22\n-Intoxicated participants reported greater positive psychotic symptoms and anxious symptoms-Intoxicated participants demonstrated impairments in working memory, episodic memory, attention, and reasoning\n\n\nAtakan, Z , Bhattacharyya, S, Allen, P , Martin-Santos, R, Crippa, JA , Borgwardt, 2SJ , Fusar-Poli, P , Seal, M , Sallis, H , Stahl, D \nCannabis affects people differently: inter-subject variation in the psychotogenic effects of &lt;U+0394&gt;9-tetrahydrocannabinol: a functional magnetic resonance imaging study with healthy volunteers \n21\n-Intoxication led to significantly greater reports of positive and negative schizophrenic symptomology in comparison to placebo-Participants reporting greater psychotic symptoms during intoxication displayed significant impairments in inhibitory control \n\n\nJacobsen, L. K., Mencl, W. E. , Westerveld, M. , Pugh, K. R.\nImpact of cannabis use on brain function in adolescents\n21\n-Cannabis users demonstrated impairments in attention in comparison with tobacco users and non-users\n\n\nJager, Gerry , Kahn, Rene S , Van Den Brink, Wim, Van Ree, Jan M , Ramsey, Nick F\nLong-term effects of frequent cannabis use on working memory and attention: an fMRI study\n20\n-No behavior differences between groups emerged\n\n\nRamaekers, J. G., , Kauert, Gerhold , van Ruitenbeek, Peter, Theunissen, Eef L , Schneider, Erhard , Moeller, Manfred R\nHigh-potency marijuana impairs executive function and inhibitory motor control\n20\n-THC intoxication significantly impaired executive function and motor control in a dose-dependent fashion\n\n\nKaufmann, RM , Kraft, B , Frey, R , Winkler, D , Weiszenbichler, S, B&lt;U+00E4&gt;cker, C , Kasper, S , Kress, HG \nAcute psychotropic effects of oral cannabis extract with a defined content of &lt;U+0394&gt;9-tetrahydrocannabinol (THC) in healthy volunteers \n16\n-THC intoxication led to significantly greater reports of psychotic symptoms \n\n\nMartin-Santos, R, a Crippa, J , Batalla, A , Bhattacharyya, S, Atakan, Z , Borgwardt, S , Allen, P , Seal, M , Langohr, K , Farre, M\nAcute effects of a single, oral dose of d9-tetrahydrocannabinol (THC) and cannabidiol (CBD) administration in healthy volunteers\n16\n- THC intoxication led to significantly greater reports of anxiety, dysphoria, positive psychotic symptoms, and physical and mental sedation in comparison to placebo and CBD\n\n\nBhattacharyya, Sagnik , Fusar-Poli, Paolo , Borgwardt, Stefan , Martin-Santos, Rocio , Nosarti, Chiara , O&lt;U+2019&gt;Carroll, Colin, Allen, Paul , Seal, Marc L , Fletcher, Paul C , Crippa, Jos&lt;U+00E9&gt; A \nModulation of mediotemporal and ventrostriatal function in humans by &lt;U+0394&gt;9-tetrahydrocannabinol: a neural basis for the effects of Cannabis sativa on learning and psychosis \n15\n-THC intoxicated participants did not demonstrate impairments in verbal learning -THC intoxicated participants reported greater levels of anxiety and psychotic symptoms than placebo or cannabidiol \n\n\nBorgwardt, Stefan J , Allen, Paul , Bhattacharyya, Sagnik, Fusar-Poli, Paolo , Crippa, Jose A , Seal, Marc L , Fraccaro, Valter , Atakan, Zerrin , Martin-Santos, Rocio , O'Carroll, Colin \nNeural basis of &lt;U+0394&gt;-9-tetrahydrocannabinol and cannabidiol: effects during response inhibition \n15\n- No behavior differences between conditions emerged \n\n\nCarey, Susan E , Nestor, Liam , Jones, Jennifer, Garavan, Hugh , Hester, Robert\nImpaired learning from errors in cannabis users: Dorsal anterior cingulate cortex and hippocampus hypoactivity\n15\n-Cannabis users demonstrated significant impairments in both verbal learning and memory\n\n\nCurran, Valerie H , Brignell, Catherine, Fletcher, Sally , Middleton, Paul , Henry, John \nCognitive and subjective dose-response effects of acute oral &lt;U+0394&gt; 9-tetrahydrocannabinol (THC) in infrequent cannabis users \n15\n- THC was associated with impairments in explicit memory and learning impairments in a dose-dependent manner \n\n\nArkell, Thomas R , Lintzeris, Nicholas , Kevin, Richard C , Ramaekers, Johannes G, Vandrey, Ryan , Irwin, Christopher , Haber, Paul S , McGregor, Iain S\nCannabidiol (CBD) content in vaporized cannabis does not prevent tetrahydrocannabinol (THC)-induced impairment of driving and cognition\n14\n- THC and THC/CBD conditions both produced significant impairments in attention and processing speed in comparison to placebo\n\n\nEnglund, Amir , Atakan, Zerrin , Kralj, Aleksandra, Tunstall, Nigel , Murray, Robin , Morrison, Paul\nThe effect of five day dosing with THCV on THC-induced cognitive, psychological and physiological effects in healthy male human volunteers: a placebo-controlled, double-blind, crossover pilot trial\n10\n-Intoxicated participants demonstrated memory impairments-Intoxicated participants did not report psychotic, anxious, or depressive symptoms\n\n\nMokrysz, Claire , Freeman, Tom P , Korkki, Saana , Griffiths, Kirsty, Curran, H Valerie\nAre adolescents more vulnerable to the harmful effects of cannabis than adults? A placebo-controlled study in human males\nNA\n-Adults demonstrated greater impairment in spatial working memory and episodic memory than adolescents-Adolescents demonstrated greater impairments in impulsivity than adults\n\n\nThayer, Rachel E , YorkWilliams, Sophie L, Hutchison, Kent E , Bryan, Angela D\nPreliminary results from a pilot study examining brain structure in older adult cannabis users and nonusers\nNA\n-Current users demonstrated impairment in working memory in comparison to non=users -No behavior differences in impulsivity, decision-making, processing speed, and episodic memory emerged\n\n\nVerdejo-Garcia, Antonio, Benbrook, Amy , Funderburk, Frank , David, Paula , Cadet, Jean-Lud , Bolla, Karen I\nThe differential relationship between cocaine use and marijuana use on decision-making performance over repeat testing with the Iowa Gambling Task\nNA\n- Cannabis use was significantly related to impairments in decision-making in a dose-dependent manner-Overall, Cannabis users performed the same as controls in the decision-making task however\n\n\n\n\n\n\n\nCode\nauths &lt;- xml2::read_xml(x) %&gt;%\n    xml2::xml_find_first(\".//record\") %&gt;%\n    xml2::xml_find_all(\".//contributors//authors\") %&gt;%\n    xml2::xml_contents() %&gt;%\n    xml2::xml_text()\n\n\n\n\nCode\ntitle &lt;- xml2::read_xml(x) %&gt;%\n    xml2::xml_find_first(\".//record\") %&gt;%\n    xml2::xml_find_all(\".//title\") %&gt;%\n    xml2::xml_contents() %&gt;%\n    xml2::xml_text()\n\n\n\n\nCode\nllcp &lt;- read_delim(\"../data/raw_data/LLCP2022.ASC\")\n\n\nNew names:\n* `01` -&gt; `01...1`\n* `` -&gt; `...2`\n* `` -&gt; `...3`\n* `` -&gt; `...4`\n* `` -&gt; `...5`\n* `` -&gt; `...6`\n* `` -&gt; `...7`\n* `` -&gt; `...8`\n* `` -&gt; `...9`\n* `` -&gt; `...10`\n* `` -&gt; `...11`\n* `` -&gt; `...12`\n* `` -&gt; `...13`\n* `` -&gt; `...14`\n* `` -&gt; `...16`\n* `` -&gt; `...17`\n* `` -&gt; `...18`\n* `` -&gt; `...19`\n* `` -&gt; `...21`\n* `` -&gt; `...22`\n* `` -&gt; `...23`\n* `` -&gt; `...24`\n* `` -&gt; `...25`\n* `` -&gt; `...26`\n* `` -&gt; `...27`\n* `` -&gt; `...28`\n* `` -&gt; `...29`\n* `` -&gt; `...30`\n* `` -&gt; `...31`\n* `` -&gt; `...32`\n* `` -&gt; `...33`\n* `` -&gt; `...34`\n* `` -&gt; `...35`\n* `` -&gt; `...36`\n* `11` -&gt; `11...37`\n* `` -&gt; `...41`\n* `` -&gt; `...42`\n* `` -&gt; `...43`\n* `` -&gt; `...44`\n* `` -&gt; `...45`\n* `` -&gt; `...46`\n* `` -&gt; `...47`\n* `` -&gt; `...48`\n* `` -&gt; `...49`\n* `` -&gt; `...50`\n* `` -&gt; `...51`\n* `` -&gt; `...52`\n* `2` -&gt; `2...53`\n* `` -&gt; `...54`\n* `` -&gt; `...55`\n* `` -&gt; `...56`\n* `` -&gt; `...57`\n* `` -&gt; `...58`\n* `` -&gt; `...59`\n* `` -&gt; `...60`\n* `` -&gt; `...61`\n* `` -&gt; `...63`\n* `` -&gt; `...65`\n* `` -&gt; `...68`\n* `` -&gt; `...69`\n* `` -&gt; `...70`\n* `` -&gt; `...71`\n* `` -&gt; `...72`\n* `` -&gt; `...73`\n* `` -&gt; `...74`\n* `` -&gt; `...75`\n* `` -&gt; `...76`\n* `` -&gt; `...77`\n* `` -&gt; `...78`\n* `` -&gt; `...79`\n* `` -&gt; `...80`\n* `` -&gt; `...81`\n* `` -&gt; `...82`\n* `` -&gt; `...83`\n* `` -&gt; `...84`\n* `` -&gt; `...85`\n* `` -&gt; `...86`\n* `` -&gt; `...87`\n* `` -&gt; `...88`\n* `` -&gt; `...89`\n* `` -&gt; `...90`\n* `` -&gt; `...91`\n* `` -&gt; `...92`\n* `` -&gt; `...93`\n* `` -&gt; `...94`\n* `` -&gt; `...95`\n* `` -&gt; `...96`\n* `` -&gt; `...97`\n* `` -&gt; `...98`\n* `` -&gt; `...99`\n* `` -&gt; `...100`\n* `` -&gt; `...101`\n* `` -&gt; `...102`\n* `` -&gt; `...104`\n* `` -&gt; `...105`\n* `` -&gt; `...106`\n* `` -&gt; `...107`\n* `` -&gt; `...108`\n* `` -&gt; `...109`\n* `` -&gt; `...110`\n* `` -&gt; `...113`\n* `` -&gt; `...114`\n* `2` -&gt; `2...116`\n* `` -&gt; `...117`\n* `` -&gt; `...118`\n* `` -&gt; `...119`\n* `` -&gt; `...120`\n* `` -&gt; `...121`\n* `` -&gt; `...122`\n* `2` -&gt; `2...123`\n* `` -&gt; `...125`\n* `` -&gt; `...126`\n* `` -&gt; `...127`\n* `` -&gt; `...128`\n* `` -&gt; `...129`\n* `` -&gt; `...130`\n* `` -&gt; `...131`\n* `` -&gt; `...132`\n* `2` -&gt; `2...133`\n* `` -&gt; `...134`\n* `` -&gt; `...136`\n* `` -&gt; `...137`\n* `` -&gt; `...138`\n* `` -&gt; `...139`\n* `` -&gt; `...140`\n* `` -&gt; `...142`\n* `` -&gt; `...143`\n* `` -&gt; `...144`\n* `` -&gt; `...145`\n* `` -&gt; `...146`\n* `` -&gt; `...148`\n* `` -&gt; `...149`\n* `` -&gt; `...150`\n* `` -&gt; `...151`\n* `` -&gt; `...152`\n* `` -&gt; `...153`\n* `` -&gt; `...154`\n* `` -&gt; `...155`\n* `` -&gt; `...156`\n* `` -&gt; `...157`\n* `` -&gt; `...158`\n* `` -&gt; `...159`\n* `` -&gt; `...160`\n* `` -&gt; `...161`\n* `` -&gt; `...162`\n* `` -&gt; `...163`\n* `` -&gt; `...164`\n* `` -&gt; `...165`\n* `` -&gt; `...166`\n* `` -&gt; `...167`\n* `` -&gt; `...168`\n* `` -&gt; `...169`\n* `` -&gt; `...170`\n* `` -&gt; `...171`\n* `` -&gt; `...172`\n* `` -&gt; `...173`\n* `` -&gt; `...174`\n* `` -&gt; `...175`\n* `` -&gt; `...176`\n* `` -&gt; `...177`\n* `` -&gt; `...178`\n* `` -&gt; `...179`\n* `` -&gt; `...180`\n* `` -&gt; `...181`\n* `` -&gt; `...182`\n* `` -&gt; `...183`\n* `` -&gt; `...184`\n* `` -&gt; `...185`\n* `` -&gt; `...186`\n* `` -&gt; `...187`\n* `` -&gt; `...188`\n* `` -&gt; `...189`\n* `` -&gt; `...190`\n* `` -&gt; `...191`\n* `` -&gt; `...192`\n* `` -&gt; `...193`\n* `` -&gt; `...194`\n* `` -&gt; `...195`\n* `` -&gt; `...196`\n* `` -&gt; `...197`\n* `` -&gt; `...198`\n* `` -&gt; `...199`\n* `` -&gt; `...200`\n* `` -&gt; `...201`\n* `` -&gt; `...202`\n* `` -&gt; `...203`\n* `` -&gt; `...204`\n* `` -&gt; `...205`\n* `` -&gt; `...206`\n* `` -&gt; `...207`\n* `` -&gt; `...208`\n* `` -&gt; `...209`\n* `` -&gt; `...210`\n* `` -&gt; `...211`\n* `` -&gt; `...212`\n* `` -&gt; `...213`\n* `` -&gt; `...214`\n* `` -&gt; `...215`\n* `` -&gt; `...216`\n* `` -&gt; `...217`\n* `` -&gt; `...218`\n* `` -&gt; `...219`\n* `` -&gt; `...220`\n* `` -&gt; `...221`\n* `` -&gt; `...222`\n* `` -&gt; `...223`\n* `` -&gt; `...224`\n* `` -&gt; `...225`\n* `` -&gt; `...226`\n* `` -&gt; `...227`\n* `` -&gt; `...228`\n* `` -&gt; `...229`\n* `` -&gt; `...230`\n* `` -&gt; `...231`\n* `` -&gt; `...232`\n* `` -&gt; `...233`\n* `` -&gt; `...234`\n* `` -&gt; `...235`\n* `` -&gt; `...236`\n* `` -&gt; `...237`\n* `` -&gt; `...238`\n* `` -&gt; `...239`\n* `` -&gt; `...240`\n* `` -&gt; `...241`\n* `` -&gt; `...243`\n* `` -&gt; `...244`\n* `` -&gt; `...245`\n* `` -&gt; `...246`\n* `` -&gt; `...247`\n* `` -&gt; `...248`\n* `` -&gt; `...249`\n* `` -&gt; `...250`\n* `` -&gt; `...251`\n* `` -&gt; `...252`\n* `` -&gt; `...253`\n* `` -&gt; `...254`\n* `` -&gt; `...255`\n* `` -&gt; `...256`\n* `` -&gt; `...257`\n* `` -&gt; `...258`\n* `` -&gt; `...259`\n* `` -&gt; `...260`\n* `` -&gt; `...261`\n* `` -&gt; `...262`\n* `` -&gt; `...263`\n* `` -&gt; `...264`\n* `` -&gt; `...265`\n* `` -&gt; `...266`\n* `` -&gt; `...267`\n* `` -&gt; `...268`\n* `` -&gt; `...269`\n* `` -&gt; `...270`\n* `` -&gt; `...271`\n* `` -&gt; `...272`\n* `` -&gt; `...273`\n* `` -&gt; `...274`\n* `` -&gt; `...275`\n* `` -&gt; `...276`\n* `` -&gt; `...277`\n* `` -&gt; `...278`\n* `` -&gt; `...279`\n* `` -&gt; `...280`\n* `` -&gt; `...281`\n* `` -&gt; `...282`\n* `` -&gt; `...283`\n* `` -&gt; `...284`\n* `` -&gt; `...285`\n* `` -&gt; `...286`\n* `` -&gt; `...287`\n* `` -&gt; `...288`\n* `` -&gt; `...289`\n* `` -&gt; `...290`\n* `` -&gt; `...291`\n* `` -&gt; `...292`\n* `` -&gt; `...293`\n* `` -&gt; `...294`\n* `` -&gt; `...295`\n* `` -&gt; `...296`\n* `` -&gt; `...297`\n* `` -&gt; `...298`\n* `` -&gt; `...299`\n* `` -&gt; `...300`\n* `` -&gt; `...301`\n* `` -&gt; `...302`\n* `` -&gt; `...303`\n* `` -&gt; `...304`\n* `` -&gt; `...305`\n* `` -&gt; `...306`\n* `` -&gt; `...307`\n* `` -&gt; `...308`\n* `` -&gt; `...309`\n* `` -&gt; `...310`\n* `` -&gt; `...311`\n* `` -&gt; `...312`\n* `` -&gt; `...313`\n* `` -&gt; `...314`\n* `` -&gt; `...315`\n* `` -&gt; `...316`\n* `` -&gt; `...317`\n* `` -&gt; `...318`\n* `` -&gt; `...319`\n* `` -&gt; `...320`\n* `` -&gt; `...321`\n* `` -&gt; `...322`\n* `` -&gt; `...323`\n* `` -&gt; `...324`\n* `` -&gt; `...325`\n* `` -&gt; `...326`\n* `` -&gt; `...327`\n* `` -&gt; `...328`\n* `` -&gt; `...329`\n* `` -&gt; `...330`\n* `` -&gt; `...331`\n* `` -&gt; `...332`\n* `` -&gt; `...333`\n* `` -&gt; `...334`\n* `` -&gt; `...335`\n* `` -&gt; `...336`\n* `` -&gt; `...337`\n* `` -&gt; `...338`\n* `` -&gt; `...339`\n* `` -&gt; `...340`\n* `` -&gt; `...341`\n* `` -&gt; `...342`\n* `` -&gt; `...343`\n* `` -&gt; `...344`\n* `` -&gt; `...345`\n* `` -&gt; `...346`\n* `` -&gt; `...347`\n* `` -&gt; `...348`\n* `` -&gt; `...349`\n* `` -&gt; `...350`\n* `` -&gt; `...351`\n* `` -&gt; `...352`\n* `` -&gt; `...353`\n* `` -&gt; `...354`\n* `` -&gt; `...355`\n* `` -&gt; `...356`\n* `` -&gt; `...357`\n* `` -&gt; `...358`\n* `` -&gt; `...359`\n* `` -&gt; `...360`\n* `` -&gt; `...361`\n* `` -&gt; `...362`\n* `` -&gt; `...363`\n* `` -&gt; `...364`\n* `` -&gt; `...365`\n* `` -&gt; `...366`\n* `` -&gt; `...367`\n* `` -&gt; `...368`\n* `` -&gt; `...369`\n* `` -&gt; `...370`\n* `` -&gt; `...371`\n* `` -&gt; `...372`\n* `` -&gt; `...373`\n* `` -&gt; `...374`\n* `` -&gt; `...375`\n* `` -&gt; `...376`\n* `` -&gt; `...377`\n* `` -&gt; `...378`\n* `` -&gt; `...379`\n* `` -&gt; `...380`\n* `` -&gt; `...381`\n* `` -&gt; `...382`\n* `` -&gt; `...383`\n* `` -&gt; `...384`\n* `` -&gt; `...385`\n* `` -&gt; `...386`\n* `` -&gt; `...387`\n* `` -&gt; `...388`\n* `` -&gt; `...389`\n* `` -&gt; `...390`\n* `` -&gt; `...391`\n* `` -&gt; `...392`\n* `` -&gt; `...393`\n* `` -&gt; `...394`\n* `` -&gt; `...395`\n* `` -&gt; `...396`\n* `` -&gt; `...397`\n* `` -&gt; `...398`\n* `` -&gt; `...399`\n* `` -&gt; `...400`\n* `` -&gt; `...401`\n* `` -&gt; `...402`\n* `` -&gt; `...403`\n* `` -&gt; `...404`\n* `` -&gt; `...405`\n* `` -&gt; `...406`\n* `` -&gt; `...407`\n* `` -&gt; `...408`\n* `` -&gt; `...409`\n* `` -&gt; `...410`\n* `` -&gt; `...411`\n* `` -&gt; `...412`\n* `` -&gt; `...413`\n* `` -&gt; `...414`\n* `` -&gt; `...415`\n* `` -&gt; `...416`\n* `` -&gt; `...417`\n* `` -&gt; `...418`\n* `` -&gt; `...419`\n* `` -&gt; `...420`\n* `` -&gt; `...421`\n* `` -&gt; `...422`\n* `` -&gt; `...423`\n* `` -&gt; `...424`\n* `` -&gt; `...425`\n* `` -&gt; `...426`\n* `` -&gt; `...427`\n* `` -&gt; `...428`\n* `` -&gt; `...429`\n* `` -&gt; `...430`\n* `` -&gt; `...431`\n* `` -&gt; `...432`\n* `` -&gt; `...433`\n* `` -&gt; `...434`\n* `` -&gt; `...435`\n* `` -&gt; `...436`\n* `` -&gt; `...437`\n* `` -&gt; `...438`\n* `` -&gt; `...439`\n* `` -&gt; `...440`\n* `` -&gt; `...441`\n* `` -&gt; `...442`\n* `` -&gt; `...443`\n* `` -&gt; `...444`\n* `` -&gt; `...445`\n* `` -&gt; `...446`\n* `` -&gt; `...447`\n* `` -&gt; `...448`\n* `` -&gt; `...449`\n* `` -&gt; `...450`\n* `` -&gt; `...451`\n* `` -&gt; `...452`\n* `` -&gt; `...453`\n* `` -&gt; `...454`\n* `` -&gt; `...455`\n* `` -&gt; `...456`\n* `` -&gt; `...457`\n* `` -&gt; `...458`\n* `` -&gt; `...459`\n* `` -&gt; `...460`\n* `` -&gt; `...461`\n* `` -&gt; `...462`\n* `` -&gt; `...463`\n* `` -&gt; `...464`\n* `` -&gt; `...465`\n* `` -&gt; `...466`\n* `` -&gt; `...467`\n* `` -&gt; `...468`\n* `` -&gt; `...469`\n* `` -&gt; `...470`\n* `` -&gt; `...471`\n* `` -&gt; `...472`\n* `` -&gt; `...473`\n* `` -&gt; `...474`\n* `` -&gt; `...475`\n* `` -&gt; `...476`\n* `` -&gt; `...477`\n* `` -&gt; `...478`\n* `` -&gt; `...479`\n* `` -&gt; `...480`\n* `` -&gt; `...481`\n* `` -&gt; `...482`\n* `` -&gt; `...483`\n* `` -&gt; `...484`\n* `` -&gt; `...485`\n* `` -&gt; `...486`\n* `` -&gt; `...487`\n* `` -&gt; `...488`\n* `` -&gt; `...489`\n* `` -&gt; `...490`\n* `` -&gt; `...491`\n* `` -&gt; `...492`\n* `` -&gt; `...493`\n* `` -&gt; `...494`\n* `` -&gt; `...495`\n* `` -&gt; `...496`\n* `` -&gt; `...497`\n* `` -&gt; `...498`\n* `` -&gt; `...499`\n* `` -&gt; `...500`\n* `` -&gt; `...501`\n* `` -&gt; `...502`\n* `` -&gt; `...503`\n* `` -&gt; `...504`\n* `` -&gt; `...505`\n* `` -&gt; `...506`\n* `` -&gt; `...507`\n* `` -&gt; `...508`\n* `` -&gt; `...509`\n* `` -&gt; `...510`\n* `` -&gt; `...511`\n* `` -&gt; `...512`\n* `` -&gt; `...513`\n* `` -&gt; `...514`\n* `` -&gt; `...515`\n* `` -&gt; `...516`\n* `` -&gt; `...517`\n* `` -&gt; `...518`\n* `` -&gt; `...519`\n* `` -&gt; `...520`\n* `` -&gt; `...521`\n* `` -&gt; `...522`\n* `` -&gt; `...523`\n* `` -&gt; `...524`\n* `` -&gt; `...525`\n* `` -&gt; `...526`\n* `` -&gt; `...527`\n* `` -&gt; `...528`\n* `` -&gt; `...529`\n* `` -&gt; `...530`\n* `` -&gt; `...531`\n* `` -&gt; `...532`\n* `` -&gt; `...533`\n* `` -&gt; `...534`\n* `` -&gt; `...535`\n* `` -&gt; `...537`\n* `` -&gt; `...538`\n* `` -&gt; `...539`\n* `` -&gt; `...540`\n* `` -&gt; `...541`\n* `` -&gt; `...542`\n* `` -&gt; `...543`\n* `` -&gt; `...544`\n* `` -&gt; `...545`\n* `` -&gt; `...546`\n* `` -&gt; `...547`\n* `` -&gt; `...548`\n* `` -&gt; `...549`\n* `` -&gt; `...550`\n* `` -&gt; `...551`\n* `` -&gt; `...552`\n* `` -&gt; `...553`\n* `` -&gt; `...554`\n* `` -&gt; `...555`\n* `` -&gt; `...556`\n* `` -&gt; `...557`\n* `` -&gt; `...558`\n* `` -&gt; `...559`\n* `` -&gt; `...560`\n* `` -&gt; `...561`\n* `` -&gt; `...562`\n* `` -&gt; `...563`\n* `` -&gt; `...564`\n* `` -&gt; `...565`\n* `` -&gt; `...566`\n* `` -&gt; `...567`\n* `` -&gt; `...568`\n* `` -&gt; `...569`\n* `` -&gt; `...570`\n* `` -&gt; `...571`\n* `` -&gt; `...572`\n* `` -&gt; `...573`\n* `` -&gt; `...574`\n* `` -&gt; `...575`\n* `` -&gt; `...576`\n* `` -&gt; `...577`\n* `` -&gt; `...578`\n* `` -&gt; `...579`\n* `` -&gt; `...580`\n* `` -&gt; `...581`\n* `` -&gt; `...582`\n* `` -&gt; `...583`\n* `` -&gt; `...584`\n* `` -&gt; `...585`\n* `` -&gt; `...586`\n* `` -&gt; `...587`\n* `` -&gt; `...588`\n* `` -&gt; `...589`\n* `` -&gt; `...590`\n* `` -&gt; `...591`\n* `` -&gt; `...592`\n* `` -&gt; `...593`\n* `` -&gt; `...594`\n* `` -&gt; `...595`\n* `` -&gt; `...596`\n* `` -&gt; `...597`\n* `` -&gt; `...598`\n* `` -&gt; `...599`\n* `` -&gt; `...600`\n* `` -&gt; `...601`\n* `` -&gt; `...602`\n* `` -&gt; `...603`\n* `` -&gt; `...604`\n* `` -&gt; `...605`\n* `` -&gt; `...606`\n* `` -&gt; `...607`\n* `` -&gt; `...608`\n* `` -&gt; `...609`\n* `` -&gt; `...610`\n* `` -&gt; `...611`\n* `` -&gt; `...612`\n* `` -&gt; `...613`\n* `` -&gt; `...614`\n* `` -&gt; `...615`\n* `` -&gt; `...616`\n* `` -&gt; `...617`\n* `` -&gt; `...618`\n* `` -&gt; `...619`\n* `` -&gt; `...620`\n* `` -&gt; `...621`\n* `` -&gt; `...622`\n* `` -&gt; `...623`\n* `` -&gt; `...624`\n* `` -&gt; `...625`\n* `` -&gt; `...626`\n* `` -&gt; `...627`\n* `` -&gt; `...628`\n* `` -&gt; `...629`\n* `` -&gt; `...630`\n* `` -&gt; `...631`\n* `` -&gt; `...632`\n* `` -&gt; `...633`\n* `` -&gt; `...634`\n* `` -&gt; `...635`\n* `` -&gt; `...636`\n* `` -&gt; `...637`\n* `` -&gt; `...638`\n* `` -&gt; `...639`\n* `` -&gt; `...640`\n* `` -&gt; `...641`\n* `` -&gt; `...642`\n* `` -&gt; `...643`\n* `` -&gt; `...644`\n* `` -&gt; `...645`\n* `` -&gt; `...646`\n* `` -&gt; `...647`\n* `` -&gt; `...648`\n* `` -&gt; `...649`\n* `` -&gt; `...650`\n* `` -&gt; `...651`\n* `` -&gt; `...652`\n* `` -&gt; `...653`\n* `` -&gt; `...654`\n* `` -&gt; `...655`\n* `` -&gt; `...656`\n* `` -&gt; `...657`\n* `` -&gt; `...658`\n* `` -&gt; `...659`\n* `` -&gt; `...660`\n* `` -&gt; `...661`\n* `` -&gt; `...662`\n* `` -&gt; `...663`\n* `` -&gt; `...664`\n* `` -&gt; `...665`\n* `` -&gt; `...666`\n* `` -&gt; `...667`\n* `` -&gt; `...668`\n* `` -&gt; `...669`\n* `` -&gt; `...670`\n* `` -&gt; `...671`\n* `` -&gt; `...672`\n* `` -&gt; `...673`\n* `` -&gt; `...674`\n* `` -&gt; `...675`\n* `` -&gt; `...676`\n* `` -&gt; `...677`\n* `` -&gt; `...678`\n* `` -&gt; `...679`\n* `` -&gt; `...680`\n* `` -&gt; `...681`\n* `` -&gt; `...682`\n* `` -&gt; `...683`\n* `` -&gt; `...684`\n* `` -&gt; `...685`\n* `` -&gt; `...686`\n* `` -&gt; `...687`\n* `` -&gt; `...688`\n* `` -&gt; `...689`\n* `` -&gt; `...690`\n* `` -&gt; `...691`\n* `` -&gt; `...692`\n* `` -&gt; `...693`\n* `` -&gt; `...694`\n* `` -&gt; `...695`\n* `` -&gt; `...696`\n* `` -&gt; `...697`\n* `` -&gt; `...698`\n* `` -&gt; `...699`\n* `` -&gt; `...700`\n* `` -&gt; `...701`\n* `` -&gt; `...702`\n* `` -&gt; `...703`\n* `` -&gt; `...704`\n* `` -&gt; `...705`\n* `` -&gt; `...706`\n* `` -&gt; `...707`\n* `` -&gt; `...708`\n* `` -&gt; `...709`\n* `` -&gt; `...710`\n* `` -&gt; `...711`\n* `` -&gt; `...712`\n* `` -&gt; `...713`\n* `` -&gt; `...714`\n* `` -&gt; `...715`\n* `` -&gt; `...716`\n* `` -&gt; `...717`\n* `` -&gt; `...718`\n* `` -&gt; `...719`\n* `` -&gt; `...720`\n* `` -&gt; `...721`\n* `` -&gt; `...722`\n* `` -&gt; `...723`\n* `` -&gt; `...724`\n* `` -&gt; `...725`\n* `` -&gt; `...726`\n* `` -&gt; `...727`\n* `` -&gt; `...728`\n* `` -&gt; `...729`\n* `` -&gt; `...730`\n* `` -&gt; `...731`\n* `` -&gt; `...732`\n* `` -&gt; `...733`\n* `` -&gt; `...734`\n* `` -&gt; `...735`\n* `` -&gt; `...736`\n* `` -&gt; `...737`\n* `` -&gt; `...738`\n* `` -&gt; `...739`\n* `` -&gt; `...740`\n* `` -&gt; `...741`\n* `` -&gt; `...742`\n* `` -&gt; `...743`\n* `` -&gt; `...744`\n* `` -&gt; `...745`\n* `` -&gt; `...746`\n* `` -&gt; `...747`\n* `` -&gt; `...748`\n* `` -&gt; `...749`\n* `` -&gt; `...750`\n* `` -&gt; `...751`\n* `` -&gt; `...752`\n* `` -&gt; `...753`\n* `` -&gt; `...754`\n* `` -&gt; `...755`\n* `` -&gt; `...756`\n* `` -&gt; `...757`\n* `` -&gt; `...758`\n* `` -&gt; `...759`\n* `` -&gt; `...760`\n* `` -&gt; `...761`\n* `` -&gt; `...762`\n* `` -&gt; `...763`\n* `` -&gt; `...764`\n* `` -&gt; `...765`\n* `` -&gt; `...766`\n* `` -&gt; `...767`\n* `` -&gt; `...768`\n* `` -&gt; `...769`\n* `` -&gt; `...770`\n* `` -&gt; `...771`\n* `` -&gt; `...772`\n* `` -&gt; `...773`\n* `` -&gt; `...774`\n* `` -&gt; `...775`\n* `` -&gt; `...776`\n* `` -&gt; `...777`\n* `` -&gt; `...778`\n* `` -&gt; `...779`\n* `` -&gt; `...780`\n* `` -&gt; `...781`\n* `` -&gt; `...782`\n* `` -&gt; `...783`\n* `` -&gt; `...784`\n* `` -&gt; `...785`\n* `` -&gt; `...786`\n* `` -&gt; `...787`\n* `` -&gt; `...788`\n* `` -&gt; `...789`\n* `` -&gt; `...790`\n* `` -&gt; `...791`\n* `` -&gt; `...792`\n* `` -&gt; `...793`\n* `` -&gt; `...794`\n* `` -&gt; `...795`\n* `` -&gt; `...796`\n* `` -&gt; `...797`\n* `` -&gt; `...798`\n* `` -&gt; `...799`\n* `` -&gt; `...800`\n* `` -&gt; `...801`\n* `` -&gt; `...802`\n* `` -&gt; `...803`\n* `` -&gt; `...804`\n* `` -&gt; `...805`\n* `` -&gt; `...806`\n* `` -&gt; `...807`\n* `` -&gt; `...808`\n* `` -&gt; `...809`\n* `` -&gt; `...810`\n* `` -&gt; `...811`\n* `` -&gt; `...812`\n* `` -&gt; `...813`\n* `` -&gt; `...814`\n* `` -&gt; `...815`\n* `` -&gt; `...816`\n* `` -&gt; `...817`\n* `` -&gt; `...818`\n* `` -&gt; `...819`\n* `` -&gt; `...820`\n* `` -&gt; `...821`\n* `` -&gt; `...822`\n* `` -&gt; `...823`\n* `` -&gt; `...824`\n* `` -&gt; `...825`\n* `` -&gt; `...826`\n* `` -&gt; `...827`\n* `` -&gt; `...828`\n* `` -&gt; `...829`\n* `` -&gt; `...830`\n* `` -&gt; `...831`\n* `` -&gt; `...832`\n* `` -&gt; `...833`\n* `` -&gt; `...834`\n* `` -&gt; `...835`\n* `` -&gt; `...836`\n* `` -&gt; `...837`\n* `` -&gt; `...838`\n* `` -&gt; `...839`\n* `` -&gt; `...840`\n* `` -&gt; `...841`\n* `` -&gt; `...842`\n* `` -&gt; `...843`\n* `` -&gt; `...844`\n* `` -&gt; `...845`\n* `` -&gt; `...846`\n* `` -&gt; `...847`\n* `` -&gt; `...848`\n* `` -&gt; `...849`\n* `` -&gt; `...850`\n* `` -&gt; `...851`\n* `` -&gt; `...852`\n* `` -&gt; `...853`\n* `` -&gt; `...854`\n* `` -&gt; `...855`\n* `` -&gt; `...856`\n* `` -&gt; `...857`\n* `` -&gt; `...858`\n* `` -&gt; `...859`\n* `` -&gt; `...860`\n* `` -&gt; `...861`\n* `` -&gt; `...862`\n* `` -&gt; `...863`\n* `` -&gt; `...864`\n* `` -&gt; `...865`\n* `` -&gt; `...866`\n* `` -&gt; `...867`\n* `` -&gt; `...868`\n* `` -&gt; `...869`\n* `` -&gt; `...870`\n* `` -&gt; `...871`\n* `` -&gt; `...872`\n* `` -&gt; `...873`\n* `` -&gt; `...874`\n* `` -&gt; `...875`\n* `` -&gt; `...876`\n* `` -&gt; `...877`\n* `` -&gt; `...878`\n* `` -&gt; `...879`\n* `` -&gt; `...880`\n* `` -&gt; `...881`\n* `` -&gt; `...882`\n* `` -&gt; `...883`\n* `` -&gt; `...884`\n* `` -&gt; `...885`\n* `` -&gt; `...886`\n* `` -&gt; `...887`\n* `` -&gt; `...888`\n* `` -&gt; `...889`\n* `` -&gt; `...890`\n* `` -&gt; `...891`\n* `` -&gt; `...892`\n* `` -&gt; `...893`\n* `` -&gt; `...894`\n* `` -&gt; `...895`\n* `` -&gt; `...896`\n* `` -&gt; `...897`\n* `` -&gt; `...898`\n* `` -&gt; `...899`\n* `` -&gt; `...900`\n* `` -&gt; `...901`\n* `` -&gt; `...902`\n* `` -&gt; `...903`\n* `` -&gt; `...904`\n* `` -&gt; `...905`\n* `` -&gt; `...906`\n* `` -&gt; `...907`\n* `` -&gt; `...908`\n* `` -&gt; `...909`\n* `` -&gt; `...910`\n* `` -&gt; `...911`\n* `` -&gt; `...912`\n* `` -&gt; `...913`\n* `` -&gt; `...914`\n* `` -&gt; `...915`\n* `` -&gt; `...916`\n* `` -&gt; `...917`\n* `` -&gt; `...918`\n* `` -&gt; `...919`\n* `` -&gt; `...920`\n* `` -&gt; `...921`\n* `` -&gt; `...922`\n* `` -&gt; `...923`\n* `` -&gt; `...924`\n* `` -&gt; `...925`\n* `` -&gt; `...926`\n* `` -&gt; `...927`\n* `` -&gt; `...928`\n* `` -&gt; `...929`\n* `` -&gt; `...930`\n* `` -&gt; `...931`\n* `` -&gt; `...932`\n* `` -&gt; `...933`\n* `` -&gt; `...934`\n* `` -&gt; `...935`\n* `` -&gt; `...936`\n* `` -&gt; `...937`\n* `` -&gt; `...938`\n* `` -&gt; `...939`\n* `` -&gt; `...940`\n* `` -&gt; `...941`\n* `` -&gt; `...942`\n* `` -&gt; `...943`\n* `` -&gt; `...944`\n* `` -&gt; `...945`\n* `` -&gt; `...946`\n* `` -&gt; `...947`\n* `` -&gt; `...948`\n* `` -&gt; `...949`\n* `` -&gt; `...950`\n* `` -&gt; `...951`\n* `` -&gt; `...952`\n* `` -&gt; `...953`\n* `` -&gt; `...954`\n* `` -&gt; `...955`\n* `` -&gt; `...956`\n* `` -&gt; `...957`\n* `` -&gt; `...958`\n* `` -&gt; `...959`\n* `` -&gt; `...960`\n* `` -&gt; `...961`\n* `` -&gt; `...962`\n* `` -&gt; `...963`\n* `` -&gt; `...964`\n* `` -&gt; `...965`\n* `` -&gt; `...966`\n* `` -&gt; `...967`\n* `` -&gt; `...968`\n* `` -&gt; `...969`\n* `` -&gt; `...970`\n* `` -&gt; `...971`\n* `` -&gt; `...972`\n* `` -&gt; `...973`\n* `` -&gt; `...974`\n* `` -&gt; `...975`\n* `` -&gt; `...976`\n* `` -&gt; `...977`\n* `` -&gt; `...978`\n* `` -&gt; `...979`\n* `` -&gt; `...980`\n* `` -&gt; `...981`\n* `` -&gt; `...982`\n* `` -&gt; `...983`\n* `` -&gt; `...984`\n* `` -&gt; `...985`\n* `` -&gt; `...986`\n* `` -&gt; `...987`\n* `` -&gt; `...988`\n* `` -&gt; `...989`\n* `` -&gt; `...990`\n* `` -&gt; `...991`\n* `` -&gt; `...992`\n* `` -&gt; `...993`\n* `` -&gt; `...994`\n* `` -&gt; `...995`\n* `` -&gt; `...996`\n* `` -&gt; `...997`\n* `` -&gt; `...998`\n* `` -&gt; `...999`\n* `` -&gt; `...1000`\n* `` -&gt; `...1001`\n* `` -&gt; `...1002`\n* `` -&gt; `...1003`\n* `` -&gt; `...1004`\n* `` -&gt; `...1005`\n* `` -&gt; `...1006`\n* `` -&gt; `...1007`\n* `` -&gt; `...1008`\n* `` -&gt; `...1009`\n* `` -&gt; `...1010`\n* `` -&gt; `...1011`\n* `` -&gt; `...1012`\n* `` -&gt; `...1013`\n* `` -&gt; `...1014`\n* `` -&gt; `...1015`\n* `` -&gt; `...1016`\n* `` -&gt; `...1017`\n* `` -&gt; `...1018`\n* `` -&gt; `...1019`\n* `` -&gt; `...1020`\n* `` -&gt; `...1021`\n* `` -&gt; `...1022`\n* `` -&gt; `...1023`\n* `` -&gt; `...1024`\n* `` -&gt; `...1025`\n* `` -&gt; `...1026`\n* `` -&gt; `...1027`\n* `` -&gt; `...1028`\n* `` -&gt; `...1029`\n* `` -&gt; `...1030`\n* `` -&gt; `...1031`\n* `` -&gt; `...1032`\n* `` -&gt; `...1033`\n* `` -&gt; `...1034`\n* `` -&gt; `...1035`\n* `` -&gt; `...1036`\n* `` -&gt; `...1037`\n* `` -&gt; `...1038`\n* `` -&gt; `...1039`\n* `` -&gt; `...1040`\n* `` -&gt; `...1041`\n* `` -&gt; `...1042`\n* `` -&gt; `...1043`\n* `` -&gt; `...1044`\n* `` -&gt; `...1045`\n* `` -&gt; `...1046`\n* `` -&gt; `...1047`\n* `` -&gt; `...1048`\n* `` -&gt; `...1049`\n* `` -&gt; `...1050`\n* `` -&gt; `...1051`\n* `` -&gt; `...1052`\n* `` -&gt; `...1053`\n* `` -&gt; `...1054`\n* `` -&gt; `...1055`\n* `` -&gt; `...1056`\n* `` -&gt; `...1057`\n* `` -&gt; `...1058`\n* `` -&gt; `...1059`\n* `` -&gt; `...1060`\n* `` -&gt; `...1061`\n* `` -&gt; `...1062`\n* `` -&gt; `...1063`\n* `` -&gt; `...1064`\n* `` -&gt; `...1065`\n* `` -&gt; `...1066`\n* `` -&gt; `...1067`\n* `` -&gt; `...1068`\n* `` -&gt; `...1069`\n* `` -&gt; `...1070`\n* `` -&gt; `...1071`\n* `` -&gt; `...1072`\n* `` -&gt; `...1073`\n* `` -&gt; `...1074`\n* `` -&gt; `...1075`\n* `` -&gt; `...1076`\n* `` -&gt; `...1077`\n* `` -&gt; `...1078`\n* `` -&gt; `...1079`\n* `` -&gt; `...1080`\n* `` -&gt; `...1081`\n* `` -&gt; `...1082`\n* `` -&gt; `...1083`\n* `` -&gt; `...1084`\n* `` -&gt; `...1085`\n* `` -&gt; `...1086`\n* `` -&gt; `...1087`\n* `` -&gt; `...1088`\n* `` -&gt; `...1089`\n* `` -&gt; `...1090`\n* `` -&gt; `...1091`\n* `` -&gt; `...1092`\n* `` -&gt; `...1093`\n* `` -&gt; `...1094`\n* `` -&gt; `...1095`\n* `` -&gt; `...1096`\n* `` -&gt; `...1097`\n* `` -&gt; `...1098`\n* `` -&gt; `...1099`\n* `` -&gt; `...1100`\n* `` -&gt; `...1101`\n* `` -&gt; `...1102`\n* `` -&gt; `...1103`\n* `` -&gt; `...1104`\n* `` -&gt; `...1105`\n* `` -&gt; `...1106`\n* `` -&gt; `...1107`\n* `` -&gt; `...1108`\n* `` -&gt; `...1109`\n* `` -&gt; `...1110`\n* `` -&gt; `...1111`\n* `` -&gt; `...1112`\n* `` -&gt; `...1113`\n* `` -&gt; `...1114`\n* `` -&gt; `...1115`\n* `` -&gt; `...1116`\n* `` -&gt; `...1117`\n* `` -&gt; `...1118`\n* `` -&gt; `...1119`\n* `` -&gt; `...1120`\n* `` -&gt; `...1121`\n* `` -&gt; `...1122`\n* `` -&gt; `...1123`\n* `` -&gt; `...1124`\n* `` -&gt; `...1125`\n* `` -&gt; `...1126`\n* `` -&gt; `...1127`\n* `` -&gt; `...1128`\n* `` -&gt; `...1129`\n* `` -&gt; `...1130`\n* `` -&gt; `...1131`\n* `` -&gt; `...1132`\n* `` -&gt; `...1133`\n* `` -&gt; `...1134`\n* `` -&gt; `...1135`\n* `` -&gt; `...1136`\n* `` -&gt; `...1137`\n* `` -&gt; `...1138`\n* `` -&gt; `...1139`\n* `` -&gt; `...1140`\n* `` -&gt; `...1141`\n* `` -&gt; `...1142`\n* `` -&gt; `...1143`\n* `` -&gt; `...1144`\n* `` -&gt; `...1145`\n* `` -&gt; `...1146`\n* `` -&gt; `...1147`\n* `` -&gt; `...1148`\n* `` -&gt; `...1149`\n* `` -&gt; `...1150`\n* `` -&gt; `...1151`\n* `` -&gt; `...1152`\n* `` -&gt; `...1153`\n* `` -&gt; `...1154`\n* `` -&gt; `...1155`\n* `` -&gt; `...1156`\n* `` -&gt; `...1157`\n* `` -&gt; `...1158`\n* `` -&gt; `...1159`\n* `` -&gt; `...1160`\n* `` -&gt; `...1161`\n* `` -&gt; `...1162`\n* `` -&gt; `...1163`\n* `` -&gt; `...1164`\n* `` -&gt; `...1165`\n* `` -&gt; `...1166`\n* `` -&gt; `...1167`\n* `` -&gt; `...1168`\n* `` -&gt; `...1169`\n* `` -&gt; `...1170`\n* `` -&gt; `...1171`\n* `` -&gt; `...1172`\n* `` -&gt; `...1173`\n* `` -&gt; `...1174`\n* `` -&gt; `...1175`\n* `` -&gt; `...1176`\n* `` -&gt; `...1177`\n* `` -&gt; `...1178`\n* `` -&gt; `...1179`\n* `` -&gt; `...1180`\n* `` -&gt; `...1181`\n* `` -&gt; `...1182`\n* `` -&gt; `...1183`\n* `` -&gt; `...1184`\n* `` -&gt; `...1185`\n* `` -&gt; `...1186`\n* `` -&gt; `...1187`\n* `` -&gt; `...1188`\n* `` -&gt; `...1189`\n* `` -&gt; `...1190`\n* `` -&gt; `...1191`\n* `` -&gt; `...1192`\n* `` -&gt; `...1193`\n* `` -&gt; `...1194`\n* `` -&gt; `...1195`\n* `` -&gt; `...1196`\n* `` -&gt; `...1197`\n* `` -&gt; `...1198`\n* `` -&gt; `...1199`\n* `` -&gt; `...1200`\n* `` -&gt; `...1201`\n* `` -&gt; `...1202`\n* `` -&gt; `...1203`\n* `` -&gt; `...1204`\n* `` -&gt; `...1205`\n* `` -&gt; `...1206`\n* `` -&gt; `...1207`\n* `` -&gt; `...1208`\n* `` -&gt; `...1209`\n* `` -&gt; `...1210`\n* `` -&gt; `...1211`\n* `` -&gt; `...1212`\n* `` -&gt; `...1213`\n* `` -&gt; `...1214`\n* `` -&gt; `...1215`\n* `` -&gt; `...1216`\n* `` -&gt; `...1217`\n* `` -&gt; `...1218`\n* `` -&gt; `...1219`\n* `` -&gt; `...1220`\n* `` -&gt; `...1221`\n* `` -&gt; `...1222`\n* `` -&gt; `...1223`\n* `` -&gt; `...1224`\n* `` -&gt; `...1225`\n* `` -&gt; `...1226`\n* `` -&gt; `...1227`\n* `` -&gt; `...1228`\n* `` -&gt; `...1229`\n* `` -&gt; `...1230`\n* `` -&gt; `...1231`\n* `` -&gt; `...1232`\n* `` -&gt; `...1233`\n* `` -&gt; `...1234`\n* `` -&gt; `...1235`\n* `` -&gt; `...1236`\n* `` -&gt; `...1237`\n* `` -&gt; `...1238`\n* `` -&gt; `...1239`\n* `` -&gt; `...1240`\n* `` -&gt; `...1241`\n* `` -&gt; `...1242`\n* `` -&gt; `...1243`\n* `` -&gt; `...1244`\n* `` -&gt; `...1245`\n* `` -&gt; `...1246`\n* `` -&gt; `...1247`\n* `` -&gt; `...1248`\n* `` -&gt; `...1249`\n* `` -&gt; `...1250`\n* `` -&gt; `...1251`\n* `` -&gt; `...1252`\n* `` -&gt; `...1253`\n* `` -&gt; `...1254`\n* `` -&gt; `...1255`\n* `` -&gt; `...1256`\n* `` -&gt; `...1257`\n* `` -&gt; `...1258`\n* `` -&gt; `...1259`\n* `` -&gt; `...1260`\n* `` -&gt; `...1261`\n* `` -&gt; `...1262`\n* `` -&gt; `...1263`\n* `` -&gt; `...1264`\n* `` -&gt; `...1265`\n* `` -&gt; `...1266`\n* `` -&gt; `...1267`\n* `` -&gt; `...1268`\n* `11` -&gt; `11...1269`\n* `` -&gt; `...1270`\n* `` -&gt; `...1271`\n* `` -&gt; `...1272`\n* `` -&gt; `...1273`\n* `` -&gt; `...1275`\n* `` -&gt; `...1276`\n* `` -&gt; `...1277`\n* `` -&gt; `...1278`\n* `` -&gt; `...1279`\n* `` -&gt; `...1280`\n* `` -&gt; `...1281`\n* `` -&gt; `...1282`\n* `` -&gt; `...1283`\n* `` -&gt; `...1284`\n* `` -&gt; `...1285`\n* `` -&gt; `...1286`\n* `` -&gt; `...1287`\n* `` -&gt; `...1288`\n* `` -&gt; `...1289`\n* `` -&gt; `...1290`\n* `` -&gt; `...1291`\n* `` -&gt; `...1292`\n* `` -&gt; `...1293`\n* `` -&gt; `...1294`\n* `` -&gt; `...1295`\n* `` -&gt; `...1296`\n* `` -&gt; `...1297`\n* `` -&gt; `...1298`\n* `` -&gt; `...1299`\n* `` -&gt; `...1300`\n* `` -&gt; `...1301`\n* `` -&gt; `...1302`\n* `` -&gt; `...1304`\n* `` -&gt; `...1305`\n* `` -&gt; `...1306`\n* `` -&gt; `...1307`\n* `01` -&gt; `01...1308`\n* `` -&gt; `...1309`\n* `` -&gt; `...1310`\n* `` -&gt; `...1311`\n* `` -&gt; `...1312`\n* `` -&gt; `...1313`\n* `` -&gt; `...1314`\n* `` -&gt; `...1315`\n* `` -&gt; `...1316`\n* `` -&gt; `...1318`\n* `` -&gt; `...1319`\n* `` -&gt; `...1320`\n* `` -&gt; `...1321`\n* `` -&gt; `...1322`\n* `` -&gt; `...1323`\n* `` -&gt; `...1324`\n* `` -&gt; `...1325`\n* `` -&gt; `...1326`\n* `` -&gt; `...1327`\n* `` -&gt; `...1328`\n* `` -&gt; `...1329`\n* `` -&gt; `...1330`\n* `` -&gt; `...1331`\n* `` -&gt; `...1332`\n* `` -&gt; `...1333`\n* `` -&gt; `...1334`\n* `` -&gt; `...1335`\n* `` -&gt; `...1336`\n* `` -&gt; `...1337`\n* `` -&gt; `...1338`\n* `` -&gt; `...1339`\n* `` -&gt; `...1340`\n* `` -&gt; `...1341`\n* `` -&gt; `...1342`\n* `` -&gt; `...1343`\n* `` -&gt; `...1344`\n* `` -&gt; `...1345`\n* `` -&gt; `...1346`\n* `` -&gt; `...1347`\n* `` -&gt; `...1348`\n* `` -&gt; `...1349`\n* `` -&gt; `...1350`\n* `` -&gt; `...1351`\n* `` -&gt; `...1352`\n* `` -&gt; `...1353`\n* `` -&gt; `...1354`\n* `` -&gt; `...1355`\n* `` -&gt; `...1356`\n* `` -&gt; `...1357`\n* `` -&gt; `...1358`\n* `` -&gt; `...1359`\n* `` -&gt; `...1360`\n* `` -&gt; `...1361`\n* `` -&gt; `...1362`\n* `` -&gt; `...1363`\n* `` -&gt; `...1364`\n* `` -&gt; `...1365`\n* `` -&gt; `...1366`\n* `` -&gt; `...1367`\n* `` -&gt; `...1368`\n* `` -&gt; `...1369`\n* `` -&gt; `...1370`\n* `` -&gt; `...1371`\n* `` -&gt; `...1372`\n* `` -&gt; `...1373`\n* `` -&gt; `...1374`\n* `` -&gt; `...1375`\n* `` -&gt; `...1376`\n* `` -&gt; `...1377`\n* `` -&gt; `...1378`\n* `` -&gt; `...1379`\n* `` -&gt; `...1380`\n* `` -&gt; `...1381`\n* `` -&gt; `...1382`\n* `` -&gt; `...1383`\n* `` -&gt; `...1384`\n* `` -&gt; `...1385`\n* `` -&gt; `...1386`\n* `` -&gt; `...1387`\n* `` -&gt; `...1388`\n* `` -&gt; `...1389`\n* `` -&gt; `...1390`\n* `` -&gt; `...1391`\n* `` -&gt; `...1392`\n* `` -&gt; `...1393`\n* `` -&gt; `...1394`\n* `` -&gt; `...1395`\n* `` -&gt; `...1396`\n* `` -&gt; `...1397`\n* `` -&gt; `...1398`\n* `` -&gt; `...1399`\n* `` -&gt; `...1400`\n* `` -&gt; `...1401`\n* `` -&gt; `...1402`\n* `` -&gt; `...1403`\n* `` -&gt; `...1404`\n* `` -&gt; `...1405`\n* `` -&gt; `...1406`\n* `` -&gt; `...1407`\n* `` -&gt; `...1408`\n* `` -&gt; `...1409`\n* `` -&gt; `...1410`\n* `` -&gt; `...1411`\n* `` -&gt; `...1412`\n* `` -&gt; `...1413`\n* `` -&gt; `...1414`\n* `` -&gt; `...1415`\n* `` -&gt; `...1416`\n* `` -&gt; `...1417`\n* `` -&gt; `...1418`\n* `` -&gt; `...1419`\n* `` -&gt; `...1420`\n* `` -&gt; `...1421`\n* `` -&gt; `...1422`\n* `` -&gt; `...1423`\n* `` -&gt; `...1424`\n* `` -&gt; `...1425`\n* `` -&gt; `...1426`\n* `` -&gt; `...1427`\n* `` -&gt; `...1428`\n* `` -&gt; `...1429`\n* `` -&gt; `...1430`\n* `` -&gt; `...1431`\n* `` -&gt; `...1432`\n* `` -&gt; `...1433`\n* `` -&gt; `...1434`\n* `` -&gt; `...1435`\n* `` -&gt; `...1436`\n* `` -&gt; `...1437`\n* `` -&gt; `...1438`\n* `` -&gt; `...1439`\n* `` -&gt; `...1440`\n* `` -&gt; `...1441`\n* `` -&gt; `...1442`\n* `` -&gt; `...1443`\n* `` -&gt; `...1444`\n* `` -&gt; `...1445`\n* `` -&gt; `...1446`\n* `` -&gt; `...1447`\n* `` -&gt; `...1448`\n* `` -&gt; `...1449`\n* `` -&gt; `...1450`\n* `` -&gt; `...1451`\n* `` -&gt; `...1452`\n* `` -&gt; `...1453`\n* `` -&gt; `...1454`\n* `` -&gt; `...1455`\n* `` -&gt; `...1456`\n* `` -&gt; `...1457`\n* `` -&gt; `...1458`\n* `` -&gt; `...1459`\n* `` -&gt; `...1460`\n* `` -&gt; `...1461`\n* `` -&gt; `...1462`\n* `` -&gt; `...1463`\n* `` -&gt; `...1464`\n* `` -&gt; `...1465`\n* `` -&gt; `...1466`\n* `` -&gt; `...1467`\n* `` -&gt; `...1468`\n* `` -&gt; `...1469`\n* `` -&gt; `...1470`\n* `` -&gt; `...1471`\n* `` -&gt; `...1472`\n* `` -&gt; `...1473`\n* `` -&gt; `...1474`\n* `` -&gt; `...1475`\n* `` -&gt; `...1476`\n* `` -&gt; `...1477`\n* `` -&gt; `...1478`\n* `` -&gt; `...1479`\n* `` -&gt; `...1480`\n* `` -&gt; `...1481`\n* `` -&gt; `...1482`\n* `` -&gt; `...1483`\n* `` -&gt; `...1484`\n* `` -&gt; `...1485`\n* `` -&gt; `...1486`\n* `` -&gt; `...1487`\n* `` -&gt; `...1488`\n* `` -&gt; `...1489`\n* `` -&gt; `...1490`\n* `` -&gt; `...1491`\n* `` -&gt; `...1492`\n* `` -&gt; `...1493`\n* `` -&gt; `...1494`\n* `` -&gt; `...1495`\n* `` -&gt; `...1496`\n* `` -&gt; `...1497`\n* `` -&gt; `...1498`\n* `` -&gt; `...1499`\n* `` -&gt; `...1500`\n* `` -&gt; `...1501`\n* `` -&gt; `...1502`\n* `` -&gt; `...1503`\n* `` -&gt; `...1504`\n* `` -&gt; `...1505`\n* `` -&gt; `...1506`\n* `` -&gt; `...1507`\n* `` -&gt; `...1508`\n* `` -&gt; `...1509`\n* `` -&gt; `...1510`\n* `` -&gt; `...1511`\n* `` -&gt; `...1512`\n* `` -&gt; `...1513`\n* `` -&gt; `...1514`\n* `` -&gt; `...1515`\n* `` -&gt; `...1517`\n* `` -&gt; `...1518`\n* `` -&gt; `...1519`\n* `` -&gt; `...1520`\n* `` -&gt; `...1521`\n* `` -&gt; `...1522`\n* `` -&gt; `...1523`\n* `` -&gt; `...1524`\n* `` -&gt; `...1525`\n* `` -&gt; `...1526`\n* `` -&gt; `...1527`\n* `` -&gt; `...1528`\n* `` -&gt; `...1529`\n* `` -&gt; `...1530`\n* `` -&gt; `...1531`\n* `` -&gt; `...1532`\n* `` -&gt; `...1533`\n* `` -&gt; `...1534`\n* `` -&gt; `...1535`\n* `` -&gt; `...1536`\n* `` -&gt; `...1537`\n* `` -&gt; `...1538`\n* `` -&gt; `...1539`\n* `` -&gt; `...1540`\n* `` -&gt; `...1541`\n* `` -&gt; `...1542`\n* `` -&gt; `...1543`\n* `` -&gt; `...1544`\n* `` -&gt; `...1545`\n* `` -&gt; `...1546`\n* `` -&gt; `...1547`\n* `` -&gt; `...1548`\n* `` -&gt; `...1549`\n* `` -&gt; `...1550`\n* `` -&gt; `...1551`\n* `` -&gt; `...1552`\n* `` -&gt; `...1553`\n* `` -&gt; `...1554`\n* `` -&gt; `...1555`\n* `` -&gt; `...1556`\n* `` -&gt; `...1557`\n* `` -&gt; `...1558`\n* `` -&gt; `...1559`\n* `` -&gt; `...1560`\n* `` -&gt; `...1561`\n* `` -&gt; `...1562`\n* `` -&gt; `...1563`\n* `` -&gt; `...1565`\n* `` -&gt; `...1566`\n* `` -&gt; `...1567`\n* `` -&gt; `...1568`\n* `` -&gt; `...1569`\n* `` -&gt; `...1570`\n* `` -&gt; `...1571`\n* `` -&gt; `...1572`\n* `` -&gt; `...1573`\n* `` -&gt; `...1574`\n* `` -&gt; `...1575`\n* `` -&gt; `...1576`\n* `` -&gt; `...1577`\n* `` -&gt; `...1578`\n* `` -&gt; `...1579`\n* `` -&gt; `...1580`\n* `` -&gt; `...1581`\n* `` -&gt; `...1582`\n* `` -&gt; `...1583`\n* `` -&gt; `...1584`\n* `` -&gt; `...1585`\n* `` -&gt; `...1586`\n* `` -&gt; `...1587`\n* `` -&gt; `...1588`\n* `` -&gt; `...1589`\n* `` -&gt; `...1590`\n* `` -&gt; `...1591`\n* `` -&gt; `...1592`\n* `` -&gt; `...1593`\n* `` -&gt; `...1594`\n* `` -&gt; `...1595`\n* `` -&gt; `...1596`\n* `` -&gt; `...1597`\n* `` -&gt; `...1598`\n* `` -&gt; `...1599`\n* `` -&gt; `...1600`\n* `` -&gt; `...1601`\n* `` -&gt; `...1602`\n* `` -&gt; `...1603`\n* `` -&gt; `...1604`\n* `` -&gt; `...1605`\n* `` -&gt; `...1606`\n* `` -&gt; `...1607`\n* `` -&gt; `...1608`\n* `` -&gt; `...1609`\n* `` -&gt; `...1610`\n* `` -&gt; `...1611`\n* `` -&gt; `...1612`\n* `` -&gt; `...1613`\n* `` -&gt; `...1614`\n* `` -&gt; `...1615`\n* `` -&gt; `...1616`\n* `` -&gt; `...1617`\n* `` -&gt; `...1618`\n* `` -&gt; `...1619`\n* `` -&gt; `...1620`\n* `` -&gt; `...1621`\n* `` -&gt; `...1622`\n* `` -&gt; `...1623`\n* `` -&gt; `...1624`\n* `` -&gt; `...1625`\n* `` -&gt; `...1626`\n* `` -&gt; `...1627`\n* `` -&gt; `...1628`\n* `` -&gt; `...1629`\n* `` -&gt; `...1630`\n* `` -&gt; `...1631`\n* `` -&gt; `...1632`\n* `` -&gt; `...1633`\n* `` -&gt; `...1634`\n* `` -&gt; `...1635`\n* `` -&gt; `...1636`\n* `` -&gt; `...1637`\n* `` -&gt; `...1638`\n* `` -&gt; `...1639`\n* `` -&gt; `...1640`\n* `` -&gt; `...1641`\n* `` -&gt; `...1642`\n* `` -&gt; `...1643`\n* `` -&gt; `...1644`\n* `` -&gt; `...1645`\n* `` -&gt; `...1646`\n* `` -&gt; `...1647`\n* `` -&gt; `...1648`\n* `` -&gt; `...1649`\n* `` -&gt; `...1650`\n* `` -&gt; `...1651`\n* `` -&gt; `...1652`\n* `` -&gt; `...1653`\n* `` -&gt; `...1654`\n* `` -&gt; `...1655`\n* `` -&gt; `...1656`\n* `` -&gt; `...1657`\n* `` -&gt; `...1658`\n* `` -&gt; `...1659`\n* `` -&gt; `...1660`\n* `` -&gt; `...1661`\n* `` -&gt; `...1662`\n* `` -&gt; `...1663`\n* `` -&gt; `...1664`\n* `` -&gt; `...1665`\n* `` -&gt; `...1666`\n* `` -&gt; `...1667`\n* `` -&gt; `...1668`\n* `` -&gt; `...1669`\n* `` -&gt; `...1670`\n* `` -&gt; `...1671`\n* `` -&gt; `...1672`\n* `` -&gt; `...1673`\n* `` -&gt; `...1674`\n* `` -&gt; `...1675`\n* `` -&gt; `...1676`\n* `` -&gt; `...1677`\n* `` -&gt; `...1678`\n* `` -&gt; `...1679`\n* `` -&gt; `...1680`\n* `` -&gt; `...1681`\n* `` -&gt; `...1682`\n* `` -&gt; `...1683`\n* `` -&gt; `...1684`\n* `` -&gt; `...1685`\n* `` -&gt; `...1686`\n* `` -&gt; `...1687`\n* `` -&gt; `...1688`\n* `` -&gt; `...1689`\n* `` -&gt; `...1690`\n* `` -&gt; `...1691`\n* `` -&gt; `...1692`\n* `` -&gt; `...1693`\n* `` -&gt; `...1694`\n* `` -&gt; `...1695`\n* `` -&gt; `...1696`\n* `` -&gt; `...1697`\n* `` -&gt; `...1698`\n* `` -&gt; `...1699`\n* `` -&gt; `...1700`\n* `` -&gt; `...1701`\n* `` -&gt; `...1703`\n* `` -&gt; `...1704`\n* `` -&gt; `...1705`\n* `` -&gt; `...1706`\n* `` -&gt; `...1707`\n* `` -&gt; `...1708`\n* `` -&gt; `...1709`\n* `` -&gt; `...1710`\n* `` -&gt; `...1711`\n* `` -&gt; `...1712`\n* `` -&gt; `...1713`\n* `` -&gt; `...1714`\n* `` -&gt; `...1715`\n* `` -&gt; `...1716`\n* `` -&gt; `...1717`\n* `` -&gt; `...1718`\n* `` -&gt; `...1719`\n* `` -&gt; `...1720`\n* `` -&gt; `...1721`\n* `` -&gt; `...1722`\n* `` -&gt; `...1723`\n* `` -&gt; `...1724`\n* `` -&gt; `...1725`\n* `` -&gt; `...1726`\n* `` -&gt; `...1727`\n* `` -&gt; `...1728`\n* `` -&gt; `...1729`\n* `` -&gt; `...1730`\n* `` -&gt; `...1731`\n* `` -&gt; `...1732`\n* `` -&gt; `...1733`\n* `` -&gt; `...1734`\n* `` -&gt; `...1735`\n* `` -&gt; `...1736`\n* `` -&gt; `...1737`\n* `` -&gt; `...1738`\n* `` -&gt; `...1739`\n* `` -&gt; `...1740`\n* `` -&gt; `...1741`\n* `` -&gt; `...1742`\n* `` -&gt; `...1743`\n* `` -&gt; `...1744`\n* `` -&gt; `...1745`\n* `` -&gt; `...1746`\n* `` -&gt; `...1747`\n* `` -&gt; `...1748`\n* `` -&gt; `...1749`\n* `` -&gt; `...1750`\n* `` -&gt; `...1751`\n* `` -&gt; `...1752`\n* `` -&gt; `...1753`\n* `` -&gt; `...1754`\n* `` -&gt; `...1755`\n* `` -&gt; `...1756`\n* `` -&gt; `...1757`\n* `` -&gt; `...1759`\n* `` -&gt; `...1761`\n* `` -&gt; `...1762`\n* `` -&gt; `...1763`\n* `` -&gt; `...1764`\n* `` -&gt; `...1765`\n* `` -&gt; `...1766`\n* `` -&gt; `...1767`\n* `` -&gt; `...1768`\n* `` -&gt; `...1769`\n* `` -&gt; `...1770`\n* `` -&gt; `...1771`\n* `` -&gt; `...1772`\n* `` -&gt; `...1773`\n* `` -&gt; `...1774`\n* `` -&gt; `...1775`\n* `1` -&gt; `1...1777`\n* `1` -&gt; `1...1778`\n* `` -&gt; `...1779`\n* `` -&gt; `...1780`\n* `` -&gt; `...1781`\n* `` -&gt; `...1782`\n* `` -&gt; `...1783`\n* `` -&gt; `...1784`\n* `` -&gt; `...1786`\n* `` -&gt; `...1787`\n* `` -&gt; `...1788`\n* `` -&gt; `...1789`\n* `` -&gt; `...1790`\n* `` -&gt; `...1791`\n* `` -&gt; `...1792`\n* `` -&gt; `...1793`\n* `` -&gt; `...1794`\n* `` -&gt; `...1795`\n* `` -&gt; `...1796`\n* `` -&gt; `...1797`\n* `` -&gt; `...1798`\nWarning message:\n\"One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\"\nRows: 445131 Columns: 1800\n-- Column specification --------------------------------------------------------\nDelimiter: \" \"\nchr (530): 01...1, 0102032022, 02, 01012, 99121208, ...65, 2222, ...68, ...1...\ndbl (393): 11002022000001, 11...37, 121, ...44, ...52, 2...53, ...54, ...55,...\nlgl (877): ...2, ...3, ...4, ...5, ...6, ...7, ...8, ...9, ...10, ...11, ......\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nCode\nlibrary(haven)\n\n\n\n\nCode\nllcp &lt;- read_xpt(\"../data/raw_data/LLCP2022.XPT\")\n\n\n\n\nCode\nvis_miss(llcp |&gt; slice_sample(n = 1000))\n\n\n\n\n\n\n\nCode\nx &lt;- llcp %&gt;%\n    drop_na(MARIJAN1)\n\nx &lt;- x %&gt;%\n    select(where(~ sum(is.na(.)) &lt; 5000))\n    \nx &lt;- x %&gt;%\n    mutate(na_count = apply(., 1, function(x) sum(is.na(x)))) %&gt;%\n    filter(na_count &lt; 20) \n\n\n\n\nCode\nx\n\n\n\nA tibble: 94919 x 115\n\n\n_STATE\nFMONTH\nIDATE\nIMONTH\nIDAY\nIYEAR\nDISPCODE\nSEQNO\n_PSU\nSEXVAR\n...\n_RFSMOK3\n_CURECI2\n_SMOKGRP\nDRNKANY6\nDROCDY4_\n_RFBING6\n_DRNKWK2\n_RFDRHV8\n_AIDTST4\nna_count\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;chr&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n...\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;int&gt;\n\n\n\n\n9\n1\n02082022\n02\n08\n2022\n1100\n2022000006\n2.022e+09\n2\n...\n1\n1\n4\n2\n0\n1\n0\n1\n9\n0\n\n\n9\n1\n02042022\n02\n04\n2022\n1100\n2022000007\n2.022e+09\n2\n...\n1\n1\n4\n2\n0\n1\n0\n1\n2\n0\n\n\n9\n1\n02162022\n02\n16\n2022\n1100\n2022000008\n2.022e+09\n2\n...\n1\n1\n3\n1\n67\n1\n933\n2\n1\n0\n\n\n9\n1\n02082022\n02\n08\n2022\n1100\n2022000010\n2.022e+09\n1\n...\n1\n1\n4\n2\n0\n1\n0\n1\n1\n0\n\n\n9\n1\n02122022\n02\n12\n2022\n1100\n2022000011\n2.022e+09\n2\n...\n1\n1\n2\n1\n43\n1\n300\n1\n2\n0\n\n\n9\n1\n02152022\n02\n15\n2022\n1100\n2022000012\n2.022e+09\n2\n...\n1\n1\n4\n2\n0\n1\n0\n1\n2\n0\n\n\n9\n1\n02082022\n02\n08\n2022\n1100\n2022000013\n2.022e+09\n2\n...\n1\n1\n4\n2\n0\n1\n0\n1\n1\n0\n\n\n9\n2\n02182022\n02\n18\n2022\n1100\n2022000014\n2.022e+09\n2\n...\n2\n1\n1\n2\n0\n1\n0\n1\n1\n0\n\n\n9\n2\n02232022\n02\n23\n2022\n1100\n2022000015\n2.022e+09\n2\n...\n1\n1\n3\n1\n33\n1\n233\n1\n1\n0\n\n\n9\n2\n02152022\n02\n15\n2022\n1100\n2022000016\n2.022e+09\n1\n...\n1\n1\n4\n1\n14\n1\n100\n1\n1\n0\n\n\n9\n2\n02142022\n02\n14\n2022\n1100\n2022000017\n2.022e+09\n2\n...\n1\n1\n4\n1\n3\n1\n23\n1\n1\n0\n\n\n9\n2\n02252022\n02\n25\n2022\n1100\n2022000018\n2.022e+09\n1\n...\n1\n1\n4\n1\n17\n1\n117\n1\n1\n0\n\n\n9\n2\n02242022\n02\n24\n2022\n1100\n2022000019\n2.022e+09\n1\n...\n1\n1\n4\n2\n0\n1\n0\n1\n2\n0\n\n\n9\n2\n02102022\n02\n10\n2022\n1100\n2022000020\n2.022e+09\n2\n...\n1\n1\n4\n1\n14\n1\n200\n1\n1\n0\n\n\n9\n2\n02272022\n02\n27\n2022\n1100\n2022000021\n2.022e+09\n1\n...\n1\n1\n4\n1\n7\n1\n93\n1\n1\n0\n\n\n9\n2\n02162022\n02\n16\n2022\n1100\n2022000022\n2.022e+09\n2\n...\n1\n1\n4\n1\n3\n1\n99900\n9\n9\n1\n\n\n9\n3\n03172022\n03\n17\n2022\n1100\n2022000024\n2.022e+09\n2\n...\n1\n1\n4\n1\n13\n1\n93\n1\n2\n0\n\n\n9\n3\n03212022\n03\n21\n2022\n1100\n2022000025\n2.022e+09\n2\n...\n1\n1\n4\n2\n0\n1\n0\n1\n1\n0\n\n\n9\n3\n03102022\n03\n10\n2022\n1100\n2022000026\n2.022e+09\n2\n...\n1\n9\n3\n2\n0\n1\n0\n1\n2\n0\n\n\n9\n3\n03092022\n03\n09\n2022\n1100\n2022000027\n2.022e+09\n2\n...\n1\n1\n4\n1\n3\n1\n23\n1\n1\n3\n\n\n9\n3\n03092022\n03\n09\n2022\n1100\n2022000028\n2.022e+09\n2\n...\n1\n1\n3\n1\n7\n1\n47\n1\n2\n0\n\n\n9\n3\n03272022\n03\n27\n2022\n1100\n2022000029\n2.022e+09\n2\n...\n1\n1\n4\n1\n27\n1\n373\n1\n1\n1\n\n\n9\n3\n03152022\n03\n15\n2022\n1100\n2022000030\n2.022e+09\n2\n...\n1\n1\n2\n2\n0\n1\n0\n1\n2\n0\n\n\n9\n3\n03232022\n03\n23\n2022\n1100\n2022000031\n2.022e+09\n2\n...\n1\n1\n4\n2\n0\n1\n0\n1\n1\n0\n\n\n9\n3\n03092022\n03\n09\n2022\n1100\n2022000032\n2.022e+09\n1\n...\n1\n1\n4\n1\n3\n1\n23\n1\n2\n0\n\n\n9\n3\n03132022\n03\n13\n2022\n1100\n2022000033\n2.022e+09\n2\n...\n1\n1\n4\n1\n100\n1\n700\n1\n1\n0\n\n\n9\n3\n03182022\n03\n18\n2022\n1100\n2022000034\n2.022e+09\n2\n...\n2\n1\n1\n2\n0\n1\n0\n1\n2\n0\n\n\n9\n3\n03082022\n03\n08\n2022\n1100\n2022000036\n2.022e+09\n2\n...\n1\n1\n3\n2\n0\n1\n0\n1\n2\n0\n\n\n9\n1\n02082022\n02\n08\n2022\n1100\n2022000037\n2.022e+09\n2\n...\n1\n1\n4\n1\n7\n1\n47\n1\n2\n0\n\n\n9\n1\n02032022\n02\n03\n2022\n1100\n2022000038\n2.022e+09\n2\n...\n1\n1\n4\n1\n14\n1\n200\n1\n2\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n78\n11\n01092023\n01\n09\n2023\n1100\n2022001498\n2022001498\n1\n...\n1\n1\n4\n1\n50\n2\n1050\n1\n1\n2\n\n\n78\n11\n01112023\n01\n11\n2023\n1100\n2022001499\n2022001499\n2\n...\n1\n1\n4\n7\n900\n9\n99900\n9\n1\n2\n\n\n78\n11\n12132022\n12\n13\n2022\n1100\n2022001500\n2022001500\n2\n...\n1\n1\n4\n1\n3\n2\n47\n1\n1\n3\n\n\n78\n11\n12072022\n12\n07\n2022\n1100\n2022001504\n2022001504\n1\n...\n1\n1\n4\n2\n0\n1\n0\n1\n1\n2\n\n\n78\n11\n12222022\n12\n22\n2022\n1100\n2022001505\n2022001505\n1\n...\n1\n1\n4\n1\n33\n1\n700\n1\n1\n2\n\n\n78\n11\n12142022\n12\n14\n2022\n1100\n2022001506\n2022001506\n2\n...\n1\n1\n4\n2\n0\n1\n0\n1\n1\n2\n\n\n78\n11\n12132022\n12\n13\n2022\n1100\n2022001507\n2022001507\n1\n...\n1\n1\n4\n1\n3\n1\n23\n1\n1\n2\n\n\n78\n11\n12092022\n12\n09\n2022\n1100\n2022001508\n2022001508\n2\n...\n1\n1\n4\n2\n0\n1\n0\n1\n2\n2\n\n\n78\n11\n12132022\n12\n13\n2022\n1100\n2022001509\n2022001509\n2\n...\n1\n1\n4\n2\n0\n1\n0\n1\n2\n2\n\n\n78\n11\n12202022\n12\n20\n2022\n1100\n2022001510\n2022001510\n1\n...\n1\n1\n4\n1\n7\n1\n47\n1\n1\n2\n\n\n78\n11\n12172022\n12\n17\n2022\n1100\n2022001511\n2022001511\n1\n...\n2\n1\n3\n1\n17\n2\n350\n1\n1\n2\n\n\n78\n11\n12162022\n12\n16\n2022\n1100\n2022001512\n2022001512\n1\n...\n1\n1\n4\n1\n90\n2\n630\n1\n2\n2\n\n\n78\n11\n12192022\n12\n19\n2022\n1100\n2022001513\n2022001513\n2\n...\n1\n1\n4\n2\n0\n1\n0\n1\n2\n2\n\n\n78\n11\n12172022\n12\n17\n2022\n1100\n2022001514\n2022001514\n2\n...\n1\n1\n4\n1\n43\n2\n600\n1\n1\n2\n\n\n78\n11\n12202022\n12\n20\n2022\n1100\n2022001515\n2022001515\n1\n...\n2\n1\n1\n1\n86\n1\n600\n1\n1\n2\n\n\n78\n11\n12162022\n12\n16\n2022\n1100\n2022001516\n2022001516\n1\n...\n1\n1\n3\n2\n0\n1\n0\n1\n1\n2\n\n\n78\n11\n01152023\n01\n15\n2023\n1100\n2022001517\n2022001517\n1\n...\n1\n1\n4\n1\n71\n1\n1000\n1\n2\n2\n\n\n78\n11\n12162022\n12\n16\n2022\n1100\n2022001518\n2022001518\n2\n...\n1\n1\n4\n1\n7\n1\n47\n1\n9\n2\n\n\n78\n11\n12172022\n12\n17\n2022\n1100\n2022001519\n2022001519\n2\n...\n1\n1\n3\n1\n7\n1\n47\n1\n2\n2\n\n\n78\n11\n12132022\n12\n13\n2022\n1100\n2022001520\n2022001520\n1\n...\n2\n1\n3\n1\n100\n2\n1400\n1\n2\n2\n\n\n78\n11\n12182022\n12\n18\n2022\n1100\n2022001521\n2022001521\n1\n...\n1\n1\n3\n2\n0\n1\n0\n1\n1\n2\n\n\n78\n11\n01142023\n01\n14\n2023\n1100\n2022001522\n2022001522\n1\n...\n1\n1\n2\n2\n0\n1\n0\n1\n2\n3\n\n\n78\n11\n01122023\n01\n12\n2023\n1100\n2022001523\n2022001523\n2\n...\n1\n1\n4\n2\n0\n1\n0\n1\n2\n2\n\n\n78\n11\n12172022\n12\n17\n2022\n1100\n2022001524\n2022001524\n1\n...\n1\n1\n4\n1\n29\n1\n400\n1\n1\n2\n\n\n78\n11\n12142022\n12\n14\n2022\n1100\n2022001526\n2022001526\n1\n...\n1\n1\n4\n1\n7\n1\n187\n1\n9\n2\n\n\n78\n11\n12192022\n12\n19\n2022\n1100\n2022001527\n2022001527\n2\n...\n1\n1\n4\n7\n900\n9\n99900\n9\n1\n2\n\n\n78\n11\n12212022\n12\n21\n2022\n1100\n2022001528\n2022001528\n2\n...\n1\n1\n4\n2\n0\n1\n0\n1\n1\n2\n\n\n78\n11\n11292022\n11\n29\n2022\n1100\n2022001529\n2022001529\n2\n...\n2\n2\n1\n7\n900\n9\n99900\n9\n2\n2\n\n\n78\n11\n12082022\n12\n08\n2022\n1100\n2022001530\n2022001530\n1\n...\n1\n1\n4\n2\n0\n1\n0\n1\n1\n2\n\n\n78\n11\n12142022\n12\n14\n2022\n1100\n2022001531\n2022001531\n1\n...\n1\n1\n3\n1\n50\n2\n99900\n9\n2\n2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "",
    "section": "",
    "text": "Marion Geary Bauman is currently pursing a masters degree in Data Science and Analytics at Georgetown University. As a data scientist and statistician, she is drawn to the intersection of technical skills and their practical applications for improving the lives of others. She is driven to do technical work that is meaningful and can have a positive impact on those it influences. She loves to utilize her programming knowledge, analytical skills, and problem-solving abilities to devise creative and actionable solutions to real-world problems.\nClifton Strengths: | Intellection | Empathy | Learner | Achiever | Input"
  },
  {
    "objectID": "index.html#meet-marion",
    "href": "index.html#meet-marion",
    "title": "DSAN-5000: Introduction",
    "section": "",
    "text": "Marion Geary Bauman is currently pursing a masters degree in Data Science and Analytics at Georgetown University. As a data scientist and statistician, she is drawn to the intersection of technical skills and their practical applications for improving the lives of others. She is driven to do technical work that is meaningful and can have a positive impact on those it influences. She loves to utilize her programming knowledge, analytical skills, and problem-solving abilities to devise creative and actionable solutions to real-world problems.\n\n\n\nIntellection\nEmpathy\nLearner\nAchiever\nInput"
  },
  {
    "objectID": "index.html#academic-interests",
    "href": "index.html#academic-interests",
    "title": "",
    "section": "Academic Interests",
    "text": "Academic Interests\nMarion is particularly interested in studying Natural Language Processing and Large Language Models. She has experience using advanced NLP techniques in industry, and she is especially focused on the ethical usage of artificial intelligence."
  },
  {
    "objectID": "index.html#previous-projects",
    "href": "index.html#previous-projects",
    "title": "",
    "section": "Previous Projects",
    "text": "Previous Projects\n\n\n\nOpen Source Development\nMarion has been a part of various open source development projects with a particular focus on R. She is currently developing a new set of themes for ggplot, and she supports the R community through answering questions on Stack Overflow.\n\n\n\nData Science Projects\nMarion developed an enterprise data science solution to classify medical research grant abstracts by intervention type to assist leadership at a large medical research organization. She has also used machine learning to build an NFL player performance predictor, which was used as the backend for a student-built app."
  },
  {
    "objectID": "index.html#about-marion",
    "href": "index.html#about-marion",
    "title": "",
    "section": "",
    "text": "Marion Geary Bauman is currently pursing a masters degree in Data Science and Analytics at Georgetown University. As a data scientist and statistician, she is drawn to the intersection of technical skills and their practical applications for improving the lives of others. She is driven to do technical work that is meaningful and can have a positive impact on those it influences. She loves to utilize her programming knowledge, analytical skills, and problem-solving abilities to devise creative and actionable solutions to real-world problems.\nClifton Strengths: | Intellection | Empathy | Learner | Achiever | Input"
  }
]